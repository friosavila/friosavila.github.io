{
  "hash": "562b2b5d216f90a9ba97bf574fd2310a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Simultaneous/Uniform Confidence Intervals\"\nsubtitle: \"Featuring `uci`\"\nformat: html\ncode-fold: true\n---\n\n## Introduction\n\nConfidence intervals (**CI**), also known as interval estimates, are ranges of values that provided along point estimates as potential values for population parameters, based on the estimation method, level of confidence, and degree of precision of those estimates.\n\nOne usual confusion that arises from CI is its interpretation. Its important to emphasize that CI are just estimates, and are not guaranteed to contain the true estimate. \n\nFor example, when we say the estimate for $\\alpha$ is 1 with a 95% CI of $(-.5,.5)$, it doesnt mean that the true parameter will be contained in that confidence interval with a 95% probability. \n\nInstead, it means that if we repeat the sampling excercise a large number of times, and construct a CI of the same level, it will contain the true parameter 95% of the time. \n\n::: {#433cec97 .cell execution_count=1}\n``` {.stata .cell-code}\nclear\nset scheme white2\nset obs 200\ngen p = (_n*2-1)/(2*_N)\ngen sd = (1/sqrt(500))\ngen b1 = invnormal(p)*sd\ngen ll=b1-sd*invnormal(0.95)\ngen uu=b1+sd*invnormal(0.95)\n\ngen n=p\ntwo (rarea ll uu n if uu<0,color(red%50)) (rarea ll uu n if ll>0,color(red%50)) ///\n\t(rarea ll uu n if inrange(0,ll,uu), color(green%20)) (line b1 n, color(gs1)) , ///\n\tlegend(order(1 \"Rejection of Null\" 3 \"Null not Rejected\"))  \ngraph export uci1.png, width(1000)  height(500) replace\n```\n:::\n\n\n![90% CI](uci1.png){#fig-uc1}\n\nAs an example, @fig-uc1 shows a 90% Confidence intervals for a coefficient that by construction has a true value of zero. When considering the CI, the middle 90% of the coefficients contain the true paramater of zero. But the upper and lower 5% do not. Thus, with a 90% confidence interval, we have 10% chances of rejecting the Null when it is true. \n\n## The Problem of Simultaneous hypothesis\n\nWhile **CI** are useful to understand the level of precision of an estimate, they are not a good tool when one is interest in testing multiple hypothesis at the same time. \n\nConsider the following example:\n\n> Say that you have access to two independent sources of data, and you test the same hypothesis in both. Say, on top of this, that you use 90% confidence intervals to make your hypothesis. How likely is that either of the test will come as possitve?\n\nThe answer in this case is straigh forward. The chances that the first estimator will be a false positive: 10%. When negative (90%) there is also a 10% chance of being a false positive, in the second sample: $10%+90%*10%=19%$. So using a second sample, almost doubles the probability of finding a *false* significant result. \n\nLet's take the excercise a bit farther. If we run the same experiment on 7 different samples, the chances of finding a possitive results is now over 50%. Its easy to see that one can very easily falsify results by just adding the number of samples used for the analysis.^[This will not happen if we just increase the sample]\n\nWhat other alternatives do we have? Well, there are at least 2:\n\n- Use joint hypothesis tests (wald-test). This will adjust for possible correlations (or lack there of) among the estimated parameters, and provide a single statistic that tests if at least one of the parameters is different from zero.\n\n- Use Simultaneous Confidence Intervals, which adjusts standard critical values, so that only $\\alpha%$ of the cases we jointly rejected coefficients of interest. I like to think of this as a way to translate joint tests into single hypothesis testing. \n\n## How will this work?\n\nLets reconsider the previous example. When I have two independent samples the naive joint share of false significance tests would be given by:\n\n$$\\alpha_{12} = \\alpha_1 + (1-\\alpha_1)* \\alpha_2$$\n\nwhere $1-\\alpha_j$ is the level of confidence you would normaly used for single Hypothesis testing. \n\nNow, we know $\\alpha_j$ is to high, and that it has to be the same for both groups. Our goal is find an appropriate $\\alpha_j$, so that the joint probability of false positive is 5% ($\\alpha_{12}$):\n\n$$\\alpha_{12}= \\alpha + (1-\\alpha)* \\alpha = 2\\alpha - \\alpha^2$$\n\nWhich suggest the level of significance we need to use on each individual sampe should be:\n\n$$\\alpha = 1-\\sqrt{1-\\alpha_{12}}$$\n\nFor the previous case, if one is trying to use a 90% confidence interval, we need to set $\\alpha_{12}$ at 10% (0.1). Which implies that for each individual test we need to use an $\\alpha$ at 0.0513 or 5.13%. Which is almost half of what we started with. \n\nOf course, if you have more than 2 coefficients you need testing, or if there are some correlations among them, the formula above may need to be adjusted. (some approaches do something like that)\n\n## UCI via Influence Functions (IF)\n\nThe first time I came through the idea of uniform confidence Intervals was when I started working with `csdid` (@callaway_santanna_2021), where both standard errors and confidence intervals are estimated via Influence functions.\n\nThis is done as follows:\n\n1. Derive the influence functions of your estimator.\n2. Use a multiplicative wildbootstrap to disturb the IF, estimate standard errors and absolute value of t-statistics for all your parameters.\n3. For each iteration of the wildbootstrap, choose the largest t-statistic. You have now a single column of t-statistics.\n4. From this new variable choose the ($1-\\alpha$) percentile. This will be your new critical value.\n5. Re-estimate your Confidence intervals.\n\nBecause the new critical value was constructed by looking at the highest t-stat, it will have a the same properties as the manual significance level I provided above.\n\nHere a small example of how it works.\n\n::: {#ee5e256f .cell execution_count=2}\n``` {.stata .cell-code code-fold=\"false\"}\nsysuse auto, clear\ngen one=1\nmata\n\tx  =st_data(.,\"mpg foreign one\")\n\ty  =st_data(.,\"price\")\n\tn  = rows(y)\n\txx =cross(x,x);\txy =cross(x,y)\n\tixx=invsym(xx)\n\tb  =ixx*xy; e = y:-x*b\n\tiff = 74*(x:*e)*ixx\n\tiffm=J(1000,3,0)\n\tfor(i=1;i<=1000;i++){\n\t\tiffm[i,]=mean(iff:*rnormal(n,1,0,1))\n\t}\n\tse=diagonal(variance(iffm):^.5)'\n\tts=abs(iffm:/se);ts=rowmax(ts);_sort(ts,1)\n\ttcrit=ts[ceil(0.95*rows(ts))]\n\tst_matrix(\"tbl\", (b,se',b:-tcrit*se',b:+tcrit*se'))\nend\n```\n:::\n\n\n::: {#213195d8 .cell execution_count=3}\n``` {.stata .cell-code}\nreg price mpg foreign, robust\nmatrix rowname tbl =mpg foreign cons\nmatrix colname tbl =b se ll uu\nmatrix list tbl\nmata:\"Adjusted Critical Value\"\nmata:tcrit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLinear regression                               Number of obs     =         74\n                                                F(2, 71)          =      12.72\n                                                Prob > F          =     0.0000\n                                                R-squared         =     0.2838\n                                                Root MSE          =     2530.9\n\n------------------------------------------------------------------------------\n             |               Robust\n       price | Coefficient  std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -294.1955   60.33645    -4.88   0.000     -414.503   -173.8881\n     foreign |   1767.292   607.7385     2.91   0.005     555.4961    2979.088\n       _cons |   11905.42   1362.547     8.74   0.000     9188.573    14622.26\n------------------------------------------------------------------------------\n\ntbl[3,4]\n                  b          se          ll          uu\n    mpg  -294.19553   59.797238   -433.1367  -155.25436\nforeign   1767.2922   616.19052   335.54999   3199.0345\n   cons   11905.415   1335.2436   8802.9257   15007.905\n  Adjusted Critical Value\n  2.323538262\n```\n:::\n:::\n\n\nFirst, I estimate the influence function `iff` for the model coefficients. Then, I disturb them using a standard normal noise. I estimate standard errors and t-statistics based on the disturb data `iffm`, and obtain the adjusted critical value `tcrit`.  This critical value is then used to reconstruct the confidence intervals. \n\nI compare the results from the regression analysis, with the adjusted critical value. And, as expected, they are a bit wider than the original confidence interval.\n\n## Via Simulation\n\nThe strategy I show above is relatively simple if you know how to estimate the **IF**. However, beyond simple statistics, obtaining them may be too involved for the average user.\n\nSo, one alternative I'm suggesting here is to do the same, but via simulation.\n\n1. For any regression analysis obtain the matrix of coefficients `b` and variance covariance `V`.\n2. Draw **N** normal distributed samples with mean zero and Variance covariance `V`. \n3. Obtain the **z/t** statistic from the simulated coefficients, and the model estimated variance matrix.\n4. Just like before, obtain the new critical value, based on the new set of **t-statistics**, and adjust the Confidence intervals!\n\nHere is where my new command `uci` comes into play. First, you may need to install it using `fra`. Also if you want to use `fra` please see [here](https://friosavila.github.io/chatgpt/fra_03_30_2023/) for instructions:\n\n::: {#65d97c5e .cell execution_count=4}\n``` {.stata .cell-code code-fold=\"false\"}\nfra install uci, replace\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nchecking uci consistency and verifying not already installed...\ninstalling into c:\\ado\\plus\\...\ninstallation complete.\n```\n:::\n:::\n\n\nThe command does not have a helpfile yet, but its use is straigh forward. \n\n- If you run it after any regression command, it will take the information from that regression and obtain the modified confidence intervals.\n- If one wants to do it using other pre-saved information, one can do so using the option `bmatrix()` and `vmatrix()`. \n- One can choose how many repetitions to run, using `reps(#)`, where the default is 999\n- Its possible to set a seed using `rseed()` for cases where replication is needed.\n- You can also set the level of confidence intervals `level(#)`, where the default is 95.\n\nSo, let me redo the above example:\n\n::: {#be10cf50 .cell execution_count=5}\n``` {.stata .cell-code code-fold=\"false\"}\nreg price mpg foreign, robust\nuci\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLinear regression                               Number of obs     =         74\n                                                F(2, 71)          =      12.72\n                                                Prob > F          =     0.0000\n                                                R-squared         =     0.2838\n                                                Root MSE          =     2530.9\n\n------------------------------------------------------------------------------\n             |               Robust\n       price | Coefficient  std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -294.1955   60.33645    -4.88   0.000     -414.503   -173.8881\n     foreign |   1767.292   607.7385     2.91   0.005     555.4961    2979.088\n       _cons |   11905.42   1362.547     8.74   0.000     9188.573    14622.26\n------------------------------------------------------------------------------\nUniform Confidence Intervals based on Simulation\n\nrtable2[3,5]\n                  b          se           t          ll          lu\n    mpg  -294.19553   60.336453  -4.8759169  -434.12725  -154.26381\nforeign   1767.2922   607.73848   2.9079815   357.83104   3176.7534\n  _cons   11905.415   1362.5472   8.7376169    8745.409   15065.422\nRepetitions:999\nLevel:95\nnew T:2.31919\n```\n:::\n:::\n\n\nThis provides a slighly different results, because of how standard errors are estimated, and the random nature bootstrap procedure, but it should be a good approximation to the Uniform Confidence intervals.\n\n## Conclusions\n\nThis aims to provide a small description of what Uniform confidence intervals are, and how to estimate them.\n\nIt also introduces a new small command, `uci`, which should help to estimate this type of confidence intervals, for almost any model.\n\nComments and suggestions are welcome.\n\nTil next time.\n\n",
    "supporting": [
      "stata_do5_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}
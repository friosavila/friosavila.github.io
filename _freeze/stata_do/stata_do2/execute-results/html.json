{
  "hash": "c71acc539384fcd3c0c6646f593d59c2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"How to Bootstrap\"\nsubtitle: \"When Sandwitch is not enough\"\nformat: html\n---\n\n## Introduction: What is Bootstrapping\n\nOne of the primary concerns of econometricians and economists is estimating point estimates with precision. However, as point estimates contain errors, we need to estimate the precision of those estimates to determine how reliable they are. This precision is typically expressed as standard errors, which reflect the level of uncertainty in the estimates given the information we have in hand, i.e., the sample.\n\nIn most introductory econometrics courses, we learn that we can estimate the precision of these estimates by drawing multiple samples, estimating the model of interest for each new sample, and summarizing the estimated coefficients. The standard deviations of the estimated coefficients are the coefficient standard errors, which reflect the variation in the coefficients due to sampling error.\n\nHowever, collecting multiple samples from the same population is technically impossible (expensive), and we need to rely on other approaches to estimate the standard errors. Two common approaches are typically used:\n\n- Asymptotic approximations: where we make use of some of the properties of the estimators (deep knowledge of how those are constructed) \n- Empirical approximations: Or what we can call Bootstrapping.\n\n## But What is Bootstrapping?\n\nAs a non-native speaker, I initially thought that bootstrapping was merely a statistical technique for obtaining empirical standard errors. However, after a few years in grad school, I heard the expression:\n\n> pull yourself up by your own bootstraps\n\na few times, which describes what bootstrapping does. Since we don't have access to other samples, we repeatedly use and reuse the same sample in various ways to estimate standard errors for our estimates.\n\nThe differences in how we reuse the sample information determine the type of bootstrapping method we're using.\n\n## Types of Bootstrapping.\n\nThere are many approaches to obtaining bootstrap standard errors, depending on the assumptions we're willing to impose on the data, and not all of them can be applied in every scenario. For simplicity, I'll refer to the ones that can be used for linear regressions.\n\nSuppose you're interested in a linear regression model with the following functional form:\n\n$$\ny_i=X_i\\beta+e_i\n$$\n\n## Setup and Asymptotic SE\n\nTo get started with bootstrapping, we will estimate a very simple linear regression model using the `auto.dta` dataset.\n\n::: {#711c7308 .cell execution_count=1}\n``` {.stata .cell-code}\nset linesize 255\nprogram drop _all\nsysuse auto, clear\nreg price mpg foreign\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<style>div.jp-Notebook .datagrid-container {min-height: 448px; }</style>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(1978 automobile data)\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(2, 71)        =     14.07\n       Model |   180261702         2  90130850.8   Prob > F        =    0.0000\n    Residual |   454803695        71  6405685.84   R-squared       =    0.2838\n-------------+----------------------------------   Adj R-squared   =    0.2637\n       Total |   635065396        73  8699525.97   Root MSE        =    2530.9\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -294.1955   55.69172    -5.28   0.000    -405.2417   -183.1494\n     foreign |   1767.292    700.158     2.52   0.014     371.2169    3163.368\n       _cons |   11905.42   1158.634    10.28   0.000     9595.164    14215.67\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nThis estimation provides the asymptotic estimation of standard errors under homoskedasticity using the well-known formula:\n\n$$\nVar(\\hat \\beta) = \\frac{\\sum \\hat e ^2}{n-k-1} (X'X)^{-1}\n$$\n\nLet's see how this standard errors compare to different types of Bootstrap Standard errors:\n\n### Parametric Bootstrap\n\nAs the name suggest, parametric bootstrap requires imposing some parametric assumptions on the source of the error in the model: $e$. We will learn those characteristics from the estimated errors in the original sample. \n\nSay, for example, that we assume $e$ follows some normal distribution, with variance equal to the variance of the observed error $\\sigma^2_e=Var(\\hat e)$. Parametric bootstrapping would require to draw/create different samples using the following rule:\n$$\n\\tilde y_i = X_i \\hat \\beta + \\tilde e_i \\ where \\ \\tilde e_i \\sim N(0,var(\\hat e))\n$$\n\nIn this case, all $X's$ are fixed, but the *new* samples are created by resampling the error, and constructing $\\tilde y's$. This differs only because of the draws of the errors $\\tilde e$.\n\nOnce you get multiple of samples, and coefficients for each, you can simply Report the associated Standard Errors.\n\nHow to do it in `Stata`? Here I will cheat a bit, and use `Mata`, because it will be faster. Note that you will need to copy the whole code into a dofile and run all of it, or type each line individualy in the command window (once you activate `Mata`):\n\n::: {#104afac7 .cell execution_count=2}\n``` {.stata .cell-code}\nset seed 10\n// First we start mata\nmata:\n    // load all data\n    y = st_data(.,\"price\")\n    // You load X's and the constant\n    x = st_data(.,\"mpg foreign\"),J(rows(y),1,1)\n    // Estimate Betas:\n    b = invsym(x'*x)*x'y\n    // Estimate errors:\n    e = y:-x*b\n    // Estimate STD of errors\n    std_e=sqrt(sum(e:^2)/73)\n\t// Now we can do the bootstrap:\n\t// We first create somewhere to store the different betas\n\tbb = J(1000,3,.)\n\t// and start a loop\n\tfor(i=1;i<=1000;i++){\n\t\t// each time we draw a different value for y..say ys\n\t\tys = x*b+rnormal(74,1,0,std_e)\n\t\t// and estimate the new beta, storing it into bb\n\t\tbb[i,]=(invsym(x'*x)*x'ys)'\n\t}\nend\n```\n:::\n\n\nIf everythings goes well, it should give you the following\n\n::: {#896257c1 .cell execution_count=3}\n``` {.stata .cell-code}\nmata: b,diagonal(sqrt(variance(bb)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  1              2\n    +-------------------------------+\n  1 |  -294.1955331    55.59108401  |\n  2 |   1767.292243    689.3703271  |\n  3 |   11905.41528    1159.921784  |\n    +-------------------------------+\n```\n:::\n:::\n\n\nNotice that we are explicitly impossing the assumption of homoskedasticity and normality on the errors. This explain why this standard errors are almost identical to the simple asymptotic standard errors.\n\n### Residual Bootstrap\n\nResidual bootstrap is very similar to the parametric bootstrap I described above. The main difference is that we no longer impose assumptions on the errors distributions, and instead use the empirical distribution.\n\nWhat does this mean? Well, In the above example, we have 74 different values for the error $e$, thus resampling means that you create a new $\\tilde y$ by drawing 74 errors from this bag of errors, where all have the same probability of being choosen.\n$$\n\\tilde y_i  =  X_i \\hat \\beta  + \\tilde e_i \\ where  \\ \\tilde e \\sim [\\hat e_1, \\hat e_2,...,\\hat e_N]\n$$\n\nLets implement it:\n\n::: {#428936db .cell execution_count=4}\n``` {.stata .cell-code}\nset seed 10\nmata:\n    // This remains the same as before\n    y = st_data(.,\"price\")\n    x = st_data(.,\"mpg foreign\"),J(rows(y),1,1)\n    b = invsym(x'*x)*x'y\n    e = y:-x*b\n    bb = J(1000,3,.)\n    // Now we need to know how many observations we have\n    nobs=rows(y)\n    \tfor(i=1;i<=1000;i++){\n\t\t// Here is where we \"draw\" a different error everytime, \n        // runiformint(nobs,1,1,nobs) <- This says Choose a randome number between 1 to K\n        // and use that value to assing as the new error to create ys\n\t\tys = x*b+e[runiformint(nobs,1,1,nobs)]\n\t\tbb[i,]=(invsym(x'*x)*x'ys)'\n\t}\t\nend\n```\n:::\n\n\n::: {#5f093a5a .cell execution_count=5}\n``` {.stata .cell-code}\nmata: b,diagonal(sqrt(variance(bb)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  1              2\n    +-------------------------------+\n  1 |  -294.1955331    53.63708346  |\n  2 |   1767.292243    703.1172879  |\n  3 |   11905.41528    1118.818836  |\n    +-------------------------------+\n```\n:::\n:::\n\n\nOnce again, this method keeps $X's$ fixed, and assumes errors are fully homoskedastic, thus interchangable. It does allow for the possiblity errors do not follow a normal distribution.\n\n### Wild-Bootstrap/multiplicative bootstrap\n\nThe wild bootstrap is another variant of residual bootstrapping. To implement it, we start by estimating the original model and obtaining the model errors. Instead of shuffling or making assumptions about the distribution of the errors, we reuse the error after adding noise to it. Mathematically, this can be expressed as:\n$$\n\\tilde y_i=X_i \\hat \\beta + \\hat e_i * v_i\n$$\n\nHere, $v$ is the source of the noise that we add to the model. Technically, we can use any distribution for $v$, as long as $E(v)=0$ and $Var(v)=1$. The most common distribution used in wild bootstrap implementations is the \"mammen\" distribution, but for simplicity, we will use a normal distribution.\n\n::: {#ead0a3f0 .cell execution_count=6}\n``` {.stata .cell-code}\nset seed 10\nmata:\n    y = st_data(.,\"price\")\n    x = st_data(.,\"mpg foreign\"),J(rows(y),1,1)\n    b = invsym(x'*x)*x'y\n    e = y:-x*b\n    nobs=rows(y)\n    bb = J(1000,3,.)\n\tfor(i=1;i<=1000;i++){\n\t\t// Here is where we \"draw\" a different error multiplying the original error by v ~ N(0,1)\n\t\tys = x*b+e:*rnormal(nobs,1,0,1)\n\t\tbb[i,]=(invsym(x'*x)*x'ys)'\n\t}\nend\n```\n:::\n\n\n::: {#b9aa3982 .cell execution_count=7}\n``` {.stata .cell-code}\nmata: b,diagonal(sqrt(variance(bb)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  1              2\n    +-------------------------------+\n  1 |  -294.1955331    59.62775803  |\n  2 |   1767.292243     586.545592  |\n  3 |   11905.41528    1357.383088  |\n    +-------------------------------+\n```\n:::\n:::\n\n\nSurprisingly, this approach allows us to control for heteroskedasticity, which is why the standard errors obtained using the wild bootstrap method are quite similar to the ones obtained using `reg, robust`.\n\nAnother advantage of this method is that we do not necessarily need to obtain an estimate of the error itself. Instead, we can obtain the Influence Functions of the estimated parameters and disturb those to obtain standard errors. This makes the method feasible for a larger set of estimators, assuming that we can derive the corresponding Influence Functions.\n\n### Paired bootstrap/Nonparametric bootstrap\n\nPaired bootstrap is perhaps the most commonly used method in applied econometrics, although it can also be computationally intensive. \n\nThe basic idea is to use the original sample to draw subsamples with replacement that are of the same size. Then, you estimate the parameters\n of interest for each subsample and summarize the results. What sets this approach apart from others is that the entire set of observations\n  and characteristics are used in the resampling, not just the residuals. This makes it robust to heteroskedasticity and relatively easy to implement for complex model estimators. Stata has a module dedicated to making the implementation of paired bootstrap easy, but it can also be implemented in Mata.\n\n::: {#fe62b88a .cell execution_count=8}\n``` {.stata .cell-code}\nset seed 10\nmata:\n    y = st_data(.,\"price\")\n    x = st_data(.,\"mpg foreign\"),J(rows(y),1,1)\n    b = invsym(x'*x)*x'y\n    e = y:-x*b\n    nobs=rows(y)\n    bb = J(1000,3,.)\n\tfor(i=1;i<=1000;i++){\n\t\t// What I do here is get a vector that will identify the resampling.\n\t\tr_smp = runiformint(nobs,1,1,nobs)\n\t\t// then use this resampling vector to reestimate the betas\n\t\tbrs = invsym(x[r_smp,]'*x[r_smp,])*x[r_smp,]'y[r_smp,]\n\t\tbb[i,]=brs'\n\t}\t\nend\n```\n:::\n\n\n::: {#6e8d35ab .cell execution_count=9}\n``` {.stata .cell-code}\nmata: b,diagonal(sqrt(variance(bb)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  1              2\n    +-------------------------------+\n  1 |  -294.1955331    61.88438186  |\n  2 |   1767.292243    575.3963704  |\n  3 |   11905.41528    1374.021799  |\n    +-------------------------------+\n```\n:::\n:::\n\n\n### Easier Paired bootstrap\nAssuming you do not like to do this with Mata, or that your estimator is a bit more complex than a simple OLS, a better approach for implementing paired bootstrap in `Stata` is simply using the bootstrap prefix:\n\nFor example:\n```stata\nbootstrap, seed(10) reps(1000):reg price mpg foreign\n```\n\nAnd of course, this approach can implement to bootstrap any 1 line command, although it may be faster for some methods than others.:\n\n```stata\nbootstrap, seed(10) reps(1000):qreg price mpg foreign\nbootstrap, seed(10) reps(1000):poisson price mpg foreign\n```\n\n### Bootstrapping a two-step regression\n\nOften, however, you may want to \"bootstrap\" something more complex. For example a two/three/...K step estimator. You can still use bootstrap, but it requires a bit more programming. So lets go with a simple 2-step heckman estimator. My recommendation, first implement the estimator for 1 run:\n\n::: {#e7546913 .cell execution_count=10}\n``` {.stata .cell-code}\nwebuse womenwk, clear\n** data prep\ngen dwage=wage!=.\n** estimation\nprobit dwage married children educ age\npredict mill, score\nreg wage educ age mill\n** delete the variable that was created as intermediate step\ndrop mill\n```\n:::\n\n\nNotice that `mill` was dropped at the end. This is important, because by bootstraping the program, it will beed to be created all over again.\nFinally, we write our little bootstrap program:\n\n::: {#4f8ed409 .cell execution_count=11}\n``` {.stata .cell-code}\n** I like to add eclass properties here\nprogram two_heckman, eclass\n\tcapture drop  mill  \n    ** you implement your estimator:\n    tempvar smp\n    probit dwage married children educ age\n    predict mill, score\n    ** save the \"sample\" from probit\n    gen byte `smp'=e(sample)\n    reg wage educ age mill\n    ** Delete all variables that were created \n    ** Finally, you will Store all the coefficients into a matrix\n    matrix b=e(b)\n    ** and \"post\" them into e() so they can be read as an estimation output\n    ereturn post b, esample(`smp')\nend\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n```\n:::\n:::\n\n\nAnd apply the `bootsrap` prefix to it:\n\n::: {#9264556b .cell execution_count=12}\n``` {.stata .cell-code}\nbootstrap, reps(250) nodots:two_heckman\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nBootstrap results                                        Number of obs = 2,000\n                                                         Replications  =   250\n\n------------------------------------------------------------------------------\n             |   Observed   Bootstrap                         Normal-based\n             | coefficient  std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   education |   .9825259   .0511768    19.20   0.000     .8822212    1.082831\n         age |   .2118695   .0223345     9.49   0.000     .1680947    .2556444\n        mill |   4.001615   .5898434     6.78   0.000     2.845544    5.157687\n       _cons |   .7340391   1.287607     0.57   0.569    -1.789625    3.257703\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## Conclusions\n\nThis post provides an overview of how bootstrap operates and how to implement it using Stata. \n\nRemember, not all techniques are suitable for all circumstances. Furthermore, to accurately estimate standard errors while\n taking into account clustering and weights, resampling methods need to also account for the original sampling structure, or how the data was gathered.\n \nAccounting for clustering is often straightforward, but handling weights and Strata may require additional attention. \n\nI hope you found this information beneficial, and please don't hesitate to reach out with any questions or feedback via email or comments.\n\n",
    "supporting": [
      "stata_do2_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}
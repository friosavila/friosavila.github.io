{
  "hash": "caf7fddb4f2e6e17ba52f636ee779dd0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Linear Regressions, OLS and Standard Errors\"\nformat: html\nbibliography: references.bib\n---\n\n## Introduction\n\nLinear regressions (**LR**) are a fundamental tool for economists to analyze data. Among the various methods for estimating the corresponding parameters, the most commonly used is Ordinary Least Squares (**OLS**). When certain assumptions are met^[See for example Introductory Econometrics: A Modern Approach by Jeffery Wooldridge] , OLS can provide unbiased and consistent estimates of how changes in independent variables ($X's$) affect the dependent variable ($y$), assuming that all other factors remain constant. One advantage of OLS is that it is easy to apply and provides a straightforward solution. \n\nConsider the following linear model (in matrix form), and assume all Classical LR assumtions hold:\n\n$$y =X  \\beta  + e\n$$\n\nIf that is the case, the OLS estimator for $\\beta's$ is simply given by:\n\n$$\n\\hat \\beta_{OLS}=(X'X)^{-1}X'y\n$$\n\nWhich is one of those formulas you probably know by memory by know. Something else you can derive from here is the relationship between the estimate $\\hat\\beta$ and the true $\\beta$:\n\n$$\n\\begin{aligned}\n\\hat \\beta_{OLS}&=(X'X)^{-1}X'(X\\beta + e) \\\\\n &=(X'X)^{-1}X'X\\beta  + (X'X)^{-1}X'e \\\\\n &= \\beta + (X'X)^{-1}X'e\n\\end{aligned}\n$$\n\nWhich simply states that $\\hat\\beta$ will be unbiased if $E((X'X)^{-1}X'e)=0$. But other wise, when using samples, there will  be *small* differences because of the sampling error. \n\n## How precise are our estimates?\n\nThe last expression has an additional value. It shows that the estimated coefficient $\\hat\\beta$ can vary across samples, it also provides us a clue of how to measure the precision of that estimate.\n\nSpecifically, if we apply $Var()$ to both sides of the equation, we will have:\n\n$$\n\\begin{aligned}\nVar(\\hat\\beta_{OLS})=Var(\\beta + (X'X)^{-1}X'e) \\\\\nVar(\\hat\\beta_{OLS})=Var(\\beta) +Var((X'X)^{-1}X'e)  \n\\end{aligned}\n$$\n\nAssuming that $X's$ are fixed, we can rewrite this equation into the Variance covariance matrix we are accostume to:\n$$Var(\\hat\\beta_{OLS})=Var(\\beta) +(X'X)^{-1}X' Var(e) X (X'X)^{-1}\n$$\n\nWhere $Var(e)$ is not necessarity the ovarall variance of $e_i$. Instead is the $N\\times N$ variance convariance matrix that combines the information of all observations in a sample.\n\n$$\nVar(e) = \\left(\n\\begin{matrix}\n\\sigma^2_1 & \\sigma_{1,2} & \\sigma_{1,3} & ... &  \\sigma_{1,n} \\\\\n\\sigma_{1,2} & \\sigma_2^2 & \\sigma_{2,3} & ... &  \\sigma_{2,n} \\\\\n\\sigma_{1,3} & \\sigma_{2,3} & \\sigma^2_{3} & ...  & \\sigma_{3,n} \\\\\n... & ... & ... & ...& ...  \\\\\n\\sigma_{1,n} & \\sigma_{n,2} & \\sigma_{n,3} & ...  & \\sigma^2_{n} \\\\\n\\end{matrix}\\right)\n$$\n\nThis may be somewhat less intuive, and will require a bit of an stretch. In a crossectional sample, we only observe a unit once, but in theory, what we observe is just one of many possible states that unit could have been at. At each one of those, that unit may have received a different $e_i$. Thus, if we could see all those states, it would be possible to obtain units own unobservable variance $\\sigma^2_i$, and its correlation with other units $\\sigma_{ij}$.\n\nBut let me make one last change and substitute $Var(e)$ with $\\Omega$, so we have the more traditional formula:\n$$\nVar(\\hat\\beta_{OLS}) = \\color{brown}{(X'X)^{-1}} \\color{green}{X'} \\color{red} \\Omega \\color{green}{X}\\color{brown}{(X'X)^{-1}}\n$$\n\nWhich is usually described as the **Sandwich** formula. Because of this, I color coded the components.\n\n- $\\color{brown}{(X'X)^{-1}}$: Is the Bread\n- $\\color{green}{X'}$: Is the Lettuce \n- $\\color{red} \\Omega$: The best part of the **Sandwich**, the meat! And Of course, depending on the kind of **meat** we use, we would have different kind of sandwiches, or in this case different types of Standard errors.\n  \nSo let us go ahead and revise the different types of Standard erros we can get from this expression. In what follows, however, I will omit any degrees of freedom correction, to simplify some of the expressions\n\n## Homoskedastic Standard errors\n\nThe first type of Standard errors we learn in Econometrics 101, is the one based on the assumption of homoskedasticity and no cross-units correlation. This means that:\n\n- Unobservables $e$ that affect unit $i$ are completely independent from the unobservables that affect unit $j$.\n- And that the magnitud or variance of the unobservables is the same across all individuals.\n\nMathematically this means:\n$$\n\\begin{aligned}\n\\sigma_i^2 &=\\sigma_j^2 = \\sigma^2 \\\\\n\\sigma_{ij} &=0 \\ \\forall \\ i\\neq j\n\\end{aligned}\n$$\n\nWhich simplifies greatly the $\\Omega$ matrix:\n$$\n\\Omega_0 = \n\\left(\n\\begin{matrix}\n\\sigma^2 & 0 & 0 & ... &  0 \\\\\n0 & \\sigma^2 & 0 & ... &  0 \\\\\n0 & 0 & \\sigma^2 & ...  & 0 \\\\\n... & ... & ... & ...& ...  \\\\\n0 & 0 & 0 & ...  & \\sigma^2 \\\\\n\\end{matrix}\\right)= \\sigma^2 I(N) \n$$\n\nAnd of course simplyfies the Variance-covariance of the $\\beta$ coefficients to:\n\n$$Var(\\hat\\beta_{OLS})_0 = (X'X)^{-1} X'\\sigma^2 I(N) X (X'X)^{-1}=\\sigma^2 (X'X)^{-1}\n$$\n\n## Robust Standard errors\n\nNow, Robust standard errors, also known as Huber-White Robust standard errors, lifts one of the assumptions compared to the previous case. It explicitly allows for the variance of the errors to be different across individuals, while still imposing no correlation across individuals.\n$$\n\\begin{aligned}\n\\sigma_{ij} &=0 \\ \\forall \\ i\\neq j \\\\\n\\exists i,j &:\\sigma^2_{i}\\neq \\sigma^2_{j}\n\\end{aligned}\n$$\n\nThis does not mean that the variance will always be different when comparing two units, just that they *could* be different. With this conditions, we can partially simply the $\\Omega$ matrix. However, I will not show any further simplications for the variance of $\\beta's$\n$$\n\\Omega_1 =\n\\left(\n\\begin{matrix}\n\\sigma_1^2 & 0 & 0 & ... &  0 \\\\\n0 & \\sigma_2^2 & 0 & ... &  0 \\\\\n0 & 0 & \\sigma_3^2 & ...  & 0 \\\\\n... & ... & ... & ...& ...  \\\\\n0 & 0 & 0 & ...  & \\sigma_n^2 \\\\\n\\end{matrix}\\right)\n$$\n\n## 1-way Cluster Standard errors\n\nCluster standard errors lifts an additional assumption to how the variance covariance matrix should be estimated. In contrast with robust standard errors, cluster standard errors assumes that the unobservables are not necessarily independent across units.\n\nFor example, people within the same family will share similar experiences, thus their unobservables will be correlated. Same for students in the same classrooms, because they share the same \"teacher exprience\", or workers in the same firm, because they face the same work enviroment. \n\nWe, of course, still need to impose some restrictions on the data.\n\n- First, as most software was imposing until few years ago, we assume that there is only 1 dimension that individuals could be related to each other. \n- Second, we still need to assume that if individuals do not belong to the same \"group\", their errors are still independent.\n\nTo show this mathematically, let me define the function $g()$, which gives me the group membership of unit $i$. In other words, if unit $i$ belongs to family $k$, then $g(i)=k$. With this function, we can define the correlations across individuals to be defined as:\n$$\n\\begin{aligned}\n\\sigma_{i,j} \\neq 0 \\text{  if  } g(i)=g(j)\n\\end{aligned}\n$$\n\nIn this case, the $\\Omega$ matrix will look very similar to the one we use for robust standard errors. It will still be block diagonal, with the main diagonal having **block** elements different from zero ($\\Sigma_j$), and all other elements outside the diagonal to be zero:\n$$\n\\Omega_2 =\n\\left(\n\\begin{matrix}\n\\sigma_1^2     & \\sigma_{1,2} & 0         & ...  &  0 \\\\\n\\sigma_{1,2} & \\sigma_2^2     & 0         & ...  &  0 \\\\\n0            & 0            & \\sigma_3^2  & ...  & \\sigma_{3,n} \\\\\n...          & ...          & ...       & ...  & ...  \\\\\n0            & 0            & \\sigma_{3,n}     & ...  & \\sigma_n^2 \\\\\n\\end{matrix}\\right) =\n\\left(\n\\begin{matrix}\n\\Sigma_1     & 0            \\\\\n0            & \\Sigma_2      \\\\\n\\end{matrix}\\right)\n$$\n\nVisually, this would look like the following\n\n::: {#a591d85e .cell execution_count=1}\n``` {.stata .cell-code code-fold=\"true\"}\nclear\nset scheme white2\ncolor_style tableau\nset seed 1\nset obs 50\ngen r1=runiformint(1,4)\ngen r2=runiformint(1,4)\ngen id=_n\nsort r1  r2\nqui:mata:\nr1=st_data(.,\"r1\")\nr2=st_data(.,\"r2\")\nrr1=J(rows(r1)*rows(r2),4,0)\nk=0\nfor(i=1;i<=50;i++){\n\tfor(j=1;j<=50;j++){\n\t\tif ((r1[i]==r1[j]) | (r2[i]==r2[j])) {\n\t\t\tk++\n\t\t\trr1[k,]=(51-i,j,(r1[i]==r1[j]),(r2[i]==r2[j]) )\t\t\t\n\t\t}\n\t}\t\n}\nrr1=rr1[1..k,]\nend\ngetmata rr1*=rr1, replace force\n\ntwo (scatter rr11 rr12 if rr13==1,  ms(s) msize(2.1))  ///\n    (scatter rr11 rr12 if 51-rr11 == rr12, ms(s) msize(2.1) color(gs1)  ) ///\n\t, aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<style>div.jp-Notebook .datagrid-container {min-height: 448px; }</style>\n```\n\nCorrelation matrix $\\Omega$\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](app_metrics7_files/figure-html/cell-2-output-3.png){}\n:::\n:::\n\n\nIn this figure, the blue blocks shows that we are allowing for some non-zero correlation among observations that belong to the same group or **cluster**. Implicily, we are also allowing for all elements of the main diagonal (in black) to be different from zero, which is why Cluster standard errors are also robust to heteroskedasticity.\n\n## 2-way Cluster Standard Errors\n\nWhat about 2-way, or for even M-way, clustered standard errors?\n\nAs I mentioned above, 1-way clustered standard errors allows for individuals unobserved errors to be correlated across each other, but only if they belong to the same group. If they are from different groups, then no correlation should exists. 1-Way cluster standard errors also assumes that there is only one set of groups that people can be connected through. \n\nThis assumption is also quite strong. People, for example, are interconnected through many chanels. My unobserved factors are likely to be related to my wife's and daughter's unobservables, because we belong to the same household. However, they may also be related to my co-workers, because we all have the same workplace, thus share similar experience. I cannot discard my grad-school peers, since we have shared experiences (and thus correlated unobservables). Some of this connections would be new and unique, but some others may overlap. \n\nThe good news is that 2-way cluster standard errors, or M-way, does allow for this more complex scenario. Specifically, if two individuals are part of at least 1 common group, then we will assume the correlation of unobserved factors is different from zero. Unfortunately, we cannot differentiate which **connection** is the one that is driven that correlation.\n\nTo show this mathematically, let me define the function $g_h()$, which indicates the group, among the set $h$ (cluster variable), that a unit belongs to, and $gg(i,j)$ which takes the value of zero if $i$ and $j$ are not connected to each other in any way (based on the cluster variables), and 1 otherwise. \n$$\n\\begin{aligned}\ngg(i,j)&=0 \\text{  if  } \\forall h:g_h(i)\\neq  g_h(j) \\\\\n&\\text{ and 1 otherwise}\n\\end{aligned}\n$$\n\nIf this is the case, $\\sigma_{ij}=0$ is assumed zero if $gg(i,j)=0$, but allow to vary otherwise.\n\nThe implications that it has on the $\\Omega$ matrix is that it will no longer be block diagonal, because elements outside the main diagonal (and block diagonal) will be different from zero.\n$$\\Omega_3 =\n\\left(\n\\begin{matrix}\n\\sigma_1^2     & \\sigma_{1,2} & 0         & ...  &  \\sigma_{1,n} \\\\\n\\sigma_{1,2} & \\sigma_2^2     & \\sigma_{2,3}         & ...  &  0 \\\\\n0            & \\sigma_{2,3}   & \\sigma_3^2  & ...  & \\sigma_{3,n} \\\\\n...          & ...            & ...       & ...  & ...  \\\\\n\\sigma_{1,n} & 0            & \\sigma_{3,n}     & ...  & \\sigma_n^2 \\\\\n\\end{matrix}\\right)\n$$\n\nIts perhaps easier to show this visually:\n\n::: {#fig-corr .cell execution_count=2}\n``` {.stata .cell-code code-fold=\"true\"}\ntwo (scatter rr11 rr12 if rr13==1,  ms(s) msize(2.1))  ///\n\t, aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off) name(m1, replace) \n\ntwo (scatter rr11 rr12 if rr14==1,  ms(s) msize(2.1))  ///\n\t, aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off) name(m2, replace)    \n\ntwo (scatter rr11 rr12 if rr14==1 | rr13==1,  ms(s) msize(2.1))  ///\n\t, aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off) name(m3, replace)\n```\n\n::: {.cell-output .cell-output-display}\n![1-way cluster](app_metrics7_files/figure-html/fig-corr-output-1.png){#fig-corr-1 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![2nd 1-way cluster](app_metrics7_files/figure-html/fig-corr-output-2.png){#fig-corr-2 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![2-way cluster](app_metrics7_files/figure-html/fig-corr-output-3.png){#fig-corr-3 fig-align='center'}\n:::\n\nCorrelation matrix $\\Omega$\n:::\n\n\n@fig-corr shows the correlation structure we would observe considering the 2 clustered variables. @fig-corr-1 and @fig-corr-2 show the interconnections based on a first and a second cluster variables, whereas @fig-corr-3 considers the combination of both. To estimate Standard errors we need to allow the *blue* cells to be different from zero, impossing the zero condition only on the \"unconnected\" pairs.\n\n## How to estimate $\\sigma_{ij}$?\n\nUp to this point, I have shown how one should think about the estimation of Standard errors. But you may also be interested in the practical approach. Specifically, how do we come up with estimates for $\\sigma_i^2$ and $\\sigma_{ij}$.?\n\nThis is actually straight forward!. Since we only see each unit once in crossection, our best estimation of terms is given by:\n$$\n\\hat \\sigma_{ij} = \\varepsilon_i*\\varepsilon_j\n$$\n\nwhere $\\varepsilon_i$ is the unit specific unobserved component. So that is pretty much it! once you have $\\hat \\sigma_{ij}$ you simply need to define which assumption you want to use (homoskedastic errors, heteroskedastic, or clustered), identify which elements you will assume to be zero, and voila!...\n\nOr almost. While what I describe here is technically correct, there are two limitations to its application compared to how is it done in most software.\n\n1. I'm disregarding completely the role of degrees of freedom and corresponding corrections. \n2. The estimation of $\\hat\\Omega$ as described here is only feasible for small datasets, because it suggest creating a $N\\times N$ matrix. This would easily consume all the resources of your computer.\n   \nRegarding point 2. While there are other ways to simply the math, thinking in terms of the unrestricted $\\Omega$ is useful to understand the role of clustering and two way clustering in the data.\n\n## `Stata` Example\n\nHere I will work on a small example that would implement the 4 approaches to estimating standard errors. Because I will use the unrestrict $\\hat\\Omega$ matrix, I will work with a very small dataset: `auto'. The next contains lots of code, so feel free to skip it.\n\n::: {#7ecef941 .cell execution_count=3}\n``` {.stata .cell-code code-fold=\"false\"}\n** Load Data, create clusters and a constant\nsysuse auto, clear\nset seed 1\nreplace price = price / 1000\ngen one = 1\ngen c1  = runiformint(1,5)\ngen c2  = runiformint(1,5)\n** Load Data to Mata to easy manipulation\nmata: y = st_data(.,\"price\")\nmata: x = st_data(.,\"mpg one\")\n** estimate betas, ixx, and erros\nmata: xx = cross(x,x); ixx = invsym(xx);xy=cross(x,y)\nmata: b  = ixx * xy ;  e   = y:-x*b\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(1978 automobile data)\nvariable price was int now float\n(74 real changes made)\n```\n:::\n:::\n\n\nHomoskedastic errors: \n\n::: {#5462198f .cell execution_count=4}\n``` {.stata .cell-code code-fold=\"false\"}\nmata: omega_0 = I(74)*mean(e:^2)\nmata: vcv_0   = ixx * x' * omega_0 * x * ixx\nmata: svcv_0  = diagonal(vcv_0):^.5;svcv_0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 1\n    +---------------+\n  1 |  .0523545211  |\n  2 |  1.154882583  |\n    +---------------+\n```\n:::\n:::\n\n\nRobust Standard errors\n\n::: {#e90f2897 .cell execution_count=5}\n``` {.stata .cell-code code-fold=\"false\"}\nmata: omega_1 = (e*e'):*I(74)\nmata: vcv_1   = ixx * x' * omega_1 * x * ixx\nmata: svcv_1  = diagonal(vcv_1):^.5;svcv_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 1\n    +---------------+\n  1 |  .0566949707  |\n  2 |   1.35766526  |\n    +---------------+\n```\n:::\n:::\n\n\n1-way Cluster Standard errors:\n\n::: {#0ebb1c93 .cell execution_count=6}\n``` {.stata .cell-code code-fold=\"false\"}\nmata: cc = st_data(.,\"c1 c2\")\nmata: i1 = J(1,74,cc[,1]):==J(74,1,cc[,1]') \nmata: i2 = J(1,74,cc[,2]):==J(74,1,cc[,2]') \nmata: omega_2a = (e*e'):*i1; omega_2b = (e*e'):*i2\nmata: vcv_2a   = ixx * x' * omega_2a * x * ixx\nmata: vcv_2b   = ixx * x' * omega_2b * x * ixx\nmata: svcv_2a  = diagonal(vcv_2a):^.5\nmata: svcv_2b  = diagonal(vcv_2b):^.5\nmata: svcv_2a, svcv_2b \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 1             2\n    +-----------------------------+\n  1 |  .0816625514   .0293288459  |\n  2 |  2.010064686   .6935766918  |\n    +-----------------------------+\n```\n:::\n:::\n\n\n2-way Cluster Standard errors\n\n::: {#9c1326e2 .cell execution_count=7}\n``` {.stata .cell-code code-fold=\"false\"}\nmata: i3 = i1:|i2\nmata: omega_3 = (e*e'):*i3\nmata: vcv_3   = ixx * x' * omega_3 * x * ixx\nmata: svcv_3  = diagonal(vcv_3):^.5; svcv_3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 1\n    +---------------+\n  1 |  .0595359543  |\n  2 |  1.523244536  |\n    +---------------+\n```\n:::\n:::\n\n\nBut for the 2-way Cluster Standard errors you also have the more commonly used formula:\n\n::: {#82c74bbd .cell execution_count=8}\n``` {.stata .cell-code code-fold=\"false\"}\n** First we need to create a variable that combines both clusters\nmata: i4 = i1:&i2\nmata: omega_4 = (e*e'):*i4\nmata: vcv_aux   = ixx * x' * omega_4 * x * ixx\nmata: vcv_4 = vcv_2a + vcv_2b - vcv_aux\nmata: svcv_4  = diagonal(vcv_4):^.5; svcv_4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 1\n    +---------------+\n  1 |  .0595359543  |\n  2 |  1.523244536  |\n    +---------------+\n```\n:::\n:::\n\n\n## Conclusions\n\nIn this note, I provide a walkthrough the estimation of different types of standard errors from a linear regression model.\n\nWhile most textbooks already cover the basics of standard errors under homoskedasticity, heteroskedasticity, and oneway clustered errors, I have not seen an intuitive approach to understanding w-way clustered standard errors. \n\nAs I shown here, the idea of 2-way, or M-way, clustered standard errors is to allow arbitrary correlations across individuals, if they have to at least one common group (one cluster variable). \n\nNow, one may be tempted to try and use this approach to allow for unconstrained correlation across all units. After all we are all connected to some way. However, because we do not see the true unobserved variance-covariance matrix, the variance of $\\beta$ would not be identified without impossing restrictions on $\\Omega$.\n\nIf you are interested in learning more about this topic, I would recommend you to read @cameron_robust_2011, @mackinnon_cluster-robust_2023 and @abadie_when_2022.\n\n",
    "supporting": [
      "app_metrics7_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}
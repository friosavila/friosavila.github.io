{
  "hash": "79d5e05d1e858a05c0a72f18af8787a9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Constructing synthetic Datasets\"\nsubtitle: \"How to come public, with private data\"\nformat: html\nbibliography: references.bib\nnocite: |\n  @jenkins_rios_2020, @jenkins_rios_2021\n---\n\n## Introduction \n\nIn my current collaboration with Stephen Jenkins, we are grappling with the challenge of providing a self-contained replication package alongside our paper.\n\nIt's relatively easy to share the code for our model estimations, including code developed by other authors. However, many researchers face the same challenge we do: how to distribute data that we're not allowed to share due to privacy or proprietary reasons.\n\nIn fact, for this particular project, only Stephen has had access to the data. I've mainly worked on the code that estimates new models (for those interested, see references below).\n\nNow that it's time to publish our \"big\" paper, we need a strategy to create a synthetic dataset that satisfies privacy protection constraints while still preserving the moments' structure we care about, as well as those that others may find interesting.\n\nTo this end, I propose a simple strategy that could work: Multiple Imputation. While it may not be the best method available, I welcome any feedback or suggestions.\n\nTo explain how the method works, I'll use the Swiss Labor Market Survey 1998 dataset, which is publicly available and used as an example dataset in the command -oaxaca- [@jann_2008].\n\n## The Problem\n\nAssume you signed a confidentiality agreement to work with Swiss Survey data and are ready to submit your work. However, you are required to provide a replication package with a code to produce the tables and the dataset itself. Since you cannot share the original data, you suggest generating 5 synthetic datasets instead. By doing so, people can apply your code and reach similar conclusions to your main paper, but with the advantage that the data is simulated, thus fulfilling privacy concerns.\n\nHere is a piece of code that can be used for that:\n\n::: {#588a8f22 .cell execution_count=1}\n``` {.stata .cell-code}\nframe reset\nset linesize 255\nuse http://fmwww.bc.edu/RePEc/bocode/o/oaxaca.dta, clear\nmisstable summarize\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<style>div.jp-Notebook .datagrid-container {min-height: 448px; }</style>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(Excerpt from the Swiss Labor Market Survey 1998)\n                                                               Obs<.\n                                                +------------------------------\n               |                                | Unique\n      Variable |     Obs=.     Obs>.     Obs<.  | values        Min         Max\n  -------------+--------------------------------+------------------------------\n        lnwage |       213               1,434  |   >500    .507681    5.259097\n         exper |       213               1,434  |   >500          0    49.16667\n        tenure |       213               1,434  |    323          0    44.83333\n          isco |       213               1,434  |      9          1           9\n  -----------------------------------------------------------------------------\n```\n:::\n:::\n\n\nFour variables (Wages, tenure, experience, and ISCO) have missing data when `lfp=0` (people are not working). \n\n## The solution\n\nThe first step is to decide on the size of the synthetic dataset. You could create a dataset with the same number of observations or adjust it to your desired sample size. I will expand the dataset to double the size, tag the new observations and make all variables, except for `lfp`, missing. This is because some data is missing as it's only available for those in the labor force. Alternatively, you could have created `lfp` using a random draw from a Bernoulli distribution with the same probability as the original data.\n\n::: {#72477c83 .cell execution_count=2}\n``` {.stata .cell-code}\nexpand 2, gen(tag)\nforeach i of varlist lnwage educ exper tenure isco female age single married divorced kids6 kids714 wt {\n  qui:replace `i'=. if tag==1\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(1,647 observations created)\n```\n:::\n:::\n\n\nNext, create multiple imputed datasets using the predictive mean matching strategy. To do this, set the data and register all variables to be imputed. Then, impute all variables using chain `pmm`. Make sure none of the variables are collinear, and variables with structural missing data are specified separately. The only explanatory variable or exogenous variable here is `lfp`.\n\n::: {#374a4fd5 .cell execution_count=3}\n``` {.stata .cell-code}\nmi set wide\nmi register impute lnwage educ exper tenure isco female age single married kids6 kids714 wt\nset seed 101\nqui:mi impute chain (pmm, knn(20))  educ female age single married kids6 kids714 wt (pmm if lfp==1, knn(20) ) lnwage  exper tenure isco  = lfp, add(5)\n```\n:::\n\n\nYou now have 5 sets of variables that can be used to create unique synthetic datasets with a similar structure to the original confidential dataset. Let's now put the newly created data into frames, so we can estimate few models and compare them with the original data.\n\n::: {#e370fc23 .cell execution_count=4}\n``` {.stata .cell-code}\nforvalues i = 1/5 {\n  frame put _`i'_* lfp if tag==1, into(fr`i')\n  frame fr`i':ren _`i'_* *\n}\nuse http://fmwww.bc.edu/RePEc/bocode/o/oaxaca.dta, clear\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Excerpt from the Swiss Labor Market Survey 1998)\n```\n:::\n:::\n\n\n## Comparing Results\n\nYou can now estimate 4 models using the original data and the synthetic data. \n\n\n\nNow lets compare the models:\n\n**Linear Regression**\n\n\n|            |  Original                  |     Fake1                  |     Fake2                  |     Fake3                  |     Fake4                  |     Fake5                  |\n| ---------- | :------------------------: | :------------------------: | :------------------------: | :------------------------: | :------------------------: | :------------------------: |\n| educ       |     0.085<sup>\\*\\*\\*</sup> |     0.076<sup>\\*\\*\\*</sup> |     0.059<sup>\\*\\*\\*</sup> |     0.077<sup>\\*\\*\\*</sup> |     0.069<sup>\\*\\*\\*</sup> |     0.063<sup>\\*\\*\\*</sup> |\n|            |   (0.005)                  |   (0.005)                  |   (0.006)                  |   (0.005)                  |   (0.005)                  |   (0.005)                  |\n| exper      |     0.011<sup>\\*\\*\\*</sup> |     0.010<sup>\\*\\*\\*</sup> |     0.011<sup>\\*\\*\\*</sup> |     0.012<sup>\\*\\*\\*</sup> |     0.006<sup>\\*\\*\\*</sup> |     0.009<sup>\\*\\*\\*</sup> |\n|            |   (0.002)                  |   (0.001)                  |   (0.002)                  |   (0.002)                  |   (0.001)                  |   (0.001)                  |\n| tenure     |     0.008<sup>\\*\\*\\*</sup> |     0.008<sup>\\*\\*\\*</sup> |     0.002                  |     0.005<sup>\\*\\*</sup>   |     0.007<sup>\\*\\*\\*</sup> |     0.005<sup>\\*\\*</sup>   |\n|            |   (0.002)                  |   (0.002)                  |   (0.002)                  |   (0.002)                  |   (0.002)                  |   (0.002)                  |\n| female     |    -0.084<sup>\\*\\*\\*</sup> |    -0.027                  |    -0.136<sup>\\*\\*\\*</sup> |    -0.063<sup>\\*</sup>     |    -0.056<sup>\\*</sup>     |    -0.113<sup>\\*\\*\\*</sup> |\n|            |   (0.025)                  |   (0.025)                  |   (0.026)                  |   (0.025)                  |   (0.023)                  |   (0.024)                  |\n| \\_cons     |     2.213<sup>\\*\\*\\*</sup> |     2.297<sup>\\*\\*\\*</sup> |     2.580<sup>\\*\\*\\*</sup> |     2.336<sup>\\*\\*\\*</sup> |     2.464<sup>\\*\\*\\*</sup> |     2.529<sup>\\*\\*\\*</sup> |\n|            |   (0.068)                  |   (0.068)                  |   (0.074)                  |   (0.070)                  |   (0.063)                  |   (0.064)                  |\n| *N*        |      1434                  |      1434                  |      1434                  |      1434                  |      1434                  |      1434                  |\n\nStandard errors in parentheses<br>\n<sup>\\*</sup> *p* < 0.05, <sup>\\*\\*</sup> *p* < 0.01, <sup>\\*\\*\\*</sup> *p* < 0.001\n\n**Quantile Regression 10**\n\n\n|            |  Original                  |     Fake1                  |     Fake2                  |     Fake3                  |     Fake4                  |     Fake5                  |\n| ---------- | :------------------------: | :------------------------: | :------------------------: | :------------------------: | :------------------------: | :------------------------: |\n| educ       |     0.103<sup>\\*\\*\\*</sup> |     0.088<sup>\\*\\*\\*</sup> |     0.069<sup>\\*\\*\\*</sup> |     0.083<sup>\\*\\*\\*</sup> |     0.075<sup>\\*\\*\\*</sup> |     0.067<sup>\\*\\*\\*</sup> |\n|            |   (0.017)                  |   (0.015)                  |   (0.018)                  |   (0.011)                  |   (0.011)                  |   (0.013)                  |\n| exper      |     0.020<sup>\\*\\*\\*</sup> |     0.012<sup>\\*\\*</sup>   |     0.014<sup>\\*\\*</sup>   |     0.012<sup>\\*\\*\\*</sup> |     0.008<sup>\\*</sup>     |     0.009<sup>\\*</sup>     |\n|            |   (0.005)                  |   (0.004)                  |   (0.005)                  |   (0.003)                  |   (0.003)                  |   (0.004)                  |\n| tenure     |     0.001                  |     0.006                  |     0.004                  |     0.002                  |     0.006                  |     0.010<sup>\\*</sup>     |\n|            |   (0.006)                  |   (0.005)                  |   (0.006)                  |   (0.004)                  |   (0.004)                  |   (0.005)                  |\n| female     |    -0.151                  |     0.022                  |    -0.128                  |    -0.019                  |    -0.161<sup>\\*\\*</sup>   |    -0.154<sup>\\*</sup>     |\n|            |   (0.081)                  |   (0.070)                  |   (0.079)                  |   (0.053)                  |   (0.054)                  |   (0.063)                  |\n| \\_cons     |     1.462<sup>\\*\\*\\*</sup> |     1.681<sup>\\*\\*\\*</sup> |     1.939<sup>\\*\\*\\*</sup> |     1.835<sup>\\*\\*\\*</sup> |     1.971<sup>\\*\\*\\*</sup> |     1.994<sup>\\*\\*\\*</sup> |\n|            |   (0.219)                  |   (0.193)                  |   (0.228)                  |   (0.149)                  |   (0.146)                  |   (0.171)                  |\n| *N*        |      1434                  |      1434                  |      1434                  |      1434                  |      1434                  |      1434                  |\n\nStandard errors in parentheses<br>\n<sup>\\*</sup> *p* < 0.05, <sup>\\*\\*</sup> *p* < 0.01, <sup>\\*\\*\\*</sup> *p* < 0.001\n\n**Quantile Regression 90**\n\n\n|            |  Original                  |     Fake1                  |     Fake2                  |     Fake3                  |     Fake4                  |     Fake5                  |\n| ---------- | :------------------------: | :------------------------: | :------------------------: | :------------------------: | :------------------------: | :------------------------: |\n| educ       |     0.064<sup>\\*\\*\\*</sup> |     0.073<sup>\\*\\*\\*</sup> |     0.047<sup>\\*\\*\\*</sup> |     0.069<sup>\\*\\*\\*</sup> |     0.062<sup>\\*\\*\\*</sup> |     0.057<sup>\\*\\*\\*</sup> |\n|            |   (0.009)                  |   (0.008)                  |   (0.009)                  |   (0.008)                  |   (0.008)                  |   (0.007)                  |\n| exper      |     0.004                  |     0.009<sup>\\*\\*\\*</sup> |     0.009<sup>\\*\\*\\*</sup> |     0.011<sup>\\*\\*\\*</sup> |     0.003                  |     0.005<sup>\\*\\*</sup>   |\n|            |   (0.003)                  |   (0.002)                  |   (0.003)                  |   (0.002)                  |   (0.002)                  |   (0.002)                  |\n| tenure     |     0.008<sup>\\*</sup>     |     0.009<sup>\\*\\*\\*</sup> |    -0.001                  |     0.008<sup>\\*\\*</sup>   |     0.012<sup>\\*\\*\\*</sup> |     0.005                  |\n|            |   (0.003)                  |   (0.003)                  |   (0.003)                  |   (0.003)                  |   (0.003)                  |   (0.003)                  |\n| female     |    -0.054                  |    -0.054                  |    -0.149<sup>\\*\\*\\*</sup> |    -0.052                  |    -0.009                  |    -0.106<sup>\\*\\*</sup>   |\n|            |   (0.044)                  |   (0.035)                  |   (0.041)                  |   (0.039)                  |   (0.039)                  |   (0.033)                  |\n| \\_cons     |     2.984<sup>\\*\\*\\*</sup> |     2.804<sup>\\*\\*\\*</sup> |     3.247<sup>\\*\\*\\*</sup> |     2.863<sup>\\*\\*\\*</sup> |     2.990<sup>\\*\\*\\*</sup> |     3.121<sup>\\*\\*\\*</sup> |\n|            |   (0.119)                  |   (0.097)                  |   (0.118)                  |   (0.111)                  |   (0.106)                  |   (0.089)                  |\n| *N*        |      1434                  |      1434                  |      1434                  |      1434                  |      1434                  |      1434                  |\n\nStandard errors in parentheses<br>\n<sup>\\*</sup> *p* < 0.05, <sup>\\*\\*</sup> *p* < 0.01, <sup>\\*\\*\\*</sup> *p* < 0.001\n\n**Heckman selection model**\n\n\n|            |  Original                  |     Fake1                  |     Fake2                  |     Fake3                  |     Fake4                  |     Fake5                  |\n| ---------- | :------------------------: | :------------------------: | :------------------------: | :------------------------: | :------------------------: | :------------------------: |\n| lnwage     |                            |                            |                            |                            |                            |                            |\n| educ       |     0.072<sup>\\*\\*\\*</sup> |     0.066<sup>\\*\\*\\*</sup> |     0.052<sup>\\*\\*\\*</sup> |     0.068<sup>\\*\\*\\*</sup> |     0.059<sup>\\*\\*\\*</sup> |     0.057<sup>\\*\\*\\*</sup> |\n|            |   (0.005)                  |   (0.005)                  |   (0.006)                  |   (0.005)                  |   (0.005)                  |   (0.005)                  |\n| exper      |     0.002                  |     0.001                  |    -0.000                  |     0.004<sup>\\*</sup>     |    -0.002                  |     0.002                  |\n|            |   (0.002)                  |   (0.002)                  |   (0.002)                  |   (0.002)                  |   (0.002)                  |   (0.002)                  |\n| tenure     |     0.002                  |     0.003                  |    -0.002                  |    -0.000                  |     0.002                  |     0.001                  |\n|            |   (0.002)                  |   (0.002)                  |   (0.002)                  |   (0.002)                  |   (0.002)                  |   (0.002)                  |\n| female     |    -0.105<sup>\\*\\*\\*</sup> |    -0.067<sup>\\*\\*</sup>   |    -0.189<sup>\\*\\*\\*</sup> |    -0.105<sup>\\*\\*\\*</sup> |    -0.096<sup>\\*\\*\\*</sup> |    -0.144<sup>\\*\\*\\*</sup> |\n|            |   (0.029)                  |   (0.025)                  |   (0.026)                  |   (0.025)                  |   (0.024)                  |   (0.024)                  |\n| age        |     0.015<sup>\\*\\*\\*</sup> |     0.013<sup>\\*\\*\\*</sup> |     0.015<sup>\\*\\*\\*</sup> |     0.012<sup>\\*\\*\\*</sup> |     0.013<sup>\\*\\*\\*</sup> |     0.011<sup>\\*\\*\\*</sup> |\n|            |   (0.002)                  |   (0.002)                  |   (0.002)                  |   (0.002)                  |   (0.002)                  |   (0.002)                  |\n| \\_cons     |     1.991<sup>\\*\\*\\*</sup> |     2.071<sup>\\*\\*\\*</sup> |     2.226<sup>\\*\\*\\*</sup> |     2.133<sup>\\*\\*\\*</sup> |     2.201<sup>\\*\\*\\*</sup> |     2.317<sup>\\*\\*\\*</sup> |\n|            |   (0.073)                  |   (0.073)                  |   (0.081)                  |   (0.075)                  |   (0.069)                  |   (0.070)                  |\n| lfp        |                            |                            |                            |                            |                            |                            |\n| educ       |     0.149<sup>\\*\\*\\*</sup> |     0.055<sup>\\*\\*</sup>   |     0.024                  |     0.018                  |     0.064<sup>\\*\\*\\*</sup> |     0.028                  |\n|            |   (0.028)                  |   (0.020)                  |   (0.020)                  |   (0.019)                  |   (0.019)                  |   (0.018)                  |\n| female     |    -1.785<sup>\\*\\*\\*</sup> |    -0.074                  |    -0.246<sup>\\*\\*</sup>   |    -0.104                  |    -0.011                  |    -0.177                  |\n|            |   (0.161)                  |   (0.091)                  |   (0.091)                  |   (0.088)                  |   (0.090)                  |   (0.090)                  |\n| age        |    -0.039<sup>\\*\\*\\*</sup> |    -0.021<sup>\\*\\*\\*</sup> |    -0.023<sup>\\*\\*\\*</sup> |    -0.018<sup>\\*\\*\\*</sup> |    -0.025<sup>\\*\\*\\*</sup> |    -0.023<sup>\\*\\*\\*</sup> |\n|            |   (0.007)                  |   (0.005)                  |   (0.005)                  |   (0.005)                  |   (0.005)                  |   (0.005)                  |\n| single     |    -0.100                  |    -0.792<sup>\\*\\*</sup>   |    -0.586<sup>\\*\\*</sup>   |    -0.498<sup>\\*\\*</sup>   |    -0.863<sup>\\*\\*\\*</sup> |    -0.780<sup>\\*\\*</sup>   |\n|            |   (0.231)                  |   (0.241)                  |   (0.201)                  |   (0.186)                  |   (0.229)                  |   (0.238)                  |\n| married    |    -0.867<sup>\\*\\*\\*</sup> |    -0.929<sup>\\*\\*\\*</sup> |    -0.765<sup>\\*\\*\\*</sup> |    -0.489<sup>\\*\\*</sup>   |    -0.882<sup>\\*\\*\\*</sup> |    -1.017<sup>\\*\\*\\*</sup> |\n|            |   (0.158)                  |   (0.219)                  |   (0.169)                  |   (0.160)                  |   (0.198)                  |   (0.213)                  |\n| kids6      |    -0.716<sup>\\*\\*\\*</sup> |    -0.730<sup>\\*\\*\\*</sup> |    -0.648<sup>\\*\\*\\*</sup> |    -0.699<sup>\\*\\*\\*</sup> |    -0.790<sup>\\*\\*\\*</sup> |    -0.714<sup>\\*\\*\\*</sup> |\n|            |   (0.082)                  |   (0.067)                  |   (0.064)                  |   (0.066)                  |   (0.069)                  |   (0.062)                  |\n| kids714    |    -0.343<sup>\\*\\*\\*</sup> |    -0.378<sup>\\*\\*\\*</sup> |    -0.373<sup>\\*\\*\\*</sup> |    -0.281<sup>\\*\\*\\*</sup> |    -0.402<sup>\\*\\*\\*</sup> |    -0.201<sup>\\*\\*\\*</sup> |\n|            |   (0.065)                  |   (0.059)                  |   (0.056)                  |   (0.058)                  |   (0.058)                  |   (0.058)                  |\n| \\_cons     |     3.543<sup>\\*\\*\\*</sup> |     2.720<sup>\\*\\*\\*</sup> |     3.006<sup>\\*\\*\\*</sup> |     2.560<sup>\\*\\*\\*</sup> |     2.729<sup>\\*\\*\\*</sup> |     3.135<sup>\\*\\*\\*</sup> |\n|            |   (0.486)                  |   (0.382)                  |   (0.388)                  |   (0.345)                  |   (0.383)                  |   (0.383)                  |\n| /mills     |                            |                            |                            |                            |                            |                            |\n| lambda     |    -0.123                  |     0.128<sup>\\*</sup>     |     0.251<sup>\\*\\*\\*</sup> |     0.062                  |     0.206<sup>\\*\\*\\*</sup> |     0.077                  |\n|            |   (0.065)                  |   (0.061)                  |   (0.065)                  |   (0.070)                  |   (0.057)                  |   (0.061)                  |\n| *N*        |      1647                  |      1647                  |      1647                  |      1647                  |      1647                  |      1647                  |\n\nStandard errors in parentheses<br>\n<sup>\\*</sup> *p* < 0.05, <sup>\\*\\*</sup> *p* < 0.01, <sup>\\*\\*\\*</sup> *p* < 0.001\n\n\n\nI wont spend too much time interpretting the models. However, it is important to notice that they do provide similar outcomes, except for quantile regressions. Still, with this, one could redistribute replication codes that use both the true data and synthetic data, providing transparency to the work.\n\n## Conclusions\n\nAs evident from the analysis, the results of the synthetic dataset are not expected to perfectly replicate the original data due to the introduction of random errors. However, by keeping this in mind, we can create synthetic datasets like this one, along with two sets of results - one based on the actual data, and the other based on the synthetic dataset(s).\n\nThis should help providing replication packages with code and data, improving the transparency of research when using restricted data.\n\n## References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "app_metrics1_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}
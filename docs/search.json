[
  {
    "objectID": "stata_do/stata_do6.html",
    "href": "stata_do/stata_do6.html",
    "title": "Marginal effects with nonlinear transformations: f_able",
    "section": "",
    "text": "Note\n\n\n\nIf you are interested only on the program f_able, please jump to the last section\n\n\nIn both classroom discussions and online forums like statalist, a recurring question revolves around the estimation of marginal effects within models that incorporate nonlinear transformations of key variables.\nThis question is challenging because we, the analalists, are more familiar with model specifications that exclusively involve variables in their original forms. Few of us dare to explore the inclusion of quadratic or cubic terms, or the introduction of interactions among continuous variables. In the past, calculating marginal effects under such circumstances required additional effort: computing partial derivatives, and obtaining both point estimates and standard errors.\nFor linear models, things are simple, as marginal effects are constant and equal to their coefficients. Even in non-linear models, where the effect varies based on characteristics, estimating marginal effects remains relatively easy, either manually or with the help of most software tools.\nStarting from version 14, Stata is able to compute marginal effects when interactions and polynomials are present, using the margins command, as long as the formulations are expressed in a manner that Stata can understand. Virtually all official commands, as well as numerous community-contributed ones, offer this functionality. However, when dealing with more complex transformations, like fractional polynomials or spline bases, manual derivation of these effects are still necessary.\nBut surely, there’s a more efficient way!\nIn Rios-Avila (2021), I introduced a user-friendly command designed to simplify the estimation of marginal effects, regardless of the specific functional forms applied to the data, that can be used with most commands. In the following sections, I’ll provide an overview of the correct approach to estimating marginal effects, describe how the margins command operates, and explain how you can employ f_able to calculate marginal effects for models that might otherwise pose challenges.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Marginal effects with nonlinear transformations: `f_able`"
    ]
  },
  {
    "objectID": "stata_do/stata_do6.html#introduction",
    "href": "stata_do/stata_do6.html#introduction",
    "title": "Marginal effects with nonlinear transformations: f_able",
    "section": "",
    "text": "Note\n\n\n\nIf you are interested only on the program f_able, please jump to the last section\n\n\nIn both classroom discussions and online forums like statalist, a recurring question revolves around the estimation of marginal effects within models that incorporate nonlinear transformations of key variables.\nThis question is challenging because we, the analalists, are more familiar with model specifications that exclusively involve variables in their original forms. Few of us dare to explore the inclusion of quadratic or cubic terms, or the introduction of interactions among continuous variables. In the past, calculating marginal effects under such circumstances required additional effort: computing partial derivatives, and obtaining both point estimates and standard errors.\nFor linear models, things are simple, as marginal effects are constant and equal to their coefficients. Even in non-linear models, where the effect varies based on characteristics, estimating marginal effects remains relatively easy, either manually or with the help of most software tools.\nStarting from version 14, Stata is able to compute marginal effects when interactions and polynomials are present, using the margins command, as long as the formulations are expressed in a manner that Stata can understand. Virtually all official commands, as well as numerous community-contributed ones, offer this functionality. However, when dealing with more complex transformations, like fractional polynomials or spline bases, manual derivation of these effects are still necessary.\nBut surely, there’s a more efficient way!\nIn Rios-Avila (2021), I introduced a user-friendly command designed to simplify the estimation of marginal effects, regardless of the specific functional forms applied to the data, that can be used with most commands. In the following sections, I’ll provide an overview of the correct approach to estimating marginal effects, describe how the margins command operates, and explain how you can employ f_able to calculate marginal effects for models that might otherwise pose challenges.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Marginal effects with nonlinear transformations: `f_able`"
    ]
  },
  {
    "objectID": "stata_do/stata_do6.html#marginal-effects-analytical-approach",
    "href": "stata_do/stata_do6.html#marginal-effects-analytical-approach",
    "title": "Marginal effects with nonlinear transformations: f_able",
    "section": "Marginal effects: Analytical approach",
    "text": "Marginal effects: Analytical approach\nLets start with a simple linear model:\n\\[\\begin{aligned}\ny &=\\beta_0+\\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + e \\\\\nE(y|x) &= \\beta_0+\\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\n\\end{aligned}\n\\tag{1}\\]\nIn this model, marginal effects are directly given by the model coefficients:\n\\[\\frac{\\partial E(y|x)}{\\partial x_k} = \\beta_k\n\\]\nWhich greatly simplifies the analysis.\nIf the model has interactions or polynomials, marginal effects are somewhat more challenging:\n\\[\\begin{aligned}\nE(y|x) = \\beta_0+\\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 \\times x_2  + \\beta_4 x_2^2\n\\end{aligned}\n\\tag{2}\\]\nwhere the marginal effects would be given by:\n\\[\\begin{aligned}\n\\frac{\\partial E(y|x)}{\\partial x_1} &= \\beta_1 + \\beta_3 x_2 \\\\\n\\frac{\\partial E(y|x)}{\\partial x_2} &= \\beta_2 + \\beta_3 x_1 + 2 \\beta_4 x_2\n\\end{aligned}\n\\]\nThese effects are not constant. They vary based on values of \\(x_1\\) and \\(x_2\\). Knowing some calculus, however, its rather straight forward to determine the marginal effects, and apply that to our work.\nSo lets see an example of this. First the setup:\n\nclear\nset linesize 255\nset seed 1\nset obs 1000           // Create a sample of 100 obs\ngen x1 = runiform(0,4) // Create two variables with mean 2\ngen x2 = runiform(0,4) //\n// create auxiliary variables\ngen x1x2=x1*x2\ngen x2sqr=x2^2\n// and set up the coefficient values\nscalar b0 = 2\nscalar b1 = 1\nscalar b2 = 1\nscalar b3 = 1\nscalar b4 = 1\ngen y = b0 + b1 * x1 + b2 * x2 + b3 * x1x2 + b4 * x2sqr + rnormal()\n\nNow, we can estimate the model and obtain marginal effects based on the analytical solution:\nqui:reg y x1 x2 x1x2 x2sqr\nwhich gives us the following\n$$ E(y|X) = 1.953 + 1.046 x1 + 0.944 x2 + 1.019 x1x2 + 1.005 x2sqr\nN = 1000 R^2 = 0.989 $$\nFrom where we can estimate marginal effects using the analytical formulas we derived.\n\nmargins, expression(_b[x2]+_b[x1x2]*x1+2*_b[x2sqr]*x2)\n\nwarning: option expression() does not contain option predict() or xb().\n\nPredictive margins                                       Number of obs = 1,000\nModel VCE: OLS\n\nExpression: _b[x2]+_b[x1x2]*x1+2*_b[x2sqr]*x2\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _cons |   7.131659   .0265967   268.14   0.000      7.07953    7.183787\n------------------------------------------------------------------------------\n\n\nNoticed I used margins expression, to allow me estimate average marginal effects using the analytical solutions. Technically, what we see is the average prediction of the expression we provided. We, of course, know they are right, because we know calculus.\nYou could use a similar approach to estimate marginal effects based on any transformation (as long as you know the how to obtain the partial derivative)",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Marginal effects with nonlinear transformations: `f_able`"
    ]
  },
  {
    "objectID": "stata_do/stata_do6.html#marginal-effects-margins",
    "href": "stata_do/stata_do6.html#marginal-effects-margins",
    "title": "Marginal effects with nonlinear transformations: f_able",
    "section": "Marginal effects: margins",
    "text": "Marginal effects: margins\nAs mentioned earlier, Stata command margins allows you to estimate marginal effects of almost any model, as long as it understands the inter-relationships among variables. So lets see what would have happend if I use it in my previous model:\n\nmargins, dydx(x1 x2)\n\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  x1 x2\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x1 |   1.045827   .0563175    18.57   0.000     .9353127    1.156342\n          x2 |   .9437094   .1142363     8.26   0.000     .7195376    1.167881\n------------------------------------------------------------------------------\n\n\nWe get something that is wrong. Instead of estimating the correct marginal effects, it is only showing the coefficients for \\(x_1\\) and \\(x_2\\). The reason for this is that margins has no way to know that the variables \\(x1x2\\) and \\(x2sqr\\) are in fact functions of \\(x_1\\) and \\(x_2\\). Not at least we use the correct syntax.\nIn Stata, interactions and polynomials can be added to the model specification using “#”, and indicating the type of variable one is using. In this case, I use “c.” because \\(x_1\\) and \\(x_2\\) are continuous variables. See help fvvarlist, for more information.\n\nqui:reg y c.x1 c.x2 c.x1#c.x2 c.x2#c.x2\nmargins, dydx(x1 x2)\n\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  x1 x2\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x1 |   3.128384   .0283469   110.36   0.000     3.072757     3.18401\n          x2 |   7.131659   .0265967   268.14   0.000     7.079467    7.183851\n------------------------------------------------------------------------------\n\n\nNow we get the same result as earlier because Stata understands that \\(x_2^2\\) or c.x2#c.x2 is a new variable that depends on \\(x_2\\). Similar for \\(x_1\\).\nThe key, then, is to teach Stata that certain variables are indeed function of others. Here is where f_able enters into play.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Marginal effects with nonlinear transformations: `f_able`"
    ]
  },
  {
    "objectID": "stata_do/stata_do6.html#how-does-margins-work-and-why-f_able",
    "href": "stata_do/stata_do6.html#how-does-margins-work-and-why-f_able",
    "title": "Marginal effects with nonlinear transformations: f_able",
    "section": "How does margins work? and why f_able?",
    "text": "How does margins work? and why f_able?\nFigure 1 provies an sketch of how margins work when estimating a model.\n\nGet the data, with correct model specifications.\nEstimate the model, which creates the e(b) and e(V), which contains all the data necessairy to estimate the marginal effects.\nmargins makes a small change in \\(x's\\) to obtain numerical derivatives.\nCalls on predict to get the predicted values of interest\nCalculates marginal effects, and reports them.\n\n\n\n\n\n\n\nFigure 1: How margins work?\n\n\n\nThe problem with using data that we create before the model has to do with step 3.\nThe change induced on \\(x\\) will not be reflected in \\(GX\\), because there is nothing to tie them together.\n\n\n\n\n\n\nFigure 2: Why margins fails?\n\n\n\nFigure 3 shows how f_able helps solving this problems.\n\nWhen creating the data with fgen, additional information will be added to show how that data was generated.\nAfter estimating the model, f_able will add information to the e() so it nows what variables depend on others (those created with fgen).\nFinally, f_able_p acts as an intermediary to update all “created variables” during the numerical differentiation process.\n\n\n\n\n\n\n\nFigure 3: How does f_able helps?",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Marginal effects with nonlinear transformations: `f_able`"
    ]
  },
  {
    "objectID": "stata_do/stata_do6.html#using-f_able-to-estimate-margins-for-any-transformation.",
    "href": "stata_do/stata_do6.html#using-f_able-to-estimate-margins-for-any-transformation.",
    "title": "Marginal effects with nonlinear transformations: f_able",
    "section": "Using f_able to estimate margins for any transformation.",
    "text": "Using f_able to estimate margins for any transformation.\n\nSetup\nYou can download f_able from SSC. However, the latest version will be avaible from my repository. To do so, type:\n* Opt 1. Get Fra installer\nnet install fra, replace from(https://friosavila.github.io/stpackages)\nfra install f_able, replace\n \n* Opt 2. Install it from here directly\nnet install f_able, replace from(https://friosavila.github.io/stpackages)\n\n* Opt 3. Use SSC\nssc install f_able\n\n\nRe-doing the example\nSo, lets retake the previous example. We know that the “offending” variables were x1x2 and x2sqr. So lets recreate them using fgen.\n\ndrop x1x2 x2sqr\nfgen x1x2  = x1*x2\nfgen x2sqr = x2*x2\n\nNow, lets estimate the model of interest, but adding a line indicating what variables are “created” variables:\n\nqui:reg y x1 x2 x1x2 x2sqr\n** This line is necessary\nf_able x1x2 x2sqr, auto\n** option Auto, makes some of the processing easier\n\nThis is an experimental feature in f_able\nUsing this option, you do not need to add nochain or numerical options in margins\nAll variables in -nlvar- have been declared\n\n\nFinally, lets estimate the marginal effects again\n\nmargins, dydx(*)\n\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: OLS\n\nExpression: Fitted values, predict()\ndy/dx wrt:  x1 x2\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x1 |   3.128384   .0283469   110.36   0.000     3.072825    3.183942\n          x2 |   7.131659   .0265967   268.14   0.000      7.07953    7.183787\n------------------------------------------------------------------------------\n\n\n\n\nWhat if \\(x\\) is no longer in the main model\nThere are a few other cases that may be of interest. Consider the previous model, and assume we are only interested in adding the quadratic term in the model, not the linear one.\nUsing factor notation, one could do the following:\n\nqui:reg y c.x2#c.x2\nmargins, dydx(*)\n\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  x2\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x2 |     7.1484    .103735    68.91   0.000     6.944836    7.351963\n------------------------------------------------------------------------------\n\n\nUsing f_able, however, you still need to include the original variable in the model. This can be done using the “o.” prefix, so that the variable is ommitted, from calculations, but remain in the model.\n\nqui:reg y x2sqr o.x2\nf_able x2sqr, auto\nmargins, dydx(*)\n\nThis is an experimental feature in f_able\nUsing this option, you do not need to add nochain or numerical options in margins\nAll variables in -nlvar- have been declared\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: OLS\n\nExpression: Fitted values, predict()\ndy/dx wrt:  x2\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x2 |     7.1484    .103735    68.91   0.000     6.945083    7.351717\n------------------------------------------------------------------------------\n\n\nAlright, these are very simple examples that you could already do with margins. So lets try something different. Estimate a model using \\(x\\) and \\(\\sqrt x\\) as explanatory variables:\n\nfgen rx1=x1^.5\nqui:reg y x1 rx1\nf_able rx1, auto\nmargins, dydx(*)\n\nThis is an experimental feature in f_able\nUsing this option, you do not need to add nochain or numerical options in margins\nAll variables in -nlvar- have been declared\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: OLS\n\nExpression: Fitted values, predict()\ndy/dx wrt:  x1\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x1 |   3.332656   .3268743    10.20   0.000     2.691994    3.973318\n------------------------------------------------------------------------------\n\n\nHow do we know is correct? We can replicate the results using nl!. Technically, it works in a similar way as f_able does.\n\nqui:nl (y={b0}+{b1}*x1+{b2}*x1^0.5), variable(x1)\nmargins, dydx(*)\n\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: GNR\n\nExpression: Fitted values, predict()\ndy/dx wrt:  x1\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x1 |   3.332656   .3268743    10.20   0.000     2.691994    3.973318\n------------------------------------------------------------------------------\n\n\nSomething even harder? What about a poisson regression:\n\nqui:poisson y x1 x2 x1x2 x2sqr\nqui:f_able x1x2 x2sqr, auto\nmargins, dydx(*)\n\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: OIM\n\nExpression: Predicted number of events, predict()\ndy/dx wrt:  x1 x2\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          x1 |   3.168351   .1185385    26.73   0.000      2.93602    3.400683\n          x2 |   7.004783   .1513991    46.27   0.000     6.708046     7.30152\n------------------------------------------------------------------------------\n\n\nAnother implementation some people usually ask for is the estimation of marginal effects when using splines, either regular polynomial splines, or restricted cubic splines.\nBecause splines are messy to code, instead of using fgen, we can use f_spline or f_rcspline. Let’s use both:\n\nwebuse dui, clear\n* model citation=f(fines)+e\n* Cubic polynomial spline with 1 knot\nf_spline spfine=fines, degree(3) nknots(1)\n* Restricted Cubic polynomial with 5 knots \nf_rcspline cspfine=fines,  nknots(5) replace\n* Different from other commands,\n* f_spline and  f_rcspline use\n* the original variable as part of the splines\n\n(Fictional data on monthly drunk driving citations)\n\n\nNow lets estimate both models:\nqui:reg citations fines spfine2 spfine3 spfine4\nqui:f_able spfine2 spfine3 spfine4, auto\nqui:emargins , dydx(fines) estore(m1)\nqui:reg citations fines cspfine2 cspfine3 cspfine4\nqui:f_able cspfine2 cspfine3 cspfine4, auto\nqui:emargins , dydx(fines) estore(m2)\nqui:reg citations fines\nqui:emargins , dydx(fines) estore(m3)\ndisplay \"#### Contrasting Models\"\nesttab m1 m2 m3, nonum mtitle(\"Cubic Spline\" \"RCubic Spline\" \"LM\") md se note(\"Contrasting effects\")\n\nContrasting Models\n\n\n\n\n\n\n\n\n\n\nCubic Spline\nRCubic Spline\nLM\n\n\n\n\nfines\n-7.911***\n-7.965***\n-7.960***\n\n\n\n(0.424)\n(0.422)\n(0.434)\n\n\nN\n500\n500\n500\n\n\n\nContrasting effects * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nOf course, when using splines, it may be far more interesting plotting the effects across values of \\(fines\\).\nset scheme white2\ncolor_style tableau\nqui:reg citations fines spfine2 spfine3 spfine4\nqui:f_able spfine2 spfine3 spfine4, auto\nqui:emargins , at(fines=(8(0.125)12)) \nqui:marginsplot, name(m1, replace)\nqui:emargins , dydx(fines) at(fines=(8(0.125)12))  \nqui:marginsplot, name(m2, replace)\n\n\n\n\n\n\n\n\n\n\n\n(a) Predicted mean\n\n\n\n\n\n\n\n\n\n\n\n(b) Marginal effect\n\n\n\n\n\n\n\nFigure 4: Predictive mean vs Marginal efects",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Marginal effects with nonlinear transformations: `f_able`"
    ]
  },
  {
    "objectID": "stata_do/stata_do6.html#conclusions",
    "href": "stata_do/stata_do6.html#conclusions",
    "title": "Marginal effects with nonlinear transformations: f_able",
    "section": "Conclusions",
    "text": "Conclusions\nf_able is a relatively simple program that allows you to estimate marginal effects of unconventional variable transformations.\nWhile I have tried to make f_able as flexible as possible, couple of challenges remain.\n\nf_able relies on numerical differentiation, which can be very time consuming in some models.\nWhen the model is fairly complex, think about interactions of splines and other variables, margins may have problems producing results, because the model itself may fail to fullfill certain estimation criterias. When that happens, one option is to add noestimcheck and/or force option.\nSome models rely on analytical solutions based on the chainrule. While this may not be a problem in the latest f_able update (auto option), it could still linger for some models. When this happens, try option nochainrule.\nSome times, no results will be shown because the variance covariance matrix is not symetric. This happens because of the small discrepancies produced by the numerical differentiation. If something like that appears, there are two utilities f_symev and f_symrv, that will fix it before you produce the necessary results again.\n\nHope you find it useful.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Marginal effects with nonlinear transformations: `f_able`"
    ]
  },
  {
    "objectID": "stata_do/stata_do4.html",
    "href": "stata_do/stata_do4.html",
    "title": "The IV-Probit: margins and ml",
    "section": "",
    "text": "In previous articles, I have shown how to use the margins command after ml for the linear regression model (assuming normality) and the probit model. Today, I’ll provide another example for a more complex case: a two-equation model estimation, specifically the “IVprobit,” or instrumental variable probit.\nThis model has gained notoriety due to its inconsistent estimated marginal effects, which have been the subject of multiple threads on Statalist. The issue comes from different versions of “marginal effects” produced by margins in different Stata versions. Each version is correct from a computational standpoint, but not always consistent with common sense.\nMy assessment, backed up by discussions with colleagues, is that:\n\nStata 13 estimated the correct marginal effects for the IVprobit MLE but not for the two-step approach.\nStata 14 and 15 estimated the Full Information Marginal Effect, which is technically correct but contradicts common sense. Prof Wooldridge has extensively discussed this issue and advocates for either MLE or two-step marginal effects.\nStata 16 released an update that now produces something equivalent to the two-step marginal effect for MLE estimation, likely due to recent discussions.\n\nNo changes that I am aware came in Stata 17, except perhaps that we still do not have marginal effects for a two-step approach.\nWithout further ado, let me present my own version of how to estimate an IVprobit model and how to set up the predict program. I’ll also explain why there were so many types of marginal effects in use.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "The IV-Probit: `margins` and `ml`"
    ]
  },
  {
    "objectID": "stata_do/stata_do4.html#introduction",
    "href": "stata_do/stata_do4.html#introduction",
    "title": "The IV-Probit: margins and ml",
    "section": "",
    "text": "In previous articles, I have shown how to use the margins command after ml for the linear regression model (assuming normality) and the probit model. Today, I’ll provide another example for a more complex case: a two-equation model estimation, specifically the “IVprobit,” or instrumental variable probit.\nThis model has gained notoriety due to its inconsistent estimated marginal effects, which have been the subject of multiple threads on Statalist. The issue comes from different versions of “marginal effects” produced by margins in different Stata versions. Each version is correct from a computational standpoint, but not always consistent with common sense.\nMy assessment, backed up by discussions with colleagues, is that:\n\nStata 13 estimated the correct marginal effects for the IVprobit MLE but not for the two-step approach.\nStata 14 and 15 estimated the Full Information Marginal Effect, which is technically correct but contradicts common sense. Prof Wooldridge has extensively discussed this issue and advocates for either MLE or two-step marginal effects.\nStata 16 released an update that now produces something equivalent to the two-step marginal effect for MLE estimation, likely due to recent discussions.\n\nNo changes that I am aware came in Stata 17, except perhaps that we still do not have marginal effects for a two-step approach.\nWithout further ado, let me present my own version of how to estimate an IVprobit model and how to set up the predict program. I’ll also explain why there were so many types of marginal effects in use.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "The IV-Probit: `margins` and `ml`"
    ]
  },
  {
    "objectID": "stata_do/stata_do4.html#the-setup",
    "href": "stata_do/stata_do4.html#the-setup",
    "title": "The IV-Probit: margins and ml",
    "section": "The Setup",
    "text": "The Setup\nFirst thing first. Unless you already have this program saved somewhere in your accesible ado files (most likely the “ado/personal” folder), make sure to have the following program in memory. It will allow you to add or modify information to e(), which is where all estimation commands store information.\n\nclear all\nprogram adde, eclass\n    ereturn `0'\nend\n\n\n\n\n\n\n\nLet’s start with some Econometrics 101: The IVprobit model is a nonlinear model that is useful when you have a binary dependent variable (0-1) but one or more of your controls suffer from endogeneity. In this case, you want to estimate the “probability of success (y=1)” given a set of characteristics, but the characteristics of insterest is a continuous but endogenous explanatory.\nNow, when a variable is endogenous, we cannot estimate the model and interpret the results as causal effects. This happens because changes in the endogenous variable can happen at the same time as changes in unobserved components. Therefore, if the outcome changes, we cannot tell if it is because the endogenous variable changed or because the unobservables changed (as they are, after all, correlated).\nTo deal unobserved confounders, we have two options:\n\nWe can use instruments to isolate the exogenous variation of the variable of interest (using the 2SLS approach, for example)\nUse the instruments to obtain an approximation of the endogenous component that we can control for directly (Control function approach).\n\nIn fact, IV-probit is the application of the latter: a control function approach.\nFormally, IVprobit model can be written as follows:\n\\[\ny_2 = z_1 \\delta_1 + z_2 \\delta_2 + u_2\n\\tag{1}\\]\n\\[\ny_1^*= z_1 \\beta_1 + y_2 \\beta_2 + u_1\n\\tag{2}\\]\n\\[\ny_1 = 1(y_1^*&gt;0)\n\\]\nwhere the errors \\(u_1,u_2\\) follow a bivariate normal distribution:\n\\[\n\\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix} \\sim Normal  \\begin{pmatrix}  \\begin{matrix} 0 \\\\ 0 \\end{matrix},\n\\begin{matrix} 1 & \\rho \\sigma_2\\\\ \\rho \\sigma_2 & \\sigma^2_2 \\end{matrix} \\end{pmatrix}\n\\]\nIn this model \\(z_1\\) and \\(z_2\\) are exogenous variables, \\(z_2\\) is a set of instruments, and \\(y_2\\) is a continuous but endogenous variable in the model. Finally we do not observe the latent variable \\(y_1^*\\), but instead observe \\(y_1\\) which only takes values of 0 or 1. So how do we estimate this model?? by parts!\nThe Equation 1 can be estimated directly, because it is a function of exogenous variables only. Thus, we could estimate that equation using standard OLS (as ivprobit-two-step does), or via MLE assuming the normality of the errors.\nThe one that requires more attention is Equation 2. We know that \\(corr(y_2,u_1)\\) is different from zero, which is the cause of the endogeneity of \\(y_2\\). We could, however, decompose \\(u1\\) into two parts. One that contains the endogenous component, and one that is exogenous and uncorrelated with all other variables.\nTo do this, we should first recall that if \\(u_1, u_2\\) follow a bivariate normal distribution, then, conditional on \\(u_2\\), \\(u_1\\) will have the following distribution:\n\\[\nu_1 \\sim N\\left(\\rho \\frac{u_2}{\\sigma_2}, {1-\\rho^2} \\right)\n\\tag{3}\\]\nwhich implies, we could write \\(u_1\\) as follows:\n\\[\nu_1 = \\rho \\frac{ u_2}{ \\sigma_2} + v_1\n\\tag{4}\\]\nIn this case, \\(v_1\\) will be, by construction, uncorrelated with \\(u_2\\) or with \\(y_2\\). So, we if substitute Equation 4 into Equation 1, we obtain:\n\\[\ny_1^* = z_1 \\beta_1 + y_2 \\beta_2 + \\rho \\frac{ u_2}{ \\sigma_2} + v_1\n\\tag{5}\\]\nThis equation can now be estimated directly, assuming we observe \\(u_2\\). However, to be estimated with a probit model, we also need to rescale the equation so that the re-scaled error \\(v_1\\) has a variance of 1.\nBased on Equation 3, we know \\(v_1\\) has a variance of \\(1-\\rho^2\\), so we just need to divide all terms in Equation 5 by \\(\\sqrt{1-\\rho^2}\\) and estimating the following model (or its simplification) using standard probit model. \\[\n\\frac{y^*_1}{\\sqrt{1-\\rho^2}} =\n    z_1 \\frac{\\beta_1}{\\sqrt{1-\\rho^2}} +\n    y_2 \\frac{\\beta_2}{\\sqrt{1-\\rho^2}} +\n    \\frac{\\rho}{\\sqrt{1-\\rho^2}} \\frac{u_2}{\\sigma_2} +\n    \\frac{v_1}{\\sqrt{1-\\rho^2}}\n\\tag{6}\\] \\[\ny^{**}_1 = z_1 \\beta^r_1 + y_2 \\beta^r_2 +\n    \\theta \\frac{u_2}{\\sigma_2} +   v_1\n\\tag{7}\\]\nWhat is the difference between using either equation?. I would argue none, as long as you know how to estimate the standard errors from the system.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "The IV-Probit: `margins` and `ml`"
    ]
  },
  {
    "objectID": "stata_do/stata_do4.html#the-actual-estimation",
    "href": "stata_do/stata_do4.html#the-actual-estimation",
    "title": "The IV-Probit: margins and ml",
    "section": "The actual estimation",
    "text": "The actual estimation\nLet’s discuss the different methods that can be used to estimate the ivprobit model. There are at least three ways to do so.\nThe first method is the two-step approach. In this method, one estimates Equation 1 using OLS, obtains the predicted residuals, which are plugged into equation Equation 7. This can be estimated using a simple probit model.\nThis method has two problems:\n\nIt only provides estimates for the “rescaled” coefficients, not the structural coefficients.\nIt will not provide you with the correct estimation of standard errors, because it will not consider the residuals are carrying over errors from the first step.\n\nSome textbooks suggest that doing so is a simple application of the delta method, or use bootstrap. But, the fact of the matter, is that you need to take into account that the residuals from Equation 1 are estimated not the true residuals \\(u_2\\).\nThe second method is to estimate Equation 1 and Equation 5 simultaneously using full information maximum likelihood. This imposes the assumption that the errors follow a bivariate normal distribution, and allows you to obtain estimates for the structural parameters, in addition to the “link” parameters \\(\\sigma_2\\) and \\(\\rho\\), providing correct standard errors.\nUnder this strategy, the contribution of a single observation to the likelihood function becomes:\n\\[\n\\begin{aligned}\nL_i &= L_i^1*L_i^2 \\\\\nL_i^1 &= \\phi(y_2,z_1\\delta_1 + z_2 \\delta_2, \\sigma_2) \\\\\n\\hat P(y_1|.) &= \\Phi \\left( \\frac{ z_1 \\beta_1 + y_2 \\beta_2 + \\rho \\frac{y_2 -z_1 \\delta_1 - z_2 \\delta_2}{\\sigma_2}}\n{\\sqrt{1-\\rho^2}} \\right) \\\\\nL^2_i & = P(y_1|.)^{y_1} * (1-P(y_1|.))^{1-y_1}\n\\end{aligned}\n\\]\nNotice that instead of plugging in \\(\\hat u_2\\) in the probit equation, I explicitly add \\(y_2 -z_1 \\delta_1 - z_2 \\delta_2\\). This allows to explicilty account for the measurement errors of the first stage.\nThere is a third option, which I will call two-step-mle. I call it this way, because the ivprobit will be estimated using Equation 1 and Equation 7. However, I call it MLE, because both equations are estimated simultaneously using MLE:\n\\[\n\\begin{aligned}\nL_i &= L_i^1*L_i^2 \\\\\nL_i^1 &= \\phi(y_2,z_1\\delta_1 + z_2 \\delta_2, \\sigma_2) \\\\\n\\hat P(y_1|.) &=  \\Phi \\left(  z_1 \\beta^r_1 + y_2 \\beta^r_2 + \\theta \\frac {y_2 -z_1 \\delta_1 - z_2 \\delta_2}{\\sigma_2} \\right) \\\\\nL^2_i & = P(y_1|.)^{y_1} * (1-P(y_1|.))^{1-y_1}\n\\end{aligned}\n\\tag{8}\\]\nThe difference with the standard FIML, is that only rescaled coefficients are estimated, and that the link between both equation is \\(\\theta\\) not \\(\\rho\\). Nevertheless, For this simplified example, both equations identify exactly the same model. If you are interested in this type of use of -ml- see my paper Rios-Avila and Canavire-Bacarreza (2018).\nCompared to the usual two-step approach, however, because the model is estimated simultaneously, the standard errors of all coefficients are correctly estimated, without further calculations (no delta method nor bootstrap).\nOne last thing to notice in this model. First, there is a close relationship between \\(\\theta\\) and \\(\\rho\\), which will affect the rescaled parameters: \\[\n\\theta = \\frac{\\rho}{\\sqrt{1-\\rho^2}} \\rightarrow \\rho =  \\frac{\\theta}{\\sqrt{1+\\theta^2}}\n\\] \\[\n\\beta = \\beta^r \\times \\sqrt{1-\\rho^2} = \\frac{\\beta^r}{\\sqrt{1+\\theta^2}}\n\\]",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "The IV-Probit: `margins` and `ml`"
    ]
  },
  {
    "objectID": "stata_do/stata_do4.html#the-log-likelihood-function",
    "href": "stata_do/stata_do4.html#the-log-likelihood-function",
    "title": "The IV-Probit: margins and ml",
    "section": "The Log Likelihood function",
    "text": "The Log Likelihood function\nWhile I have shown that using FIML and two-step-ml will provide the same results, I’ll stick with the two-step approach, as it allows me to derive marginal effects telling the story of what happened to -margins- through different Stata versions.\nThe following program defines this the log-likelihood function for the IV probit, using the two-step approach (Equation 8), using the following walk-through for the specification:\n\nxb will contain all the exogenous variables \\(z_1\\) plus the endogenous variable \\(y_2\\)\nzb will contain all the exogenous variables \\(z_1\\) and the instruments \\(z_2\\)\n\n\nprogram myivprobit_2sls\n    args lnf xb theta zb lnsigma\n    qui {\n        local y1 $ML_y1\n        local y2 $ML_y2\n        local u2 (`y2'-`zb')\n        tempvar xb_zb p1 p0\n        gen double `xb_zb'= `xb'+`theta'*((`u2')/exp(`lnsigma')) \n        gen double `p1'   = normal( `xb_zb')\n        gen double `p0'   = normal(-`xb_zb')\n        tempvar lnf1 lnf2\n        gen double `lnf1'  = log(normalden(`y2', `zb', exp(`lnsigma')))\n        gen double `lnf2' = log(`p1') if `y1'==1\n        replace    `lnf2' = log(`p0') if `y1'==0\n        replace `lnf' = `lnf1' + `lnf2'\n    }\nend",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "The IV-Probit: `margins` and `ml`"
    ]
  },
  {
    "objectID": "stata_do/stata_do4.html#the-predict-program",
    "href": "stata_do/stata_do4.html#the-predict-program",
    "title": "The IV-Probit: margins and ml",
    "section": "The predict program",
    "text": "The predict program\nSo finally, the part that will be a bit more controversial. The prediction of the probability of success!.\nThe reason why this is controversial is because there are two candidates to identify this expression.\nThe first candidate relates to the structural Equation 2. Basically, if we can estimate the unscaled coefficients, the predicted outcome could be identified by:\n\\[\nP(y_1=1| z_1 , y_2) = \\Phi \\left( z_1 \\beta_1 + y_2 \\beta_2  \\right)\n\\]\nor if one prefers the version based on rescaled coefficients: \\[\nP(y_1=1| z_1 , y_2) = \\Phi \\left( z_1 \\beta^r_1 * {\\sqrt{1-\\rho^2}} + y_2 \\beta^r_2 * {\\sqrt{1-\\rho^2}} \\right)\n\\]\nThus, marginal effects can be obtained by analyzing either one of these equations alone. Standard errors for this expression can be identified directly only if we estimate the structural equation using FIML, or using the rescaled coefficients, making sure standard errors are calculated acounting for the estimation errors of the first stage.\nThe second option relates to estimate the marginal effects using Equation 7:\n\\[\nP(y_1=1| z_1 , y_2, \\hat{u}_2 ) = \\Phi \\left( z_1 \\beta^r_1 + y_2 \\beta^r_2 + \\theta \\hat{u}_2 \\right)\n\\tag{9}\\]\nHowever, because \\(hat u_2\\) is never observed, it is usually recommended to average (but not ignore) the impact of \\(\\hat u_2\\) on the equation:\n\\[\nP(y_1=1| z_1 , y_2) = E \\left( \\Phi \\left( z_1 \\beta^r_1 + y_2 \\beta^r_2 + \\theta \\hat{u}_2 \\right)| z_1, y_2 \\right)\n\\tag{10}\\]\nThe bottom line: If one uses the two-step approach, marginal effects could be estimated assuming \\(\\hat u_2\\) is just another exogenous variable in the model. The difficulty would be obtaining the correct estimation of standard errors.\nSo lets write these two options into a “predict” program.\n\nprogram myivprobit_p\n    syntax newvarname [if] [in] , [ pr1 pr2  *]\n    if \"`pr1'`pr2'\" ==\"\" {\n        ml_p `0'\n    }\n    tokenize `e(depvar)'\n    local y1  `1'\n    local y2  `2'\n    marksample touse, novarlist\n    if \"`pr1'\" !=\"\"  {\n        tempvar xb zb theta lnsigma\n        _predict double `xb'   , eq(#1)\n        _predict double `theta', eq(#2)\n        _predict double `zb'   , eq(#3) \n        _predict double `lnsigma', eq(#4)       \n        gen `typlist' `varlist' = ///\n            normal(`xb'+`theta'*(`y2'-`zb')/exp(`lnsigma')) if `touse'\n        label var `varlist' \"P(y=1|X) two-step\"\n    }       \n    else if \"`pr2'\"!=\"\" {\n        tempvar xb zb theta lnsigma\n        _predict double `xb' , eq(#1)\n        _predict double `theta'  , eq(#2)\n        gen `typlist' `varlist' = ///\n            normal(`xb'/sqrt(1+`theta'^2)) if `touse'\n        label var `varlist' \"P(y=1|X) FIML\"\n    }\nend\n\n\n\n\nThe first option pr1 will estimate the predicted probability as if the model were estimated using the two-step approach, whereas the second will estimate the predicted probability based on the structural equation.\nAlright, so lets estimate the model and compare the results with the built-in ivprobit command:\n\nclear  \nwebuse laborsup\nglobal  y1   fem_work\nglobal  z1   fem_educ   kids  \nglobal  y2   other_inc\nglobal  z2   male_educ   \n\n*Built in command:\nivprobit $y1  $z1 ($y2 = $z2), two\n\nChecking reduced-form model...\n\nTwo-step probit with endogenous regressors        Number of obs   =        500\n                                                  Wald chi2(3)    =      93.97\n                                                  Prob &gt; chi2     =     0.0000\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   other_inc |   -.058473   .0093364    -6.26   0.000    -.0767719    -.040174\n    fem_educ |    .227437   .0281628     8.08   0.000     .1722389     .282635\n        kids |  -.1961748   .0496323    -3.95   0.000    -.2934522   -.0988973\n       _cons |   .3956061   .4982649     0.79   0.427    -.5809752    1.372187\n------------------------------------------------------------------------------\nWald test of exogeneity: chi2(1) = 6.50                   Prob &gt; chi2 = 0.0108\nInstrumented: other_inc\n Instruments: fem_educ kids male_educ\n\n\n\n*my ivprobit two-step\nml model lf myivprobit_2sls ($y1 = $z1  $y2 ) ///\n         (theta:) ($y2 = $z1 $z2  ) (lnsigma:) , ///\n         technique(nr bhhh) init(lnsigma:_cons = 2.81 ) maximize nolog\nml display\n\n\n                                                        Number of obs =    500\n                                                        Wald chi2(3)  =  94.01\nLog likelihood = -2368.2062                             Prob &gt; chi2   = 0.0000\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\neq1          |\n    fem_educ |    .227437   .0281561     8.08   0.000     .1722519     .282622\n        kids |  -.1961748   .0496179    -3.95   0.000    -.2934242   -.0989254\n   other_inc |  -.0584729   .0093339    -6.26   0.000    -.0767671   -.0401788\n       _cons |   .3956051   .4981099     0.79   0.427    -.5806724    1.371883\n-------------+----------------------------------------------------------------\ntheta        |\n       _cons |   .4008081   .1626174     2.46   0.014     .0820838    .7195323\n-------------+----------------------------------------------------------------\neq3          |\n    fem_educ |   .3351867   .2825972     1.19   0.236    -.2186937     .889067\n        kids |   .8329056   .5475666     1.52   0.128    -.2403052    1.906116\n   male_educ |   2.845253    .282746    10.06   0.000     2.291081    3.399425\n       _cons |   9.872559   5.029193     1.96   0.050     .0155214     19.7296\n-------------+----------------------------------------------------------------\nlnsigma      |\n       _cons |   2.813383   .0316228    88.97   0.000     2.751404    2.875363\n------------------------------------------------------------------------------\n\n\nYou can see right away that except for differences attributed to rounding errors and degrees of freedom, the results are virtually the same. It is also reasuring to see that the results are also the same when we compared ivprobit-mle and the rescaled coefficients:\n\n*FIML\nivprobit $y1  $z1 ($y2 = $z2), ml nolog\n\n\nProbit model with endogenous regressors                 Number of obs =    500\n                                                        Wald chi2(3)  = 163.88\nLog likelihood = -2368.2062                             Prob &gt; chi2   = 0.0000\n\n-------------------------------------------------------------------------------\n              | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n--------------+----------------------------------------------------------------\n    other_inc |  -.0542756   .0060854    -8.92   0.000    -.0662028   -.0423485\n     fem_educ |    .211111   .0268648     7.86   0.000     .1584569    .2637651\n         kids |  -.1820929   .0478267    -3.81   0.000    -.2758315   -.0883542\n        _cons |   .3672086   .4480724     0.82   0.412    -.5109971    1.245414\n--------------+----------------------------------------------------------------\ncorr(e.othe~c,|\n   e.fem_work)|   .3720375   .1300518                      .0946562    .5958136\nsd(e.other_~c)|   16.66621   .5270318                      15.66461    17.73186\n-------------------------------------------------------------------------------\nWald test of exogeneity (corr = 0): chi2(1) = 6.70        Prob &gt; chi2 = 0.0096\nInstrumented: other_inc\n Instruments: fem_educ kids male_educ\n\n\n\n*my ivprobit two-step\nml model lf myivprobit_2sls ($y1 = $z1  $y2 )  (theta:) ($y2 = $z1 $z2  ) (lnsigma:) , ///\n    technique(nr bhhh)   init(lnsigma:_cons = 2.81 ) maximize nolog\nadde local predict myivprobit_p                 \nest store myivp\n*with rescaled coefficients:\nnlcom   (other_inc: _b[other_inc]/sqrt(1+_b[theta:_cons]^2)) ///\n        (fem_educ: _b[fem_educ]/sqrt(1+_b[theta:_cons]^2)) ///\n        (kids: _b[kids]/sqrt(1+_b[theta:_cons]^2)) ///\n        (cons: _b[_cons]/sqrt(1+_b[theta:_cons]^2)) \n\n\n   other_inc: _b[other_inc]/sqrt(1+_b[theta:_cons]^2)\n    fem_educ: _b[fem_educ]/sqrt(1+_b[theta:_cons]^2)\n        kids: _b[kids]/sqrt(1+_b[theta:_cons]^2)\n        cons: _b[_cons]/sqrt(1+_b[theta:_cons]^2)\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   other_inc |  -.0542756   .0060854    -8.92   0.000    -.0662027   -.0423485\n    fem_educ |    .211111   .0268648     7.86   0.000      .158457    .2637651\n        kids |  -.1820929   .0478267    -3.81   0.000    -.2758316   -.0883543\n        cons |   .3672077   .4480724     0.82   0.412    -.5109982    1.245414\n------------------------------------------------------------------------------\n\n\nAgain, showing exactly the same results",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "The IV-Probit: `margins` and `ml`"
    ]
  },
  {
    "objectID": "stata_do/stata_do4.html#a-story-of-marginal-effects",
    "href": "stata_do/stata_do4.html#a-story-of-marginal-effects",
    "title": "The IV-Probit: margins and ml",
    "section": "A Story of marginal effects",
    "text": "A Story of marginal effects\nLet me now walk you through the Story of marginal effects with ivprobit.\n\nStata 13\nBack in Stata 13, marginal effects for IV probit were estimated using the structural equation coeffients: \\[\nP(y_1=1|z_1,y_2)=\\Phi(z_1\\beta_1+y_2\\beta_2)\n\\]\nSo that marginal effects were defined as: \\[\n\\begin{aligned}\n\\frac{\\partial P(y_1=1|.)}{\\partial z_1} = \\phi ( z_1 \\beta_1 + y_2 \\beta_2  ) \\beta_1 \\\\\n\\frac{\\partial P(y_1=1|.)}{\\partial y_2} = \\phi ( z_1 \\beta_1 + y_2 \\beta_2  ) \\beta_2\n\\end{aligned}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nAs you may have noticed, I’m rewritting few of my older posts using Quarto. So, I can only use Stata17 dynamically. Because of that the code you will see below will not be reproducible, unless you have the same Stata version\n\n\nIf you have access to Stata 13, you will be able to reproduce the following output:\nmargins, dydx(*) predict(pr)\n\nAverage marginal effects                          Number of obs   =        500\nModel VCE    : OIM\n\nExpression   : Probability of positive outcome, predict(pr)\ndy/dx w.r.t. : other_inc fem_educ kids male_educ\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n   other_inc |   -.014015   .0009836   -14.25   0.000    -.0159428   -.0120872\n    fem_educ |   .0545129   .0066007     8.26   0.000     .0415758      .06745\n        kids |  -.0470199   .0123397    -3.81   0.000    -.0712052   -.0228346\n   male_educ |          0  (omitted)\n------------------------------------------------------------------------------\nThis marginal effects are emulated using pr2 after myivprobit:\n\nest restore myivp\nmargins, dydx(*) predict(pr2) force\n\n(results myivp are active now)\nnote: prediction is a function of possibly stochastic quantities other than\n      e(b).\n\nAverage marginal effects                                   Number of obs = 500\nModel VCE: OIM\n\nExpression: P(y=1|X) FIML, predict(pr2)\ndy/dx wrt:  fem_educ kids other_inc male_educ\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    fem_educ |   .0545129   .0066003     8.26   0.000     .0415766    .0674493\n        kids |  -.0470199   .0123394    -3.81   0.000    -.0712047   -.0228351\n   other_inc |   -.014015   .0009837   -14.25   0.000    -.0159431   -.0120869\n   male_educ |          0  (omitted)\n------------------------------------------------------------------------------\n\n\nYou will see output from margins also include male_educ in the list of exogenous variables. This happens because it is an explanatory variable for at least one equation in the model (first). However, because this variable is not included in the second equation, it has a no effect on it.\n\n\nStata 14.1\nWhen we reached Stata 14.1, a change was introduced in how probabilities were calculated after ivprobit. As it says in the “whatsnew” material, the new formulation would take into account endogeneity.\nSpecifically, they use what I call the 2sls predicted probabilities, following equation (5) wth the caveat that \\(\\hat u_2\\) was substituted by \\(y_2 - z_1 \\delta_1 -z2\\delta_2\\):\n\\[\nP(y_1=1|z_1,y_2,z_2) =\\Phi \\left(\n    \\frac{z_1 \\beta_1 + y_2 \\beta_2 + \\rho \\frac{y_2 - z_1 \\delta_1 - z_2 \\delta_2}{\\sigma_2}}\n    {\\sqrt{1-\\rho^2}}   \\right)\n\\tag{11}\\]\nWhile the two equations above are basically the same, they have important differences when marginal effects are estimated by software. Specifically, the probability of sucess is now a function of \\(z_2\\)!.\nSo let me explain first what Stata 14.1, did. To estimate marginal effects, partial derivatives were based on Equation 11:\n\\[\n\\begin{aligned}\n\\frac{\\partial P(y=1|.)}{\\partial z_1} &=\n  \\phi(.)*\\left( \\frac{\\beta_1}{\\sqrt{1-\\rho^2}}  \n  -\\frac{\\rho}{\\sqrt{1-\\rho^2}} * \\frac{\\delta_1}{\\sigma_2} \\right)  \\\\\n   \\frac{\\partial P(y=1|.)}{\\partial z_2} &=\n    \\phi(.)*\\left( 0\n  -\\frac{\\rho}{\\sqrt{1-\\rho^2}} * \\frac{\\delta_2}{\\sigma_2} \\right)  \\\\\n\\frac{\\partial P(y=1|.)}{\\partial y_2} &=\n    \\phi(.)*\\left( \\frac{\\beta_2}{\\sqrt{1-\\rho^2}}  \n    +\\frac{1}{\\sqrt{1-\\rho^2}} * \\frac{1}{\\sigma_2} \\right)    \n\\end{aligned}\n\\]\nFrom the technical point of view, these partial derivatives are correct, since they are capturing both the direct and indirect effects of all variables on the probability of success. Something similar to total, rather than partial, derivative.\nThe problem, however, is that this assumes we could actually observe how the unobserved component changes when other variables change. Standard regression analysis, however, would say that these unobserved components should be considered as fixed, and instead one should estimate marginal effects averaging over the unobserved factors. Thus, the second term on each one of the above derivatives should be zero.\nNevertheless, if you try estimating marginal effects with Stata 14.2, you will get the following result:\nmargins, dydx(*) predict(pr)\n\nAverage marginal effects                        Number of obs     =        500\nModel VCE    : OIM\n\nExpression   : Probability of positive outcome, predict(pr)\ndy/dx w.r.t. : other_inc fem_educ kids male_educ\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n   other_inc |  -.0097802   .0014994    -6.52   0.000     -.012719   -.0068414\n    fem_educ |   .0623273    .007099     8.78   0.000     .0484135     .076241\n        kids |  -.0614265   .0139446    -4.41   0.000    -.0887574   -.0340956\n   male_educ |  -.0194406   .0022103    -8.80   0.000    -.0237728   -.0151084\n------------------------------------------------------------------------------\nTo replicate this using myivprobit, I would estimate marginal effects using option pr1, requesting derivates to be estimated without the chain rule (nochain). This makes sure that one takes into account the effect of all changes in \\(y_2, z_1\\) and \\(z_2\\) on the predicted outcome \\(P(y=1|.)\\):\n\nest restore myivp\nmargins, dydx(*) predict(pr1) force nochain\n\n(results myivp are active now)\nnote: prediction is a function of possibly stochastic quantities other than\n      e(b).\n\nAverage marginal effects                                   Number of obs = 500\nModel VCE: OIM\n\nExpression: P(y=1|X) two-step, predict(pr1)\ndy/dx wrt:  fem_educ kids other_inc male_educ\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    fem_educ |   .0623273   .0060316    10.33   0.000     .0505055     .074149\n        kids |  -.0614265   .0128383    -4.78   0.000    -.0865891   -.0362639\n   other_inc |  -.0097802   .0009828    -9.95   0.000    -.0117065   -.0078539\n   male_educ |  -.0194406   .0074886    -2.60   0.009    -.0341181   -.0047631\n------------------------------------------------------------------------------\n\n\n\n\nStata 16\nThe earlier version of Stata 16 came with very similar problems as the ones mentioned above. However, due in part to an earlier version of this article, in November of 2020, Stata made a correction in how marginal effects were estimated for ivprobit as well as other related commands (see update 19nov2020).\nIn this update, they change the default option and now produces the correct marginal effects, assuming the predicted errors are fixed.\nLet’s take a closer look at the findings. First, it appears that Stata 14, 15, and early 16 versions were unintentionally estimating a partial effect that accounted for a second-order effect through the first stage regression. While this might have a negligible effect on the exogenous variables, it could have a considerable impact on the endogenous variable of interest, resulting in some people reporting negative marginal effects even when the estimated coefficient was positive. Additionally, the instrument, in this case male_educ, would also appear in the output, capturing only a second-order effect on the outcome of interest.\nHowever, after a lively discussion on Statalist, including input from Prof. Wooldridge, it was revealed that Stata (and margins) was incorrectly estimating marginal effects. As shown here, partial derivatives were being estimated through the first and second equations, leading to incorrect results.\nProf. Wooldridge recommended a manual two-step approach for estimating marginal effects, with standard errors obtained via bootstrap, using Equation 9 to estimate the partial effects. This makes a difference because we will be making the explicit assumption that \\(\\hat u_2\\) does not change when the other variables change. This will modify the partial effects to the following:\n\\[\n\\begin{aligned}\n\\frac{\\partial P(y=1|.)}{\\partial z_1} &=\n  \\phi(.)*\\left( \\frac{\\beta_1}{\\sqrt{1-\\rho^2}} \\right)   \\\\\n   \\frac{\\partial P(y=1|.)}{\\partial z_2} &=\n    \\phi(.)* 0  \\\\\n  \\frac{\\partial P(y=1|.)}{\\partial y_2} &=\n    \\phi(.)* \\left( \\frac{\\beta_2}{\\sqrt{1-\\rho^2}}  \\right)     \n\\end{aligned}\n\\]\nThe differences with the “structural” marginal effects are that the evaluation of \\(\\phi(.)\\) includes the predicted values of the errors (\\(\\hat u_2\\)), and that coefficients used correspond to the two-step procedure ones (rescaled).\nTo show empirically how this works, we can compare the builtin command, with the “two-step” procedure suggested by Prof. Wooldridge:\n\n* two step procedure\n* 1st\nqui: reg $y2 $z1 $z2\npredict double u2, resid\n* 2nd\nqui: probit $y1 $z1 $y2 u2, nolog\n\nmargins, dydx(*) predict(pr)\n\n\nAverage marginal effects                                   Number of obs = 500\nModel VCE: OIM\n\nExpression: Pr(fem_work), predict(pr)\ndy/dx wrt:  fem_educ kids other_inc u2\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    fem_educ |   .0646175   .0060271    10.72   0.000     .0528045    .0764304\n        kids |  -.0557355   .0129302    -4.31   0.000    -.0810782   -.0303929\n   other_inc |  -.0166128   .0022499    -7.38   0.000    -.0210225   -.0122032\n          u2 |   .0068326    .002632     2.60   0.009     .0016741    .0119912\n------------------------------------------------------------------------------\n\n\nThe standard errors here will not be correct, but bootstrap could be applied to obtain corrected standard errors.\nWith the correction to estimation of marginal effects pushed in Novenber of 2020, we can produce the correct point estimates for marginal effects, which follows Prof Wooldrige suggestion, and my discussion presented here.\n** built-in command\nqui:ivprobit $y1  $z1 ($y2 = $z2), ml\nmargins, dydx(*) predict(pr)\n\nAverage marginal effects                        Number of obs     =        500\nModel VCE    : OIM\n\nExpression   : Average structural function probabilities, predict(pr)\ndy/dx w.r.t. : other_inc fem_educ kids\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n   other_inc |  -.0166128   .0012889   -12.89   0.000     -.019139   -.0140867\n    fem_educ |   .0646175   .0073529     8.79   0.000      .050206     .079029\n        kids |  -.0557355   .0144233    -3.86   0.000    -.0840047   -.0274664\n------------------------------------------------------------------------------\nSo you can see that the two-step approach and the built-in approach now provide the same marginal effects. And since the official command estimates all coefficients simultaneously, the standard errors can be taken as correct (more on that later).\nSo how can we correct for this with our predict program. Since there is nothing to prevent margins to obtain numerical derivatives across both equations, we need to modify the specification slighly. First, we create clone copies of all variables that enter the second stage: z_1 and y_2, and use them for the model estimation:\n\nclonevar c_other_inc = other_inc\nclonevar c_fem_educ  = fem_educ\nclonevar c_kids      = kids\nglobal  y2b c_other_inc\nglobal  z1b c_fem_educ c_kids \n\nml model lf myivprobit_2sls ($y1 = $z1  $y2 )  (theta:) ($y2b = $z1b $z2  ) (lnsigma:) , ///\n            technique(nr bhhh)   init(lnsigma:_cons = 2.81 ) maximize nolog\nadde local predict myivprobit_p                 \nest sto myivp   \n\nThe idea of using “clones” of the exogenous variables and endogenous one is to have access to the same information as the original data, but making sure they do not change when the original data changes.\nMarginal effects can be calculated as I did before, except that I now make it explicit to request marginal effects with respect to \\(z_1\\) and \\(y_2\\) only.\n\nest restore myivp       \nmargins, dydx($z1 $y2) predict(pr1) force \n\n(results myivp are active now)\nnote: prediction is a function of possibly stochastic quantities other than\n      e(b).\n\nAverage marginal effects                                   Number of obs = 500\nModel VCE: OIM\n\nExpression: P(y=1|X) two-step, predict(pr1)\ndy/dx wrt:  fem_educ kids other_inc\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    fem_educ |   .0646175    .006331    10.21   0.000     .0522089    .0770261\n        kids |  -.0557355   .0134693    -4.14   0.000    -.0821349   -.0293362\n   other_inc |  -.0166128   .0023499    -7.07   0.000    -.0212185   -.0120072\n------------------------------------------------------------------------------\n\n\nAnd done!. We have been able to reproduce the second version, two-step, marginal effects for the instrumental variable probit model, that follows the two-step approach advocated by Prof. Wooldridge, and officially included in Stata 16 and above.\nThere is only one last perky detail. If you look at the marginal effect standard errors I produce with the myivprobit command, and compare it with the marginal effects the ivprobit command produces, you will notice they are different.\nThe reason for this was that, based on unofficial words from the developers, at the time:\n\nthe current formulation assumes \\(\\rho\\) and \\(\\sigma\\) to be constant, when standard errors are obtained.\n\nWhile this may seem incorrect, I understand the intuition behind this idea.\nIf you recall the estimation of marginal effects from the structural equation, it is not affected by \\(\\rho\\) nor \\(\\sigma\\). Perhaps this was one of the reasons why the estimated standard errors (Nov2020) are so similar to the ones based on the “old” structural marginal effects.\nMy own command, however, accounts for the uncertainty in these parameter. This also seems correct since two-step marginal procedures are expected to be less efficient than the Full Information counterparts.\nOf course, if you prefer to have a tie-breaker on which one is correct, I can use a Bootstrap procedure to produce the elusive standard errors. Basically, I’ll use the manual two-step procedure, along with a 250 bootstrap repetitions, to report the results:\n\nprogram bs_ivprobit, eclass\n    reg $y2 $z1 $z2\n    capture drop u2\n    predict double u2, resid\n    probit $y1 $z1 $y2 u2\n    margins, dydx(*) predict(pr) nose post\nend\nbootstrap , reps(250) seed(1) nodots:bs_ivprobit\n\n\n\nAverage marginal effects                                   Number of obs = 500\n                                                           Replications  = 250\n\n------------------------------------------------------------------------------\n             |   Observed   Bootstrap                         Normal-based\n             | coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    fem_educ |   .0646175   .0058983    10.96   0.000      .053057     .076178\n        kids |  -.0557355   .0138358    -4.03   0.000    -.0828532   -.0286179\n   other_inc |  -.0166128   .0023778    -6.99   0.000    -.0212732   -.0119525\n          u2 |   .0068326   .0027569     2.48   0.013     .0014292    .0122361\n------------------------------------------------------------------------------\n\n\nIn this case, it seems that the bootstrap estimates seem to favor my version of marginal effects and standard errors!",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "The IV-Probit: `margins` and `ml`"
    ]
  },
  {
    "objectID": "stata_do/stata_do4.html#one-last-change.",
    "href": "stata_do/stata_do4.html#one-last-change.",
    "title": "The IV-Probit: margins and ml",
    "section": "One last change.",
    "text": "One last change.\nSeems my guess was correct!. In March of 2021, Stata pushed another update to Stata16. They have now changed how SE are estimated after margins, which now coincides with the experiment I started with.\n\nivprobit $y1  $z1 ($y2 = $z2), ml nolog\nmargins, dydx(*) predict(pr)\n\n\nProbit model with endogenous regressors                 Number of obs =    500\n                                                        Wald chi2(3)  = 163.88\nLog likelihood = -2368.2062                             Prob &gt; chi2   = 0.0000\n\n-------------------------------------------------------------------------------\n              | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n--------------+----------------------------------------------------------------\n    other_inc |  -.0542756   .0060854    -8.92   0.000    -.0662028   -.0423485\n     fem_educ |    .211111   .0268648     7.86   0.000     .1584569    .2637651\n         kids |  -.1820929   .0478267    -3.81   0.000    -.2758315   -.0883542\n        _cons |   .3672086   .4480724     0.82   0.412    -.5109971    1.245414\n--------------+----------------------------------------------------------------\ncorr(e.othe~c,|\n   e.fem_work)|   .3720375   .1300518                      .0946562    .5958136\nsd(e.other_~c)|   16.66621   .5270318                      15.66461    17.73186\n-------------------------------------------------------------------------------\nWald test of exogeneity (corr = 0): chi2(1) = 6.70        Prob &gt; chi2 = 0.0096\nInstrumented: other_inc\n Instruments: fem_educ kids male_educ\n\nAverage marginal effects                                   Number of obs = 500\nModel VCE: OIM\n\nExpression: Average structural function probabilities, predict(pr)\ndy/dx wrt:  other_inc fem_educ kids\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   other_inc |  -.0166128   .0023501    -7.07   0.000    -.0212189   -.0120068\n    fem_educ |   .0646175   .0063309    10.21   0.000     .0522091    .0770258\n        kids |  -.0557355   .0134692    -4.14   0.000    -.0821347   -.0293364\n------------------------------------------------------------------------------\n\n\nSo there is my small contribution to Stata!",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "The IV-Probit: `margins` and `ml`"
    ]
  },
  {
    "objectID": "stata_do/stata_do4.html#conclusion",
    "href": "stata_do/stata_do4.html#conclusion",
    "title": "The IV-Probit: margins and ml",
    "section": "Conclusion",
    "text": "Conclusion\nThe command -ml- is a powerful tool that can be used to estimate single or multiple equation models, as long as the loglikelihood functions (and their inter-relations) can be properly defined.\n-margins- is also a very flexible command that can be easily combined with -ml- to expand the estimation of marginal effects for properly defined outcomes. While the command is flexible and relatively easy to use, these properties can also be double-edge swords, if one is not aware of the mechanics behind the actual estimation of partial effects.\nIn my view, the original estimation of marginal effects after iv-probit was correct, but the changes it received in Stata 14.1 introduced what we could call a bug, that was based on solid Math. However, unless you dig deeper into what ivprobit tries to estimate, it would be difficult to say why that change produced undesirable results.\nThe updates pushed in Stata 16 made the necesary corrections following my suggestions, and now produces correct partial effects (two-step like), even adopting my comment regarding standard errors.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "The IV-Probit: `margins` and `ml`"
    ]
  },
  {
    "objectID": "stata_do/stata_do2.html",
    "href": "stata_do/stata_do2.html",
    "title": "How to Bootstrap",
    "section": "",
    "text": "One of the primary concerns of econometricians and economists is estimating point estimates with precision. However, as point estimates contain errors, we need to estimate the precision of those estimates to determine how reliable they are. This precision is typically expressed as standard errors, which reflect the level of uncertainty in the estimates given the information we have in hand, i.e., the sample.\nIn most introductory econometrics courses, we learn that we can estimate the precision of these estimates by drawing multiple samples, estimating the model of interest for each new sample, and summarizing the estimated coefficients. The standard deviations of the estimated coefficients are the coefficient standard errors, which reflect the variation in the coefficients due to sampling error.\nHowever, collecting multiple samples from the same population is technically impossible (expensive), and we need to rely on other approaches to estimate the standard errors. Two common approaches are typically used:\n\nAsymptotic approximations: where we make use of some of the properties of the estimators (deep knowledge of how those are constructed)\nEmpirical approximations: Or what we can call Bootstrapping.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "How to Bootstrap"
    ]
  },
  {
    "objectID": "stata_do/stata_do2.html#introduction-what-is-bootstrapping",
    "href": "stata_do/stata_do2.html#introduction-what-is-bootstrapping",
    "title": "How to Bootstrap",
    "section": "",
    "text": "One of the primary concerns of econometricians and economists is estimating point estimates with precision. However, as point estimates contain errors, we need to estimate the precision of those estimates to determine how reliable they are. This precision is typically expressed as standard errors, which reflect the level of uncertainty in the estimates given the information we have in hand, i.e., the sample.\nIn most introductory econometrics courses, we learn that we can estimate the precision of these estimates by drawing multiple samples, estimating the model of interest for each new sample, and summarizing the estimated coefficients. The standard deviations of the estimated coefficients are the coefficient standard errors, which reflect the variation in the coefficients due to sampling error.\nHowever, collecting multiple samples from the same population is technically impossible (expensive), and we need to rely on other approaches to estimate the standard errors. Two common approaches are typically used:\n\nAsymptotic approximations: where we make use of some of the properties of the estimators (deep knowledge of how those are constructed)\nEmpirical approximations: Or what we can call Bootstrapping.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "How to Bootstrap"
    ]
  },
  {
    "objectID": "stata_do/stata_do2.html#but-what-is-bootstrapping",
    "href": "stata_do/stata_do2.html#but-what-is-bootstrapping",
    "title": "How to Bootstrap",
    "section": "But What is Bootstrapping?",
    "text": "But What is Bootstrapping?\nAs a non-native speaker, I initially thought that bootstrapping was merely a statistical technique for obtaining empirical standard errors. However, after a few years in grad school, I heard the expression:\n\npull yourself up by your own bootstraps\n\na few times, which describes what bootstrapping does. Since we don’t have access to other samples, we repeatedly use and reuse the same sample in various ways to estimate standard errors for our estimates.\nThe differences in how we reuse the sample information determine the type of bootstrapping method we’re using.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "How to Bootstrap"
    ]
  },
  {
    "objectID": "stata_do/stata_do2.html#types-of-bootstrapping.",
    "href": "stata_do/stata_do2.html#types-of-bootstrapping.",
    "title": "How to Bootstrap",
    "section": "Types of Bootstrapping.",
    "text": "Types of Bootstrapping.\nThere are many approaches to obtaining bootstrap standard errors, depending on the assumptions we’re willing to impose on the data, and not all of them can be applied in every scenario. For simplicity, I’ll refer to the ones that can be used for linear regressions.\nSuppose you’re interested in a linear regression model with the following functional form:\n\\[\ny_i=X_i\\beta+e_i\n\\]",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "How to Bootstrap"
    ]
  },
  {
    "objectID": "stata_do/stata_do2.html#setup-and-asymptotic-se",
    "href": "stata_do/stata_do2.html#setup-and-asymptotic-se",
    "title": "How to Bootstrap",
    "section": "Setup and Asymptotic SE",
    "text": "Setup and Asymptotic SE\nTo get started with bootstrapping, we will estimate a very simple linear regression model using the auto.dta dataset.\n\n\nCode\nset linesize 255\nprogram drop _all\nsysuse auto, clear\nreg price mpg foreign\n\n\n\n\n\n(1978 automobile data)\n\n      Source |       SS           df       MS      Number of obs   =        74\n-------------+----------------------------------   F(2, 71)        =     14.07\n       Model |   180261702         2  90130850.8   Prob &gt; F        =    0.0000\n    Residual |   454803695        71  6405685.84   R-squared       =    0.2838\n-------------+----------------------------------   Adj R-squared   =    0.2637\n       Total |   635065396        73  8699525.97   Root MSE        =    2530.9\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -294.1955   55.69172    -5.28   0.000    -405.2417   -183.1494\n     foreign |   1767.292    700.158     2.52   0.014     371.2169    3163.368\n       _cons |   11905.42   1158.634    10.28   0.000     9595.164    14215.67\n------------------------------------------------------------------------------\n\n\nThis estimation provides the asymptotic estimation of standard errors under homoskedasticity using the well-known formula:\n\\[\nVar(\\hat \\beta) = \\frac{\\sum \\hat e ^2}{n-k-1} (X'X)^{-1}\n\\]\nLet’s see how this standard errors compare to different types of Bootstrap Standard errors:\n\nParametric Bootstrap\nAs the name suggest, parametric bootstrap requires imposing some parametric assumptions on the source of the error in the model: \\(e\\). We will learn those characteristics from the estimated errors in the original sample.\nSay, for example, that we assume \\(e\\) follows some normal distribution, with variance equal to the variance of the observed error \\(\\sigma^2_e=Var(\\hat e)\\). Parametric bootstrapping would require to draw/create different samples using the following rule: \\[\n\\tilde y_i = X_i \\hat \\beta + \\tilde e_i \\ where \\ \\tilde e_i \\sim N(0,var(\\hat e))\n\\]\nIn this case, all \\(X's\\) are fixed, but the new samples are created by resampling the error, and constructing \\(\\tilde y's\\). This differs only because of the draws of the errors \\(\\tilde e\\).\nOnce you get multiple of samples, and coefficients for each, you can simply Report the associated Standard Errors.\nHow to do it in Stata? Here I will cheat a bit, and use Mata, because it will be faster. Note that you will need to copy the whole code into a dofile and run all of it, or type each line individualy in the command window (once you activate Mata):\n\n\nCode\nset seed 10\n// First we start mata\nmata:\n    // load all data\n    y = st_data(.,\"price\")\n    // You load X's and the constant\n    x = st_data(.,\"mpg foreign\"),J(rows(y),1,1)\n    // Estimate Betas:\n    b = invsym(x'*x)*x'y\n    // Estimate errors:\n    e = y:-x*b\n    // Estimate STD of errors\n    std_e=sqrt(sum(e:^2)/73)\n    // Now we can do the bootstrap:\n    // We first create somewhere to store the different betas\n    bb = J(1000,3,.)\n    // and start a loop\n    for(i=1;i&lt;=1000;i++){\n        // each time we draw a different value for y..say ys\n        ys = x*b+rnormal(74,1,0,std_e)\n        // and estimate the new beta, storing it into bb\n        bb[i,]=(invsym(x'*x)*x'ys)'\n    }\nend\n\n\nIf everythings goes well, it should give you the following\n\n\nCode\nmata: b,diagonal(sqrt(variance(bb)))\n\n\n                  1              2\n    +-------------------------------+\n  1 |  -294.1955331    55.59108401  |\n  2 |   1767.292243    689.3703271  |\n  3 |   11905.41528    1159.921784  |\n    +-------------------------------+\n\n\nNotice that we are explicitly impossing the assumption of homoskedasticity and normality on the errors. This explain why this standard errors are almost identical to the simple asymptotic standard errors.\n\n\nResidual Bootstrap\nResidual bootstrap is very similar to the parametric bootstrap I described above. The main difference is that we no longer impose assumptions on the errors distributions, and instead use the empirical distribution.\nWhat does this mean? Well, In the above example, we have 74 different values for the error \\(e\\), thus resampling means that you create a new \\(\\tilde y\\) by drawing 74 errors from this bag of errors, where all have the same probability of being choosen. \\[\n\\tilde y_i  =  X_i \\hat \\beta  + \\tilde e_i \\ where  \\ \\tilde e \\sim [\\hat e_1, \\hat e_2,...,\\hat e_N]\n\\]\nLets implement it:\n\n\nCode\nset seed 10\nmata:\n    // This remains the same as before\n    y = st_data(.,\"price\")\n    x = st_data(.,\"mpg foreign\"),J(rows(y),1,1)\n    b = invsym(x'*x)*x'y\n    e = y:-x*b\n    bb = J(1000,3,.)\n    // Now we need to know how many observations we have\n    nobs=rows(y)\n        for(i=1;i&lt;=1000;i++){\n        // Here is where we \"draw\" a different error everytime, \n        // runiformint(nobs,1,1,nobs) &lt;- This says Choose a randome number between 1 to K\n        // and use that value to assing as the new error to create ys\n        ys = x*b+e[runiformint(nobs,1,1,nobs)]\n        bb[i,]=(invsym(x'*x)*x'ys)'\n    }   \nend\n\n\n\n\nCode\nmata: b,diagonal(sqrt(variance(bb)))\n\n\n                  1              2\n    +-------------------------------+\n  1 |  -294.1955331    53.63708346  |\n  2 |   1767.292243    703.1172879  |\n  3 |   11905.41528    1118.818836  |\n    +-------------------------------+\n\n\nOnce again, this method keeps \\(X's\\) fixed, and assumes errors are fully homoskedastic, thus interchangable. It does allow for the possiblity errors do not follow a normal distribution.\n\n\nWild-Bootstrap/multiplicative bootstrap\nThe wild bootstrap is another variant of residual bootstrapping. To implement it, we start by estimating the original model and obtaining the model errors. Instead of shuffling or making assumptions about the distribution of the errors, we reuse the error after adding noise to it. Mathematically, this can be expressed as: \\[\n\\tilde y_i=X_i \\hat \\beta + \\hat e_i * v_i\n\\]\nHere, \\(v\\) is the source of the noise that we add to the model. Technically, we can use any distribution for \\(v\\), as long as \\(E(v)=0\\) and \\(Var(v)=1\\). The most common distribution used in wild bootstrap implementations is the “mammen” distribution, but for simplicity, we will use a normal distribution.\n\n\nCode\nset seed 10\nmata:\n    y = st_data(.,\"price\")\n    x = st_data(.,\"mpg foreign\"),J(rows(y),1,1)\n    b = invsym(x'*x)*x'y\n    e = y:-x*b\n    nobs=rows(y)\n    bb = J(1000,3,.)\n    for(i=1;i&lt;=1000;i++){\n        // Here is where we \"draw\" a different error multiplying the original error by v ~ N(0,1)\n        ys = x*b+e:*rnormal(nobs,1,0,1)\n        bb[i,]=(invsym(x'*x)*x'ys)'\n    }\nend\n\n\n\n\nCode\nmata: b,diagonal(sqrt(variance(bb)))\n\n\n                  1              2\n    +-------------------------------+\n  1 |  -294.1955331    59.62775803  |\n  2 |   1767.292243     586.545592  |\n  3 |   11905.41528    1357.383088  |\n    +-------------------------------+\n\n\nSurprisingly, this approach allows us to control for heteroskedasticity, which is why the standard errors obtained using the wild bootstrap method are quite similar to the ones obtained using reg, robust.\nAnother advantage of this method is that we do not necessarily need to obtain an estimate of the error itself. Instead, we can obtain the Influence Functions of the estimated parameters and disturb those to obtain standard errors. This makes the method feasible for a larger set of estimators, assuming that we can derive the corresponding Influence Functions.\n\n\nPaired bootstrap/Nonparametric bootstrap\nPaired bootstrap is perhaps the most commonly used method in applied econometrics, although it can also be computationally intensive.\nThe basic idea is to use the original sample to draw subsamples with replacement that are of the same size. Then, you estimate the parameters of interest for each subsample and summarize the results. What sets this approach apart from others is that the entire set of observations and characteristics are used in the resampling, not just the residuals. This makes it robust to heteroskedasticity and relatively easy to implement for complex model estimators. Stata has a module dedicated to making the implementation of paired bootstrap easy, but it can also be implemented in Mata.\n\n\nCode\nset seed 10\nmata:\n    y = st_data(.,\"price\")\n    x = st_data(.,\"mpg foreign\"),J(rows(y),1,1)\n    b = invsym(x'*x)*x'y\n    e = y:-x*b\n    nobs=rows(y)\n    bb = J(1000,3,.)\n    for(i=1;i&lt;=1000;i++){\n        // What I do here is get a vector that will identify the resampling.\n        r_smp = runiformint(nobs,1,1,nobs)\n        // then use this resampling vector to reestimate the betas\n        brs = invsym(x[r_smp,]'*x[r_smp,])*x[r_smp,]'y[r_smp,]\n        bb[i,]=brs'\n    }   \nend\n\n\n\n\nCode\nmata: b,diagonal(sqrt(variance(bb)))\n\n\n                  1              2\n    +-------------------------------+\n  1 |  -294.1955331    61.88438186  |\n  2 |   1767.292243    575.3963704  |\n  3 |   11905.41528    1374.021799  |\n    +-------------------------------+\n\n\n\n\nEasier Paired bootstrap\nAssuming you do not like to do this with Mata, or that your estimator is a bit more complex than a simple OLS, a better approach for implementing paired bootstrap in Stata is simply using the bootstrap prefix:\nFor example:\nbootstrap, seed(10) reps(1000):reg price mpg foreign\nAnd of course, this approach can implement to bootstrap any 1 line command, although it may be faster for some methods than others.:\nbootstrap, seed(10) reps(1000):qreg price mpg foreign\nbootstrap, seed(10) reps(1000):poisson price mpg foreign\n\n\nBootstrapping a two-step regression\nOften, however, you may want to “bootstrap” something more complex. For example a two/three/…K step estimator. You can still use bootstrap, but it requires a bit more programming. So lets go with a simple 2-step heckman estimator. My recommendation, first implement the estimator for 1 run:\n\n\nCode\nwebuse womenwk, clear\n** data prep\ngen dwage=wage!=.\n** estimation\nprobit dwage married children educ age\npredict mill, score\nreg wage educ age mill\n** delete the variable that was created as intermediate step\ndrop mill\n\n\nNotice that mill was dropped at the end. This is important, because by bootstraping the program, it will beed to be created all over again. Finally, we write our little bootstrap program:\n\n\nCode\n** I like to add eclass properties here\nprogram two_heckman, eclass\n    capture drop  mill  \n    ** you implement your estimator:\n    tempvar smp\n    probit dwage married children educ age\n    predict mill, score\n    ** save the \"sample\" from probit\n    gen byte `smp'=e(sample)\n    reg wage educ age mill\n    ** Delete all variables that were created \n    ** Finally, you will Store all the coefficients into a matrix\n    matrix b=e(b)\n    ** and \"post\" them into e() so they can be read as an estimation output\n    ereturn post b, esample(`smp')\nend\n\n\n\n\n\nAnd apply the bootsrap prefix to it:\n\n\nCode\nbootstrap, reps(250) nodots:two_heckman\n\n\n\nBootstrap results                                        Number of obs = 2,000\n                                                         Replications  =   250\n\n------------------------------------------------------------------------------\n             |   Observed   Bootstrap                         Normal-based\n             | coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   education |   .9825259   .0511768    19.20   0.000     .8822212    1.082831\n         age |   .2118695   .0223345     9.49   0.000     .1680947    .2556444\n        mill |   4.001615   .5898434     6.78   0.000     2.845544    5.157687\n       _cons |   .7340391   1.287607     0.57   0.569    -1.789625    3.257703\n------------------------------------------------------------------------------",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "How to Bootstrap"
    ]
  },
  {
    "objectID": "stata_do/stata_do2.html#conclusions",
    "href": "stata_do/stata_do2.html#conclusions",
    "title": "How to Bootstrap",
    "section": "Conclusions",
    "text": "Conclusions\nThis post provides an overview of how bootstrap operates and how to implement it using Stata.\nRemember, not all techniques are suitable for all circumstances. Furthermore, to accurately estimate standard errors while taking into account clustering and weights, resampling methods need to also account for the original sampling structure, or how the data was gathered.\nAccounting for clustering is often straightforward, but handling weights and Strata may require additional attention.\nI hope you found this information beneficial, and please don’t hesitate to reach out with any questions or feedback via email or comments.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "How to Bootstrap"
    ]
  },
  {
    "objectID": "stata_do/index.html",
    "href": "stata_do/index.html",
    "title": "Stata Programming",
    "section": "",
    "text": "Welcome to the Stata programming section! As a “seasoned” (just another word for oldish) Stata user, I have developed quite few tricks of knowledge and experience in programming with Stata and Mata. I am excited to share my insights with you.\nStata programming is a powerful tool that can enhance your data analysis and streamline your research workflows. For instance, if you do the same thing over and over, why not write a program that will replicate it as well!. Of course, it can be daunting for beginners, and even experienced users may encounter challenges and roadblocks in their programming endeavors (The more skillful you become, the harder the problems you face!).\nIn this section, I provide hints and suggestions for programming, some times replicating simulation excercises, some times for making examples statistical methods, and some times just for fun.\nHere, I will try to cover a wide range of programming topics, from basic syntax and functions to advanced techniques (depending on my own skills), providing examples and code snippets to demonstrate how the code works.\nMy main goal is to help you and myself becoming a more proficient Stata programmer. I hope you find this section informative and useful, and I look forward to helping you advance your Stata programming skills!\nComments and suggestions are always welcome.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Programming"
    ]
  },
  {
    "objectID": "stataviz/stataviz5.html",
    "href": "stataviz/stataviz5.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "In this post, I will show some of the basics of data visualization using the Palmer Pinguins dataset. This dataset is available in R using the palmerpenguins package. And now there is also a copy of in using frause for Stata. The dataset contains data on 344 penguins collected from 3 islands in the Palmer Archipelago, Antarctica. The data was collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Palmer Penguins"
    ]
  },
  {
    "objectID": "stataviz/stataviz5.html#introduction",
    "href": "stataviz/stataviz5.html#introduction",
    "title": "Palmer Penguins",
    "section": "",
    "text": "In this post, I will show some of the basics of data visualization using the Palmer Pinguins dataset. This dataset is available in R using the palmerpenguins package. And now there is also a copy of in using frause for Stata. The dataset contains data on 344 penguins collected from 3 islands in the Palmer Archipelago, Antarctica. The data was collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Palmer Penguins"
    ]
  },
  {
    "objectID": "stataviz/stataviz5.html#setup",
    "href": "stataviz/stataviz5.html#setup",
    "title": "Palmer Penguins",
    "section": "Setup",
    "text": "Setup\nFirst, we need to install the frause package. This can be done using the ssc install command. We will also need some additional commands to change colors of shemes, and create and select custom colors. Please copy the following code to your do-file, or run it from your command line.\n\n\n\n\n\ngraph size was (5.5in, 4in), is now (default, default).\n\n\n\nssc install frause\nssc install color_style\nnet install colrspace, replace from(https://raw.githubusercontent.com/benjann/colrspace/master/)\nnet install palettes, replace from(https://raw.githubusercontent.com/benjann/palettes/master/)\nnet install grstyle, replace from(https://raw.githubusercontent.com/benjann/grstyle/master/)\n\nNow, lets load the data, and set a cleaner scheme than the default. We will also set the default color scheme to tableau.\n\nset scheme white2\ncolor_style tableau\nfrause penguins, clear",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Palmer Penguins"
    ]
  },
  {
    "objectID": "stataviz/stataviz5.html#basic-plots-scatter-plot",
    "href": "stataviz/stataviz5.html#basic-plots-scatter-plot",
    "title": "Palmer Penguins",
    "section": "Basic plots: Scatter plot",
    "text": "Basic plots: Scatter plot\nLets start with a basic Scatter plot\n\ntwoway scatter flipper_length_mm body_mass_g, /// Creates a Scatter plot\n    ytitle(\"Flipper length (mm)\") ///             Sets the title for the y-axis\n    xtitle(\"Body mass (g)\") ///                   Sets the title for the x-axis\n    legend(off) scale(1.1) ///                    Set legends off, and scale the plot by 1.1 (fonts) \n    xsize(9) ysize(6) //                          Finally, setting Size of the plot  \n\n\n\n\n\n\n\n\nBut we can also add some additional information to the plot. For example, we can color the points by the species of the penguin. We can change the markers, and finally, add a title and subtitle to the plot.\n\ntwoway (scatter body_mass_g flipper_length_mm if species == 1, msymbol(o) ) /// changes Symbol to a circle\n    (scatter body_mass_g flipper_length_mm if species == 2, msymbol(t) ) /// changes Symbol to a triangle, etc\n    (scatter body_mass_g flipper_length_mm if species == 3, msymbol(s) ) , /// Creates a Scatter plot by groups\n    ytitle(\"Flipper length (mm)\") ///             Sets the title for the y-axis\n    xtitle(\"Body mass (g)\") ///                   Sets the title for the x-axis\n    legend(order(1 \"Adelie\" 2 \"Chinstrap\" 3 \"Gentoo\") ring(0)) ///  Adds text to the legend  \n    title(\"Penguin size, Palmer Station LTER\", pos(11) span )  /// Adds a title to the plot             \n    subtitle(\"Flipper lengh and body mass by species\", pos(11) span )  /// Adds a title to the plot             \n    scale(1.1) xsize(9) ysize(6) //                                 Scales the plot by 1.1 (fonts) \n\n\n\n\n\n\n\n\nOf course, we could also add a regression line to the plot. On top of that, I will change the colors and size of the symbols. For this example, I will use different variables: Bill lenght and Flipper lenght. Changing the colors i will use color_style:\n\n** This line changes the color of the scheme, setting the first n colors based \n** on a list of colors, or a palette. See help colorpalette for more options\ncolor_style darkorange purple LightSeaGreen\n\ntwoway (scatter bill_length_mm flipper_length_mm if species == 1, msymbol(o) ) /// \n    (scatter bill_length_mm flipper_length_mm if species == 2, msymbol(t) ) /// \n    (scatter bill_length_mm flipper_length_mm if species == 3, msymbol(s) ) ///\n    (lfit bill_length_mm flipper_length_mm if species == 1, pstyle(p1) color(%60)) /// Creates fitted lines\n    (lfit bill_length_mm flipper_length_mm if species == 2, pstyle(p2) color(%60)) /// Need to use pstyle to keep colors\n    (lfit bill_length_mm flipper_length_mm if species == 3, pstyle(p3) color(%60)), /// consistent\n    ytitle(\"Bill length (mm)\") ///             Sets the title for the y-axis\n    xtitle(\"Flipper lenght (mm)\") ///                   Sets the title for the x-axis\n    legend(order(1 \"Adelie\" 2 \"Chinstrap\" 3 \"Gentoo\") ring(0) pos(5)) ///  Adds text to the legend  \n    title(\"Flipper and Bill length\", pos(11) span )  /// Adds a title to the plot             \n    subtitle(\"Dimensions by species\", pos(11) span )  /// Adds a title to the plot             \n    scale(1.1) xsize(9) ysize(6) //",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Palmer Penguins"
    ]
  },
  {
    "objectID": "stataviz/stataviz5.html#basic-plots-histogram",
    "href": "stataviz/stataviz5.html#basic-plots-histogram",
    "title": "Palmer Penguins",
    "section": "Basic plots: Histogram",
    "text": "Basic plots: Histogram\n\nhistogram flipper_length_mm, lcolor(*.80) /// Creates a histogram, and \n    ytitle(\"Frequency\") ///             Sets the title for the y-axis\n    xtitle(\"Flipper length (mm)\") ///                   Sets the title for the x-axis\n    title(\"Penguin Flipper length\", pos(11) span )  /// Adds a title to the plot             \n    subtitle(\"Distribution of flipper length\", pos(11) span )  /// Adds a title to the plot             \n    scale(1.1) xsize(9) ysize(6) //                                 Scales the plot by 1.1 (fonts)                                Scales the plot by 1.1 (fonts) \n\n(bin=18, start=172, width=3.2777778)\n\n\n\n\n\n\n\n\n\nNow by color. We will need to set width and binwidth to make the plot look better.\n\ntwo (histogram flipper_length_mm if species==1, color(%50) lcolor(%0) ///\n    frequency start(170) width(2) pstyle(p1)) ///\n    (histogram flipper_length_mm if species==2, color(%50) lcolor(%0) ///\n    frequency start(170) width(2) pstyle(p2)) ///\n    (histogram flipper_length_mm if species==3, color(%50) lcolor(%0) ///\n    frequency start(170) width(2) pstyle(p3)), ///\n    ytitle(\"Frequency\") ///                             Sets the title for the y-axis\n    xtitle(\"Flipper length (mm)\") ///                   Sets the title for the x-axis\n    legend(order(1 \"Adelie\" 2 \"Chinstrap\" 3 \"Gentoo\")  ) ///  Adds text to the legend  \n    title(\"Penguin Flipper length\", pos(11) span )  /// Adds a title to the plot             \n    subtitle(\"Distribution of flipper length\", pos(11) span )  /// Adds a title to the plot             \n    scale(1.1) xsize(9) ysize(6) //                                 Scales the plot by 1.1",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Palmer Penguins"
    ]
  },
  {
    "objectID": "stataviz/stataviz5.html#basic-plots-scatter-plot-by",
    "href": "stataviz/stataviz5.html#basic-plots-scatter-plot-by",
    "title": "Palmer Penguins",
    "section": "Basic plots: scatter plot by",
    "text": "Basic plots: scatter plot by\n\ncolor_style darkorange   LightSeaGreen\n\ntwo (scatter body_mass_g flipper_length_mm if sex==1) ///\n    (scatter body_mass_g flipper_length_mm if sex==2), ///\n    by(species, col(3) note(\"\")  ///\n    title(\"Penguin Flipper and Body Mass\", pos(11) span )  /// Adds a title to the plot             \n    subtitle(\"Dimensions for Male and Female\", pos(11) span ))  /// Adds a title to the plot \n    legend(order(1 \"Female\" 2 \"Male\") cols(2) ) ///\n    ytitle(\"Body Mass (g)\") ///             Sets the title for the y-axis\n    xtitle(\"Flipper lenght (mm)\") ///                   Sets the title for the x-axis\n    scale(1.1) xsize(9) ysize(6) //",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Palmer Penguins"
    ]
  },
  {
    "objectID": "stataviz/stataviz5.html#conclusions",
    "href": "stataviz/stataviz5.html#conclusions",
    "title": "Palmer Penguins",
    "section": "Conclusions",
    "text": "Conclusions\nSo this was just for fun, as I wanted to produce this, and use an extra fix of nbstata Enjoy",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Palmer Penguins"
    ]
  },
  {
    "objectID": "stataviz/stataviz3.html",
    "href": "stataviz/stataviz3.html",
    "title": "How to make Waffles: waffle_plot",
    "section": "",
    "text": "Let me start by stating that what I am about to present is not the only way. It is simply the approach I prefer, particularly after some flexing some programming mussiles, and when I wish to avoid repetitive typing.\nHaving said that, I want to acknowledge that step-by-step guides on this type of plots can be found elsewhere. They can be quite beneficial, particularly if the command I am presenting does not meet your needs.\nLearning how to create these plots step-by-step provides greater flexibility, but I believe that using the waffle_plot (available from SSC and fra) is versatile enough for most purposes.\nSo, without further ado, let’s get started.",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "How to make Waffles: `waffle_plot`"
    ]
  },
  {
    "objectID": "stataviz/stataviz3.html#disclaimer",
    "href": "stataviz/stataviz3.html#disclaimer",
    "title": "How to make Waffles: waffle_plot",
    "section": "",
    "text": "Let me start by stating that what I am about to present is not the only way. It is simply the approach I prefer, particularly after some flexing some programming mussiles, and when I wish to avoid repetitive typing.\nHaving said that, I want to acknowledge that step-by-step guides on this type of plots can be found elsewhere. They can be quite beneficial, particularly if the command I am presenting does not meet your needs.\nLearning how to create these plots step-by-step provides greater flexibility, but I believe that using the waffle_plot (available from SSC and fra) is versatile enough for most purposes.\nSo, without further ado, let’s get started.",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "How to make Waffles: `waffle_plot`"
    ]
  },
  {
    "objectID": "stataviz/stataviz3.html#what-is-a-waffle-plot",
    "href": "stataviz/stataviz3.html#what-is-a-waffle-plot",
    "title": "How to make Waffles: waffle_plot",
    "section": "What is a waffle plot?",
    "text": "What is a waffle plot?\nA waffle plot is a type of chart similar to a pie chart (how interesting we like to name them as foods), because they can be used to visualize sample compositions.\nThe key difference between a waffle plot and a pie chart is the way the information is displayed. Whereas a pie chart depicts the composition as pieces of a circle, a waffle plot represents it as dots in a box. Interestingly, the dots in a waffle plot resemble a waffle, hence the name.\n\n\n\nwaffle",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "How to make Waffles: `waffle_plot`"
    ]
  },
  {
    "objectID": "stataviz/stataviz3.html#but-what-are-they-really",
    "href": "stataviz/stataviz3.html#but-what-are-they-really",
    "title": "How to make Waffles: waffle_plot",
    "section": "But what are they, really?",
    "text": "But what are they, really?\nIf you want an insider look of a waffle plot, they are nothing else than a scatter plot within a box. The points are spaced out evenly both horizontally and vertically, which makes it a bit harder to display detailed information. For example in a 10x10 scatterplot, you cannot differentiate between 13.6 and 13.7.\nMaking waffle plots isn’t too tricky, but they can take a bit of time. Thus, I decided to write a small ado that makes creating these type of plots easily.\nIn what follows, I will show you how to use waffle_plot to make this type of plots in Stata!. But first, I would suggest you to get the following two packages from ssc (or fra):\nssc install waffle_plot\nssc install color_style",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "How to make Waffles: `waffle_plot`"
    ]
  },
  {
    "objectID": "stataviz/stataviz3.html#making-waffles",
    "href": "stataviz/stataviz3.html#making-waffles",
    "title": "How to make Waffles: waffle_plot",
    "section": "Making Waffles",
    "text": "Making Waffles\n\nSimple waffle plot\nSay that you want to do a waffle plot, and want to show that 35% of individuals do something. If so, you will type:\n\nset scheme white2\ncolor_style tableau\nwaffle_plot 35 \n\n\n\n\n\n\n\n\n\n\n\nWhen used like this, waffle_plot acts like an immediate command. You just feed it data, and it makes the waffle.\nHowever, as you can see on above, the simple waffle is not very attractive. So lets try improve on it.\n\n\nBeautify\nYou can change the size and symbol. However, you need transfer that information using sctopt (scatter options). Although, if the options are related to the marker options, you can add them directly to command line.\n\nwaffle_plot 35, msize(6)        /// Increase size of Dots\n                msymbol(square) /// and change them to Squares\n                legend(order(1 \"The blues\" 2 \"The rest\")) /// ads Labels\n                title(\"First nice? Waffle\")\n\n\n\n\n\n\n\n\nOf course, if your goal is to show the distribution for more than just 1 group, you can add more numbers to the command. However, the command will only recognize new numbers as long as the total is below 100. If the total is below 100, the last group is considered the “rest”, and colors for that group is assigned using option color0. If the total is above 100, the last values will be ignored, unless you use the option total[(number)]. To either rescale it based on the information given, or providing a “Total” for rescaling.\n\nwaffle_plot 12 45 7, /// lets use 3 groups\n    sctopt(msize(6) msymbol(square)) /// with some sct options\n    color0(gs10) /// adding color to the \"rest\" category\n    color(blue red) /// and to the other two\n    legend(order(1 \"Democrats\" 2 \"Republicans\" 3 \"Green Party\"))\n\n\n\n\n\n\n\n\nSomething else I added to the command was the ability of chaning the dimensions of the BOX. By default, your waffle is a 10x10 matrix. this can be changed using xnobs() and ynobs().\n\nwaffle_plot 12 45 7, /// Uses numbers between 0-1.\n  msize(6) msymbol(square) mlcolor(black) /// Other SCT options\n  color0(gs10) color(blue red green) /// Color for rest and each group\n  legend(order(1 \"Rep\" 2 \"dem\" 3 \"green\" 4 \"Undecided\")) xnobs(12) ynobs(8)",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "How to make Waffles: `waffle_plot`"
    ]
  },
  {
    "objectID": "stataviz/stataviz3.html#further-use-beyond-immediate",
    "href": "stataviz/stataviz3.html#further-use-beyond-immediate",
    "title": "How to make Waffles: waffle_plot",
    "section": "Further use: Beyond immediate",
    "text": "Further use: Beyond immediate\nThe previous section shows how to produce waffle plots providing the data you want to plot. But what if you want to use your own data set. You can certainly estimate the numbers yourself, and then make the plot, but it may be convinient using data from a stored dataset.\nDoing that would even allow you to create multiple plots using by(). I provide some examples next mix and matching many options.\n\nset graph off\nsysuse pop2000, clear\nwaffle_plot black indian asian island, /// provides 3 variables with Pop for each\nlegend(order(1 \"Black\" 2 \"Indian\" 3 \"Asian\" 4 \"Island\") cols(4)) ///\nmsize(3) mlcolor(black)  /// Scatter options\ntotal  /// This is different. Im asking to construct shares internally\nby(agegrp, title(\"Population Composition\") /// Using by(), Notice (sub)Title\ncompact subtitle(\"by Age group\") note(\"\") cols(6)) /// and Cols\nxnobs(15)  // Just Because I want to show a rectangle\ngraph export sv10.png, height(500) width(800) replace\n\n\nThis first one uses data directly from the cesus, to plot the proportion of Blacks, indian, asiand, and islanders by age group in the US. I excluded white population on purpose, so i want the numbers create the shares based on this 5 groups only.\n\nsysuse cancer, clear\nwaffle_plot died, ///\ncolor0(gs15) /// Color for the left out \nlegend(off) /// Legend\nby(drug, cols(3)) msize(3) ///\ntitle(\"Share of people who died by Drug type\") ///\nflip nobs(15)   // two new things. FLIP and increase to 15x15 s\ngraph export sv11.png, height(500) width(800) replace\n\n\nThis one plots the share of people who died based on access to a particular drug. It uses nobs() to allow for more Dots, and flip, for a different way of presenting the information.\n\nsysuse voter, clear\nxi, noomit:waffle_plot i.cand [w=pop], /// Notice weights, and \"factor\"\ncolor(cranberry*0.7 navy*0.7 forest_green*.5) /// Special colors\nmsize(4) mlcolor(black%50) ///\nby(inc, title(\"1992 President Elections\") subtitle(\"by Family Income\")) ///\nlegend(order(1 \"Clinton\" 2 \"Bush\" 3 \"Perot\") cols(3)) \ngraph export sv12.png, height(500) replace\n\n\nThis last one shows you can use the option “factor”, you can also use weights, and make use of special colors selection.",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "How to make Waffles: `waffle_plot`"
    ]
  },
  {
    "objectID": "stataviz/stataviz3.html#conclusions",
    "href": "stataviz/stataviz3.html#conclusions",
    "title": "How to make Waffles: waffle_plot",
    "section": "Conclusions",
    "text": "Conclusions\nBecause waffle_plot is just a collection of scatter plots, you can use most twoway graph options with this command. Although I have not tried all of them. So handle with care.\nIn any case, this command does the heavy lifting, you can do the fineness.\nTil next time",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "How to make Waffles: `waffle_plot`"
    ]
  },
  {
    "objectID": "stataviz/stataviz1.html",
    "href": "stataviz/stataviz1.html",
    "title": "How to add Colors to your Life: color_style",
    "section": "",
    "text": "This is the way\nI should start by saying that what I will be presenting here is not the only way. There are other ways to create and modify your schemes however you would like to do. Most people provide you with the full set of schemes for that purpose.\nThat being said, the command I’ll be presenting depends on a few tools built by Ben Jann: colorpalette, colorspace, as well as grstyle.\nIt also has only 1 purpose. Change the color settings from your current “scheme” to any other palette either available in colorpalette, or provided by the program.\nSo let me start",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "How to add Colors to your Life: `color_style`"
    ]
  },
  {
    "objectID": "stataviz/stataviz1.html#so-how-does-it-work",
    "href": "stataviz/stataviz1.html#so-how-does-it-work",
    "title": "How to add Colors to your Life: color_style",
    "section": "So how does it work?",
    "text": "So how does it work?\nAs I mentioned earlier, there are two very useful programs that mine builds on. These are colorpalette and grstyle, both by Ben Jann.\n\ncolorpalette helps create colors, call on palettes, modify properties, and translate them in a way that Stata will understand.\ngrstyle helps modifying the current scheme, so all graphs done afterward will use the modify schemes.",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "How to add Colors to your Life: `color_style`"
    ]
  },
  {
    "objectID": "stataviz/stataviz1.html#where-does-mine-fit",
    "href": "stataviz/stataviz1.html#where-does-mine-fit",
    "title": "How to add Colors to your Life: color_style",
    "section": "Where does mine fit?",
    "text": "Where does mine fit?\nMy program color_style is in fact a wrapper around colorpalette and grstyle. You can use it to call on a particular color palette, and will automatically apply it into your scheme.\nIt also brings some other colorpalettes that can be stored in memory, and applied to your graphs, but enough talk, lets see how it works!\nFirst, setup. make sure you have the following programs installed in your computer\nssc install palettes\nssc install colrspace  \nssc install grstyle\n\n* And of course my program\n\nssc install color_style \n\n\n\n\n\n%set error(s):\n    'graph_width' configuration invalid. Reverting to: graph_width = 5.5in\n    'graph_height' configuration invalid. Reverting to: graph_height = 4in\n\n\nLets start using default options for something simple. A pie Graph.\n\n\nCode\nsysuse auto, clear\nset scheme s2color\ngraph pie mpg, over(rep78)\n\n\n(1978 automobile data)\n\n\n\n\n\n\n\n\n\nThis creates a pie graph with average MPG’s by number of repairs in 1978. But what I didn’t like default colors, you can request a change:\n\n\nCode\nset scheme white2\ncolor_style viridis, n(5)\ngraph pie mpg, over(rep78)\n\n\n\n\n\n\n\n\n\nHere, I’ll be using Viridis palette, but requesting only 5 colors. One can request up to 15 different colors.",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "How to add Colors to your Life: `color_style`"
    ]
  },
  {
    "objectID": "stataviz/stataviz1.html#i-want-more",
    "href": "stataviz/stataviz1.html#i-want-more",
    "title": "How to add Colors to your Life: color_style",
    "section": "I want more!",
    "text": "I want more!\nBut every body uses Viridis, what other palette can I use?\nYou can look into the helpfile of colorpalette for a large set of colors. But, color_style also comes with its own set!. Just type: color_style, list, and a list of additional palettes will come up.\nBut you want to see examples! Below, you can see three examples, first using egypt color palette, second using tableau, and third using your own colors\n\n\nCode\ncolor_style egypt,\ngraph pie mpg, over(rep78) name(m1, replace)\n\n**\ncolor_style tableau\ngraph pie mpg, over(rep78) name(m2, replace)\n\n** or your own\ncolor_style #01161E #124559 #598392 #AEC3B0 #EFF6E0\ngraph pie mpg, over(rep78) name(m3, replace)",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "How to add Colors to your Life: `color_style`"
    ]
  },
  {
    "objectID": "stataviz/stataviz1.html#how-does-it-work",
    "href": "stataviz/stataviz1.html#how-does-it-work",
    "title": "How to add Colors to your Life: color_style",
    "section": "How does it work?",
    "text": "How does it work?\nNow that you know the magic, it is fair to know the trick.\nThis command color_style works as a wrapper behind colorpalette. It basically passes the information to this program to translate and create colors from either palettes, or directly provided colors, so it translates into RGB color code that Stata understands.\nAfter that, grstyle uses this information to change the colors of all graphic related styles (p1-p15), which is why there is a limitation of how many colors one can define using this method.\nIn any case, once you decide you like certain scheme, this small program should help to make it easy change colors, based on your needs.",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "How to add Colors to your Life: `color_style`"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Stata: Contributed packages",
    "section": "",
    "text": "Hi, I can see you are interested in some of the programs I have written over the years. I have quite a few that are available from SSC. However, I recently open my own repository\nhttps://github.com/friosavila/stpackages\nIt will contain the most uptodate versions of my packages, that come with its own installer!\nSee how to write your own installer\nIf you encounter any problems. Let me know."
  },
  {
    "objectID": "papers/cv2024/cv2024.html",
    "href": "papers/cv2024/cv2024.html",
    "title": "Fernando Rios-Avila",
    "section": "",
    "text": "64 Oak Street • Rensselaer, NY, 12144 • (404) 924-5176 • friosa@gmail.com"
  },
  {
    "objectID": "papers/cv2024/cv2024.html#contact-information",
    "href": "papers/cv2024/cv2024.html#contact-information",
    "title": "Fernando Rios-Avila",
    "section": "",
    "text": "64 Oak Street • Rensselaer, NY, 12144 • (404) 924-5176 • friosa@gmail.com"
  },
  {
    "objectID": "papers/cv2024/cv2024.html#summary",
    "href": "papers/cv2024/cv2024.html#summary",
    "title": "Fernando Rios-Avila",
    "section": "Summary",
    "text": "Summary\nPhD economist with over a decade of research experience at the Levy Economics Institute, specializing in labor economics, applied econometrics, and applied microeconomics. Expertise in poverty and inequality analysis, data-driven policy evaluation, econometric modeling, with experience teaching graduate-level courses. Key contributor to the Levy Institute Measure of Income and Wealth (LIMEW) and the Levy Institute Measure of Time and Income Poverty (LIMTIP)."
  },
  {
    "objectID": "papers/cv2024/cv2024.html#research-interests",
    "href": "papers/cv2024/cv2024.html#research-interests",
    "title": "Fernando Rios-Avila",
    "section": "Research Interests",
    "text": "Research Interests\n\nApplied Econometrics: Causal inference, decomposition methods, non-parametric methods.\nApplied Microeconomics: Policy evaluation, consumer behavior.\nLabor Economics: Wage dynamics, union impacts, labor market institutions.\nPoverty and Inequality: Measurement techniques, intersectionality."
  },
  {
    "objectID": "papers/cv2024/cv2024.html#education",
    "href": "papers/cv2024/cv2024.html#education",
    "title": "Fernando Rios-Avila",
    "section": "Education",
    "text": "Education\nPhD in Economics | Georgia State University | 2008–2013 - Dissertation: “Essays on Unions, Wages, and Performance: Evidence from Latin America”\nAdvanced Studies (M.Sc) International Economics | Kiel University - Germany | 2007–2008\nLicenciatura en Economía (5 yr program, BS equivalent) | Universidad Católica Boliviana | 2000–2004"
  },
  {
    "objectID": "papers/cv2024/cv2024.html#professional-experience",
    "href": "papers/cv2024/cv2024.html#professional-experience",
    "title": "Fernando Rios-Avila",
    "section": "Professional Experience",
    "text": "Professional Experience\nResearch Scholar | Levy Economics Institute of Bard College | August 2013–Present\n\nWorked on research projects on labor market dynamics, inequality, and time and income poverty.\nDeveloped and applied econometric models for the analysis of time use and consumption data.\nMaintained and updated the Levy Institute Measure of Income and Wealth (LIMEW) and the Levy Institute Measure of Time and Income Poverty (LIMTIP).\nPrimary instructor for econometric courses at the Levy Institute’s Master of Science in Economic Theory and Policy program.\n\nConsultant | World Bank | 2022–Present\n\nConducted analysis in labor and development economics topics in Latin America.\nDeveloped and applied econometric models for analysis of income and consumption data.\n\nResearch Assistant | Federal Reserve Bank of Atlanta | August 2010–June 2013\nGraduate Research Assistant | Georgia State University | August 2008–December 2013\nConsultant (UNICEF Project) | Economic and Social Policy Analysis Unit (UDAPE) | July 2005–June 2007\nInternship | Superintendence of Hydrocarbon, Financial and Economic Analysis-Department | January 2004–July 2004"
  },
  {
    "objectID": "papers/cv2024/cv2024.html#teaching-experience",
    "href": "papers/cv2024/cv2024.html#teaching-experience",
    "title": "Fernando Rios-Avila",
    "section": "Teaching Experience",
    "text": "Teaching Experience\nLevy Economics Institute\n\nEconometrics of Poverty, Inequality and Social Science Research | 2015–2021\nResearch Methods I: Econometrics | 2018–Present\nApplied Econometric Methods for Empirical Research & Policy Evaluation | 2023–Present\nResearch Methods II: Applied Methods in Microeconomics (Shared Course) | 2024–Present\nIntersecting Inequalities (Shared Course): Labor Market Institutions | 2018\nIntersecting Inequalities (Shared Course): Decompositions Methods in Economics | 2017"
  },
  {
    "objectID": "papers/cv2024/cv2024.html#publications",
    "href": "papers/cv2024/cv2024.html#publications",
    "title": "Fernando Rios-Avila",
    "section": "Publications",
    "text": "Publications\n\nRefereed Publications\n\n\n1. Rios-Avila, F., Özeki, A., & Komuryakan, F. (Forthcoming). Glass ceiling, sticky floor, or both? Public and private sector differences in türkiye. Metroeconomica.\n\n\n2. Canavire-Bacarreza, G. J., Gomez-Aliaga, Guillermo., Britton, chevanne, Rios-Avila, F., Jimenez-Pozo, W., Granados Ibarra, S., & Li, R. (2024). Fiscal incidence on the island: Grenada’s fiscal system and its incidence. Social and Economic Studies.\n\n\n3. Hotchkiss, J. L., Moore, R. E., & Rios-Avila, F. (2024). Impact of the 2017 tax cuts and jobs act on household labor supply and welfare across the income distribution. National Tax Journal, 77(2), 313–348.\n\n\n4. Mundra, K., & Rios-Avila, F. (2024). Education-occupation mismatch and social networks for hispanics in the u.s.: Role of citizenship. Education Economics, 23(2), 185–209.\n\n\n5. Rios-Avila, F., Canavire-Bacarreza, G. J., & Sacco-Capurro, F. (2024). Recovering income distribution in the presence of interval-censored data. Journal of Economic Inequality. https://doi.org/10.1007/s10888-023-09617-2\n\n\n6. Jenkins, S. P., & Rios-Avila, F. (2023). Finite mixture model for linked survey and administrative data: Estimation and post estimation. The Stata Journal, 23(1), 53–85.\n\n\n7. Jenkins, S. P., & Rios-Avila, F. (2023). Reconciling reports: Modeling employment earnings and measurement errors using linked survey and administrative data. Journal of the Royal Statistical Society Series A: Statistics in Society, 186(1), 110–136. https://doi.org/10.1093/jrsssa/qnac003\n\n\n8. Rios-Avila, F., & Maroto, M. L. (2022). Moving beyond linear regression: Implementing and interpreting quantile regression models with fixed effects. Sociological Methods and Research. https://doi.org/10.1177/0049124121103616\n\n\n9. Jenkins, S. P., & Rios-Avila, F. (2021). Measurement error in earnings data: Replication of meijer, rohwedder, and wansbeek’s mixture model approach to combining survey and register data. Journal of Applied Econometrics, 1–10.\n\n\n10. Morrar, R., & Rios-Avila, F. (2021). Discrimination against refugees in the palestinian labor market. International Journal of Manpower, 42(2), 1002–1024.\n\n\n11. Mundra, K., & Rios-Avila, F. (2021). Using repeated cross-sectional data to examine the role of immigrant birth-country networks on unemployment duration: An application of guell and hu (2006) approach. Empirical Economics, 61, 389–415.\n\n\n12. Rios-Avila, F. (2021). Estimation of marginal effects for models with alternative variable transformations. The Stata Journal, 21(1), 81–96.\n\n\n13. Canavire-Bacarreza, G. J., Chong, A., Rios-Avila, F., & Yanez-Pagans, M. (2020). Will elders provide for their grandchildren? Unconditional cash transfers and educational expenditures in bolivia. Review of Development Economics, 24(2), 424–447.\n\n\n14. Hotchkiss, J. L., Moore, R. E., & Rios-Avila, F. (2020). Cost of policy choices: A microsimulation analysis of the impact on family welfare of unemployment and price changes. Journal of Macroeconomics, 63, 103167.\n\n\n15. Jenkins, S. P., & Rios-Avila, F. (2020). Modelling errors in survey and administrative data on employment earnings: Sensitivity to the fraction assumed to have error-free earnings. Economics Letters, 192, 109253.\n\n\n16. Rios-Avila, F. (2020). Recentered influence functions (RIF) in stata: RIF regression and RIF decomposition. Stata Journal, 20(1), 51–94.\n\n\n17. Rios-Avila, F. (2020). Smooth varying coefficient models in stata. The Stata Journal, 20(3), 647–679.\n\n\n18. Rios-Avila, F., & Canavire-Bacarreza, G. J. (2020). The effect of immigration on labor market transitions of native-born unemployed in the united states. Journal of Labor Research, 41, 295–331.\n\n\n19. Masterson, T., Rios-Avila, F., Zacharias, A., & Wolf, E. N. (2019). The great recession and racial inequality: Evidence from measures of economic well-being. Journal of Economic Issues, 53(4), 1048–1069.\n\n\n20. Rios-Avila, F. (2019). A semi-parametric approach to the oaxaca–blinder decomposition with continuous group variable and self-selection. Econometrics, 7(2), 28.\n\n\n21. Rios-Avila, F., & Canavire-Bacarreza, G. J. (2018). Standard error correction in two-stage optimization models: A quasi-maximum likelihood estimation approach. The Stata Journal, 18(1), 206–222.\n\n\n22. Canavire-Bacarreza, G. J., & Rios-Avila, F. (2017). On the determinants of wage inequality changes in urban bolivia. Journal of Human Development and Capabilities, 18(4), 464–496.\n\n\n23. Hotchkiss, J. L., Moore, R. E., Rios-Avila, F., & Trussel, M. (2017). A tale of two decades: Relative intra-family earning capacity and changes in family welfare over time. Review of Economics of the Household, 15(3), 707–737.\n\n\n24. Rioja, F. K., Rios-Avila, F., & Valev, N. T. (2017). Productivity during recessions with banking crises: Inter-industry evidence. Economics Letters, 152, 50–53.\n\n\n25. Rios-Avila, F. (2017). Unions and economic performance in developing countries: Case studies from latin america. Ecos de Economia: A Latin American Journal of Applied Economics, 21(44), 4–36.\n\n\n26. Saravia, A., Canavire-Bacarreza, G. J., & Rios-Avila, F. (2017). Intellectual property rights, foreign direct investment and economic freedom. Global Economy Journal, 23(2), 20170017.\n\n\n27. Canavire-Bacarreza, G. J., & Rios-Avila, F. (2016). The effect of intimate partner violence on labor market decisions: Evidence from a multi-ethnic country. International Journal of Social Economics, 44(1), 75–92.\n\n\n28. Aristizabal-Ramirez, M., Canavire-Bacarreza, G. J., & Rios-Avila, F. (2015). Revisiting the effects of innovation on growth: A threshold analysis. Applied Economics Letters, 22(18), 1474–1479.\n\n\n29. Hotchkiss, J. L., Moore, R. E., & Rios-Avila, F. (2015). Reevaluation of the employment impact of the 1996 summer olympic games. Southern Economic Journal, 81(3), 619–632.\n\n\n30. Hotchkiss, J. L., Quispe-Agnoli, M., & Rios-Avila, F. (2015). The wage impact of undocumented workers: Evidence from administrative data. Southern Economic Journal, 81(4), 874–906.\n\n\n31. Rios-Avila, F. (2015). Feasible estimation of linear models with n-fixed effects. Stata Journal, 15(3), 881–898.\n\n\n32. Hotchkiss, J. L., Pitts, M., & Rios-Avila, F. (2014). A search for evidence of skill mismatch in the aftermath of the great recession. Applied Economics Letters, 21(9), 587–592.\n\n\n33. Rioja, F. K., Rios-Avila, F., & Valev, N. T. (2014). Serial banking crises and capital investment. Emerging Markets Finance and Trade, 50(6), 193–208.\n\n\n34. Rioja, F. K., Rios-Avila, F., & Valev, N. T. (2014). The persistent effect of banking crises on investment and the role of financial markets. Journal of Financial Economic Policy, 6(1), 64–77.\n\n\n35. Rios-Avila, F., & Hirsch, B. (2014). Unions, wage gaps, and wage dispersion: New evidence from the americas. Industrial Relations: A Journal of Economy and Society, 53(1), 1–27.\n\n\n36. Hotchkiss, J. L., & Rios-Avila, F. (2013). Identifying factors behind the decline in the u.s. Labor force participation rate. Business and Economic Research, 3(1).\n\n\n37. Searing, E. A. M., Rios-Avila, F., & Lecy, J. D. (2013). Mental health and labor market outcomes in bosnia-herzegovina. Social Science & Medicine, 96(20), 165–173.\n\n\n38. Hotchkiss, J. L., Moore, R. E., & Rios-Avila, F. (2012). Assessing the welfare impact of tax reform: A case study of the 2001 US tax cut. The Review of Income and Wealth, 58(2), 233–256.\n\n\n39. Mercado-Salazar, A., Leiton, J., & Rios-Avila, F. (2004). Segmentación en el mercado matrimonial. Revista Latinoamericana de Desarrollo Económico, 3, 49–76."
  },
  {
    "objectID": "papers/cv2024/cv2024.html#awards-and-honors",
    "href": "papers/cv2024/cv2024.html#awards-and-honors",
    "title": "Fernando Rios-Avila",
    "section": "Awards and Honors",
    "text": "Awards and Honors\n\nJournal of Economic Issues, Editor’s Prize, 2019 - Recognition for outstanding contribution to institutional economics\nThe Stata Journal, Editors’ Prize, 2023 - Excellence in statistical software development and documentation"
  },
  {
    "objectID": "papers/cv2024/cv2024.html#thesis-advisor",
    "href": "papers/cv2024/cv2024.html#thesis-advisor",
    "title": "Fernando Rios-Avila",
    "section": "Thesis Advisor",
    "text": "Thesis Advisor\n\nMaría Aristizábal-Ramírez (Undergraduate Thesis)\nIan Bowen (Master Thesis)\nAlejandra Montoya (Master Thesis)\nDaniella Medina (Master Thesis)\nRhett Maiorama (Master Thesis)\nEsteban Rivera (Master Thesis)\nBrandon Istenes (Master Thesis)\nMasooma Hassan (Master Thesis)"
  },
  {
    "objectID": "papers/cv2024/cv2024.html#technical-skills",
    "href": "papers/cv2024/cv2024.html#technical-skills",
    "title": "Fernando Rios-Avila",
    "section": "Technical Skills",
    "text": "Technical Skills\nStatistical Software\n\nStata: Advanced proficiency, developed and maintained various user-written programs.\nR: Intermediate proficiency, used software for Econometrics classes.\nPython: Intermediate proficiency for programming and basic econometric analysis.\nBasic knowledge of Julia and SQL.\n\nLanguages\n\nSpanish (Native)\nEnglish (Fluent)"
  },
  {
    "objectID": "draconian.html",
    "href": "draconian.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "draconian.html#phonology-sounds",
    "href": "draconian.html#phonology-sounds",
    "title": "",
    "section": "Phonology (Sounds)",
    "text": "Phonology (Sounds)\n\nConsonants: [List of consonants]\nVowels: [List of vowels]\nTones: [If applicable]"
  },
  {
    "objectID": "draconian.html#grammar",
    "href": "draconian.html#grammar",
    "title": "",
    "section": "Grammar",
    "text": "Grammar\n\nWord Order: Subject-Verb-Object (SVO)\nPlurals: [Pluralization rules]\nTenses:\n\nPresent: asis\npast: pr\nfuture:fut\n\nCases: [If applicable]\nArticles: [If applicable]"
  },
  {
    "objectID": "draconian.html#writing-system",
    "href": "draconian.html#writing-system",
    "title": "",
    "section": "Writing System",
    "text": "Writing System\n\n[Description of the writing system, or example characters]"
  },
  {
    "objectID": "draconian.html#syntax-sentence-structure",
    "href": "draconian.html#syntax-sentence-structure",
    "title": "",
    "section": "Syntax (Sentence Structure)",
    "text": "Syntax (Sentence Structure)\n\n[Description of syntax rules]"
  },
  {
    "objectID": "draconian.html#vocabulary",
    "href": "draconian.html#vocabulary",
    "title": "",
    "section": "Vocabulary",
    "text": "Vocabulary"
  },
  {
    "objectID": "draconian.html#verbs",
    "href": "draconian.html#verbs",
    "title": "",
    "section": "Verbs",
    "text": "Verbs\n\nFirath - To Burn\nZyndar - To Fly\nGorin - To Roar\nThyrae - To Protect\nVorak - To Fight\nSylth - To Speak\nXyldor - To Guard\nRyndal - To Hunt\nAelor - To Soar\nDrakal - To Enchant\nSornal - To Climb\nZyraeth - To Love\nFyrandor - To Create\nValcor - To Watch\nKyril - To Sleep\n\n\nVerb Tenses\n\nprFirath - Burned\nfutFirath - Will Burn\nprZyndar - Flew\nfutZyndar - Will Fly\nprGorin - Roared\nfutGorin - Will Roar"
  },
  {
    "objectID": "draconian.html#nouns",
    "href": "draconian.html#nouns",
    "title": "",
    "section": "Nouns",
    "text": "Nouns\n\nDralorin - Dragonkin\nFiraeth - Flame\nSylthor - Speech\nAilith - Wing\nRyndorin - Hunter\nThyral - Shield\nZyndalor - Skyship\nGorthor - Mountain\nSornalith - Scales\nVorinor - Warrior"
  },
  {
    "objectID": "draconian.html#pronouns",
    "href": "draconian.html#pronouns",
    "title": "",
    "section": "Pronouns",
    "text": "Pronouns\n\nAzar - I, Me\nDroth - You\nThyra - He/She/It\nVyra - We, Us\nZyndra - They, Them"
  },
  {
    "objectID": "draconian.html#example-sentences",
    "href": "draconian.html#example-sentences",
    "title": "",
    "section": "Example Sentences",
    "text": "Example Sentences\n\nAzar zyraeth droth. (I love you)\n[Add more example sentences here]"
  },
  {
    "objectID": "draconian.html#cultural-influences",
    "href": "draconian.html#cultural-influences",
    "title": "",
    "section": "Cultural Influences",
    "text": "Cultural Influences\n\n[Description of cultural aspects influencing the language]"
  },
  {
    "objectID": "draconian.html#usage-in-fictional-world",
    "href": "draconian.html#usage-in-fictional-world",
    "title": "",
    "section": "Usage in Fictional World",
    "text": "Usage in Fictional World\n\n[How the language is used in the context of the story]\n\n\nNote: This documentation provides a basic overview. Further details can be added as the language evolves."
  },
  {
    "objectID": "chatgpt.html",
    "href": "chatgpt.html",
    "title": "Odds and Ends + ChatGPT",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStata Crash Course: Graphs and EDA\n\n\n\nStata\n\n\nBasics\n\n\n\nA brief introduction to Graphs production in Stata\n\n\n\nFernando Rios-Avila\n\n\nAug 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStata Crash Course: Publication Ready Tables\n\n\n\nStata\n\n\nBasics\n\n\n\nProducing tables for publication using Stata\n\n\n\nFernando Rios-Avila\n\n\nAug 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStata Crash Course: Data management\n\n\n\nStata\n\n\nBasics\n\n\n\nA brief introduction to data management in Stata\n\n\n\nFernando Rios-Avila\n\n\nAug 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto 1.4\n\n\n\nQuarto\n\n\n\nSome examples for Table cross-referencing!\n\n\n\nFernando Rios-Avila\n\n\nJan 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKeynes vs Hayek: The Rap battle\n\n\n\nChatGPT\n\n\nRap Battles\n\n\n\nChat-GPT creates various version of Rap Battle between Keynes and Hayek. All with different levels of similarity to Epic Rap battles of History (self-rated)\n\n\n\nChatGPT and Fernando Rios-Avila\n\n\nAug 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeak IV: And the problem of Inference\n\n\n\nStata\n\n\nProgramming\n\n\nEconometrics\n\n\nWeak IV\n\n\n\nUsing a Small Simulation, and based on current research, I ilustrate why F=10, is not enough for inference.\n\n\n\nFernando Rios-Avila\n\n\nAug 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHow to suest a HDFE\n\n\n\nStata\n\n\nProgramming\n\n\nFixed Effects\n\n\n\nI propose a feasible strategy to compare coefficients across models with high dimensional fixed effects\n\n\n\nFernando Rios-Avila\n\n\nJul 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRenaming variables in Bulk\n\n\n\nStata\n\n\nTips\n\n\n\nI describe an extended option from rename to easily rename your variabes\n\n\n\nFernando Rios-Avila and Fahad Mirza\n\n\nJun 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSynthetic Control: Role of rescaling\n\n\n\nStata\n\n\nProgramming\n\n\nCausal effects\n\n\n\nI discuss the role of variable transformations on the use of Synthetic control for Causal Analysis\n\n\n\nFernando Rios-Avila\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMerging Data 1:1, m:1, 1:m and m:m?\n\n\n\nStata\n\n\nTips\n\n\nProgramming\n\n\n\nI describe how to use merge, and the meaning of the different types of merging options\n\n\n\nFernando Rios-Avila\n\n\nApr 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nA Poem for Abby’s Teeth\n\n\n\nChatGPT\n\n\nPoem\n\n\n\nChat-GPT writes a small poem about babies first teeth\n\n\n\nChatGPT and Fernando Rios-Avila\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWhy are my estimates changing every time I run my do-files?!\n\n\n\nStata\n\n\nTips\n\n\n\nI describe the main suspects one should look for when wondering of lack of replicability on a dofile\n\n\n\nFernando Rios-Avila\n\n\nMar 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHow to write your own Installer\n\n\n\nStata\n\n\nTips\n\n\nProgramming\n\n\n\nI provide a small description on how to use net to program an easy installer for your own repository\n\n\n\nFernando Rios-Avila\n\n\nMar 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nLoading Large Datasets\n\n\n\nStata\n\n\nTips\n\n\n\nuse in vs use if\n\n\n\nFernando Rios-Avila\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAdding info to e() and r()\n\n\n\nStata\n\n\nProgramming\n\n\n\nI describe and introduce the programs adde and addr. They allow you to add and modify information to some Stata Objects\n\n\n\nFernando Rios-Avila\n\n\nMar 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDrop 2.0: Small Stata Programming\n\n\n\nStata\n\n\nProgramming\n\n\n\nI describe and introduce the program drop2, for more flexible data dropping\n\n\n\nFernando Rios-Avila\n\n\nMar 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nStata vs R: The battle\n\n\n\nChatGPT\n\n\nStories\n\n\n\nChat-GPT creates a high fantasy battle between this titans\n\n\n\nChatGPT and Fernando Rios-Avila\n\n\nMar 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTech Titans Rap Battle\n\n\n\nChatGPT\n\n\nRap Battle\n\n\n\nChat-GPT creates a rap battle between Siri, Alexa and Cortana. Who wins?\n\n\n\nChatGPT and Fernando Rios-Avila\n\n\nMar 12, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "chatgpt/use_03_27_2023/index.html",
    "href": "chatgpt/use_03_27_2023/index.html",
    "title": "Loading Large Datasets",
    "section": "",
    "text": "If you ever wanted to work with very large datasets, you will find that doing may be difficult, if not impossible, due to memory limitations, or at the very least time costly.\nOne option you may want to try when doing this, however, is to use the options if in and/or using varlists.\nFor example, say that you want to work with the dataset oaxaxa which is located in your working directory. You can load only a subset of the data typing:\n\n\n\n\n\nc:\\ado\\personal\n\n\n\nuse oaxaca if female ==1, clear\nsum female age\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      female |        888           1           0          1          1\n         age |        888    39.88401    10.72665         18         62\n\n\nyou could alternatively load only few variables in the dataset:\n\nuse female age educ using oaxaca , clear\nsum *\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        educ |      1,647    11.40134    2.374952          5       17.5\n      female |      1,647    .5391621    .4986154          0          1\n         age |      1,647    39.25379    11.03187         18         62\n\n\nor use a combination of both. Now, if your dataset is really large, i suggest you to use in, which will do the task much faster than using if. Why? because using if you still need to go over every observation of your dataset. However using in only goes over the pre-specified number of observations.\n\nuse female age educ using oaxaca in 1/10, clear\nlist , sep(0)\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n     +---------------------+\n     | educ   female   age |\n     |---------------------|\n  1. |    9        1    37 |\n  2. |    9        0    62 |\n  3. | 10.5        1    40 |\n  4. |   12        0    55 |\n  5. |   12        0    36 |\n  6. | 10.5        0    31 |\n  7. | 10.5        0    50 |\n  8. | 17.5        0    42 |\n  9. | 17.5        0    36 |\n 10. | 10.5        0    30 |\n     +---------------------+"
  },
  {
    "objectID": "chatgpt/synth_05_09_2023/index.html",
    "href": "chatgpt/synth_05_09_2023/index.html",
    "title": "Synthetic Control: Role of rescaling",
    "section": "",
    "text": "As a wise man once said:\n\nThe best way to learn something is to teach it.\n\nNot sure who said that, but I find it to be true, except for instances when the learning process puts you in a position where you have a question, but you do not know how to find an answer. This is one of those ocasions.\nAs some of you may know, the technique known as Synthetic Control is a widely recognized methodology used to determine the effects of a treatment on a single treated entity. It accomplishes this by identifying the optimal combination of control variables that form a synthetic control.\nThe purpose of this SControl is to provide an answer to the question of “What would have happened to the treated group if no treatment had been administered?”\nHowever, when dealing with a single unit of interest, there is a risk of mistaking random fluctuations with the actual treatment effect. To mitigate this risk one may run a set of placebo tests. One of them involves estimating pseudo treatment effects for each individual “control” unit. Ideally, if the controls were adequate, there should be no discernible effect among them. If the impact on the treated group is significant or unusual enough, we could conclude that there is an effect beyond random chance or noise.\nBecause the optimal weights are such you can only construct synthetic controls via interpolation, rather than extrapolation, the estimation of synthetic controls for some of the non-treated units can be difficult. Unique units may serve well as controls, but are not suitable as treated unit, because no interpolation of data can simulate those units. Because of this, its recommended that one excludes those units when analyzing the significance of the treatment effect."
  },
  {
    "objectID": "chatgpt/synth_05_09_2023/index.html#introduction",
    "href": "chatgpt/synth_05_09_2023/index.html#introduction",
    "title": "Synthetic Control: Role of rescaling",
    "section": "",
    "text": "As a wise man once said:\n\nThe best way to learn something is to teach it.\n\nNot sure who said that, but I find it to be true, except for instances when the learning process puts you in a position where you have a question, but you do not know how to find an answer. This is one of those ocasions.\nAs some of you may know, the technique known as Synthetic Control is a widely recognized methodology used to determine the effects of a treatment on a single treated entity. It accomplishes this by identifying the optimal combination of control variables that form a synthetic control.\nThe purpose of this SControl is to provide an answer to the question of “What would have happened to the treated group if no treatment had been administered?”\nHowever, when dealing with a single unit of interest, there is a risk of mistaking random fluctuations with the actual treatment effect. To mitigate this risk one may run a set of placebo tests. One of them involves estimating pseudo treatment effects for each individual “control” unit. Ideally, if the controls were adequate, there should be no discernible effect among them. If the impact on the treated group is significant or unusual enough, we could conclude that there is an effect beyond random chance or noise.\nBecause the optimal weights are such you can only construct synthetic controls via interpolation, rather than extrapolation, the estimation of synthetic controls for some of the non-treated units can be difficult. Unique units may serve well as controls, but are not suitable as treated unit, because no interpolation of data can simulate those units. Because of this, its recommended that one excludes those units when analyzing the significance of the treatment effect."
  },
  {
    "objectID": "chatgpt/synth_05_09_2023/index.html#how-does-sc-works",
    "href": "chatgpt/synth_05_09_2023/index.html#how-does-sc-works",
    "title": "Synthetic Control: Role of rescaling",
    "section": "How does SC works?",
    "text": "How does SC works?\nThe basic implementation of SC involves finding a set of optimal weights \\(w\\) such that:\n\\[\nw^*=\\min_w \\sum_{h} \\left(X_{1,h}-\\sum_{j=2}^J w_j X_{j,h}\\right)^2\n\\]\nwhere \\(X's\\) is a set of pre-treatment characteristics we would like to equalize between the treated unit and the control units. \\(X\\) can contain pretreament outcome characteristics. We also require \\(\\sum w_j =1\\) and \\(w_j\\geq 0\\).\nOnce the weights have been estimated, we can estimate the treatment effect at any point in time simply as:\n\\[ \\tau_t = Y_{1,t}-\\sum_{j=2}^J w^*_j \\times Y_{j,t}\n\\]"
  },
  {
    "objectID": "chatgpt/synth_05_09_2023/index.html#the-question-without-an-answer",
    "href": "chatgpt/synth_05_09_2023/index.html#the-question-without-an-answer",
    "title": "Synthetic Control: Role of rescaling",
    "section": "The question without an answer",
    "text": "The question without an answer\nSomething that seems interesting to me is that the construction of treatment effects, or estimation of optimal weights say nothing about how should data be used, nor which variation should we be interested in.\nGranted, standard approach is to just use data asis, but that seems unsatisfactory. Consider, for example, a case when the interest is on analyzing the effect on GDP of a country level policy in the US. The US being one of the largest economies in the world, it would be difficult, if not impossible to, ex ante, find good controls.\nBut what if we change the measure of interest? and look into GDP percapita, or GDP relative levels, or something else. After the estimation is done, we could certainly reconstruct the original question.\nImplementing these kind of transformations would help finding better controls, but could have important impacts when estimating the placebo tests assessing the significance of the estimated effect. Here the question:\n\nTo what extend can we transform our explanatory variables when implementing SC Would the transformations need to be the same for all units? or panel units?\n\nBelow, I show an example of what could happen when we make these decisions:"
  },
  {
    "objectID": "chatgpt/synth_05_09_2023/index.html#smoking-in-california",
    "href": "chatgpt/synth_05_09_2023/index.html#smoking-in-california",
    "title": "Synthetic Control: Role of rescaling",
    "section": "Smoking in California",
    "text": "Smoking in California\nFor the example I have in mind, I will use the Smoking dataset that is typically used to teach the methodology. I will also use two community-contributed programs synth and frause. The second one, just to make it easy uploading the data.\nI will also use a small programm to prepare the data before creating the figures. This one is rather long, but if you are interested, please take a look.\n\n\nCode\nset scheme white2\ncapture program drop sc_doer\nprogram sc_doer\n** Estimates the Effect for California.\n    tempfile sc3\n    synth cigsale cigsale(1971) cigsale(1975) cigsale(1980)   cigsale(1985), trunit(3) trperiod(1989) keep(`sc3') replace  \n** And all other states\n    forvalues i =1/39{\n        if `i'!=3 {\n            local pool\n            foreach j of local stl {\n                if `j'!=3 & `j'!=`i' local pool `pool' `j'\n            }\n            tempfile sc`i'\n            synth cigsale cigsale(1971) cigsale(1975) cigsale(1980) cigsale(1985) , ///\n            trunit(`i') trperiod(1989) keep(`sc`i'') replace counit(`pool')\n        }\n    }\n** Collects the Saved files to estimate the Treatment effect\n** and the p-value/ratio statistic\n\n    forvalues i =1/39{\n        use `sc`i'' , clear\n        gen tef`i' = _Y_treated - _Y_synthetic\n        egen sef`i'a =mean( (_Y_treated - _Y_synthetic)^2) if _time&lt;=1988\n        egen sef`i'b =mean( (_Y_treated - _Y_synthetic)^2) if _time&gt;1988\n        gen sef`i'aa=sqrt(sef`i'a[2])\n        gen sef`i'bb=sqrt(sef`i'b[_N])\n        replace sef`i'a=sef`i'aa\n        replace sef`i'b=sef`i'bb\n        drop if _time==.\n        keep tef`i' sef`i'* _time\n        save `sc`i'', replace\n    }\n\n    clear\n    use `sc1'\n    forvalues i = 2/39 {\n        merge 1:1 _time using `sc`i'', nogen\n    }\n    global toplot\n    global toplot2\n** Stores which models will be saved for plotting    \n    forvalues i = 1/39 {\n        global toplot $toplot (line tef`i' _time, color(gs11) )\n    if (sef`i'a[1])&lt;(2*sef3a[1]) {\n            global toplot2 $toplot2 (line tef`i' _time, color(gs11) )\n        }\n    }\n** Estimates the post/pre RMSE ratio\n    capture matrix drop rt\n    forvalues i = 1/39 {\n        if (sef`i'a[1])&lt;(2*sef3a[1]) {\n            matrix rt=nullmat(rt)\\[`i',sef`i'b[1]/sef`i'a[1]]\n        }\n    }\n    svmat rt\n    egen rnk=rank(rt2)\n** and the ranking /p-value for each period\n    gen rnk2=0\n    forvalues i = 1/39 {\n        if   (sef`i'a[1])&lt;(2*sef3a[1]) {\n            local t = `t'+1\n            replace rnk2=rnk2+(tef`i'&lt;=tef3)    \n        }\n    } \n    gen pv=rnk2*100/`t'\nend\n\n\n\n\n\n\n\n\n\nASIS case\nThe first case will be the “vanilla” one. I will only use 4 pretreatment outcomes for the weight construction: 1971, 1975, 1980 and 1985. The outcome of interest is cigarette sale per capita (in packs). Thus, to some extend, data has already been standardized to be measured in comparable units.\nThe basic code will look like this:\nsynth cigsale cigsale(1971) cigsale(1975) cigsale(1980)   cigsale(1985), trunit(3) trperiod(1989) \n\n\nCode\nqui:frause smoking2, clear\nqui:xtset state year\nqui:sort state year\nqui:sc_doer\n\n\nwhich will produce the following:\n\nCode\ntwo $toplot (line tef3 _time, lw(1) color(navy*.8)), xline(1989) legend(off) name(m1, replace)\n\ntwo $toplot2 (line tef3 _time, lw(1) color(navy*.8)), xline(1989) legend(off) name(m2, replace)\n\ntwo bar rt2 rnk || bar rt2 rnk if rt1==3 , legend(order( 2 \"California\")) name(m3, replace)\n\ntwo bar pv _time if _time&gt;1988 & pv&lt;40, legend(off) name(m4, replace) ylabel(0(5)25)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Unrestricted effect\n\n\n\n\n\n\n\n\n\n\n\n(b) Restricted effect\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) RMSE Ratio\n\n\n\n\n\n\n\n\n\n\n\n(d) P-value\n\n\n\n\n\n\n\nFigure 1: Synthetic Control Results: ASIS\n\n\n\nBased on Figure 1, there is an effect of the Policy which reduced sales of cigarates. Figure 1 (c) and Figure 1 (d) suggest the effect is significant across most post-treatment periods.\n\n\nLog Case\nLets make one change. Instead of using cigarette sale per capita, I will use the log of that variable. The idea is that while raw data may not be comparable, because of different scales across units, data may be more comparable if its comparessed using a logScale.\n\nCode\ntwo $toplot (line tef3 _time, lw(1) color(navy*.8)), xline(1989) legend(off) name(m1, replace)\n\ntwo $toplot2 (line tef3 _time, lw(1) color(navy*.8)), xline(1989) legend(off) name(m2, replace)\n\ntwo bar rt2 rnk || bar rt2 rnk if rt1==3 , legend(order( 2 \"California\")) name(m3, replace)\n\ntwo bar pv _time if _time&gt;1988 & pv&lt;40, legend(off) name(m4, replace) ylabel(0(5)25)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Unrestricted effect\n\n\n\n\n\n\n\n\n\n\n\n(b) Restricted effect\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) RMSE Ratio\n\n\n\n\n\n\n\n\n\n\n\n(d) P-value\n\n\n\n\n\n\n\nFigure 2: Synthetic Control Results: Logs\n\n\n\nInterestingly enough Figure 2 shows a very similar effect plot as in Figure 1. The RMSE ratio is even better, but with few post-treatment effects that are not as significant.\n\n\nRelative Change\nThe next alternative is to use a relative rescaling. Specifically, I will change the baseline of cigsales across all States, so that Cigarette Sales in 1970 is normalized to be 100. The changes, then could be interpreted in units relative to what happened in 1970.\n\nCode\ntwo $toplot (line tef3 _time, lw(1) color(navy*.8)), xline(1989) legend(off) name(m1, replace)\n\ntwo $toplot2 (line tef3 _time, lw(1) color(navy*.8)), xline(1989) legend(off) name(m2, replace)\n\ntwo bar rt2 rnk || bar rt2 rnk if rt1==3 , legend(order( 2 \"California\")) name(m3, replace)\n\ntwo bar pv _time if _time&gt;1988 & pv&lt;40, legend(off) name(m4, replace) ylabel(0(5)25)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Unrestricted effect\n\n\n\n\n\n\n\n\n\n\n\n(b) Restricted effect\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) RMSE Ratio\n\n\n\n\n\n\n\n\n\n\n\n(d) P-value\n\n\n\n\n\n\n\nFigure 3: Synthetic Control Results: Logs\n\n\n\nBased on Figure 3, we still have many states with very bad model fitness. Figure 3 (c) and Figure 3 (d), however, still shows good model performance, with slighly higher significance than when the data was used as is.\n\n\nFully Rescale (to California pre-treament)\nThe last transformation I implement is one where all data is rescaled and shifted so that all countries have the same average and standard deviation in the period before treatment. I believe this could be the best approach, because forces all data to be perfectly comparable in the pre-treatment period, increasing the posibility to find good controls even for the other wise extreme states.\n\nCode\ntwo $toplot (line tef3 _time, lw(1) color(navy*.8)), xline(1989) legend(off) name(m1, replace)\n\ntwo $toplot2 (line tef3 _time, lw(1) color(navy*.8)), xline(1989) legend(off) name(m2, replace)\n\ntwo bar rt2 rnk || bar rt2 rnk if rt1==3 , legend(order( 2 \"California\")) name(m3, replace)\n\ntwo bar pv _time if _time&gt;1988 & pv&lt;40, legend(off) name(m4, replace) ylabel(0(5)25)\nframe put *, into(m4)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Unrestricted effect\n\n\n\n\n\n\n\n\n\n\n\n(b) Restricted effect\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) RMSE Ratio\n\n\n\n\n\n\n\n\n\n\n\n(d) P-value\n\n\n\n\n\n\n\nFigure 4: Synthetic Control Results: Rescale to California\n\n\n\nWhat I consider quite interesting is that even the effect plot that uses no restrictions shows very good pre-treatment model fit for all donor units. Although the fitness for California is even better. Once we impose the same restriction to the data, excluding units with a RMSE twice as large as the one in California, we obtain a plot similar to the ones in the previous cases.\nFigure 4 (c) and Figure 4 (d) show that the effect may not be as strong as in the previous example. The RMSE ratio method suggests a p-value of 12.5%, above the standard 5% we like to consider as minimum threshold. The significance across periods is also slighly worse, because there is one state that exhibits a pseudo treatment that is larger than California."
  },
  {
    "objectID": "chatgpt/synth_05_09_2023/index.html#comparing-magnitude-effects",
    "href": "chatgpt/synth_05_09_2023/index.html#comparing-magnitude-effects",
    "title": "Synthetic Control: Role of rescaling",
    "section": "Comparing Magnitude effects",
    "text": "Comparing Magnitude effects\n\n\nCode\nqui:frause smoking2, clear\ntempfile sc1x sc2x sc3x sc4x\nxtset state year\nqui: {\n    gen or_cigsale=cigsale\n    synth cigsale cigsale(1971) cigsale(1975) cigsale(1980)   cigsale(1985), trunit(3) trperiod(1989) keep(`sc1x') replace  \n    replace cigsale=log(or_cigsale)\n    synth cigsale cigsale(1971) cigsale(1975) cigsale(1980)   cigsale(1985), trunit(3) trperiod(1989) keep(`sc2x') replace  \n    sort state year\n    by state:replace cigsale=or_cigsale/or_cigsale[1]*100\n    synth cigsale cigsale(1971) cigsale(1975) cigsale(1980)   cigsale(1985), trunit(3) trperiod(1989) keep(`sc3x') replace  \n\n    sum or_cigsale if state==3 & year&lt;=1988\n    local cmean=r(mean)\n    local smean=r(sd)\n    forvalues i = 1/39 {\n        sum or_cigsale if state==`i' & year&lt;=1988\n        replace cigsale = `cmean' + `smean' * (or_cigsale - r(mean))/r(sd) if state==`i'\n    }\n    synth cigsale cigsale(1971) cigsale(1975) cigsale(1980)   cigsale(1985), trunit(3) trperiod(1989) keep(`sc4x') replace  \n}\n\nclear\nuse `sc1x'\nrename  _Y_synthetic y1_synth\ndrop if _time==.\nsave `sc1x', replace\n\nuse `sc2x'\nrename  _Y_synthetic y2_synth\nreplace y2_synth = exp(y2_synth )\ndrop if _time==.\nsave `sc2x', replace\n\nuse `sc3x'\nren _Y_synthetic y3_synth\ndrop if _time==.\nsave `sc3x', replace\n\nuse `sc4x'\nren _Y_synthetic y4_synth\ndrop if _time==.\nsave `sc4x', replace\n\nuse `sc1x', clear\nmerge 1:1 _time using `sc2x', nogen\nmerge 1:1 _time using `sc3x', nogen\nmerge 1:1 _time using `sc4x', nogen\n\nreplace y3_synth=y3_synth* _Y_treated[1]/100\ngen eff1= _Y_treated-y1_synth\ngen eff2= _Y_treated-y2_synth\ngen eff3= _Y_treated-y3_synth\ngen eff4= _Y_treated-y4_synth\n\n\nPerhaps what would be the best way to comapre the different cases presented above is to rescale the results as to obtain results that can be compared across specifications. In Figure 5, I puts together this effects, so that they can be compared across each other.\n\n\nCode\nscatter eff* _time, connect(l l l l) msymbol(O D T S) xline(1989) ///\nlegend(order(1 \"Asis  2.550\" 2 \"Logs 2.859\" 3 \"Rate 1.928\" 4 \"Rescaled 1.955\"))\n\n\n\n\n\n\n\n\nFigure 5: Synthetic Control Effects\n\n\n\n\n\nInterestingly enough, Model Specification does very little on the estimated effect, as they all suggest a sharp decline in Cigarette sales of just above 10 fewer packs per capita 3 years after implementation, up to 25/27 fewer packs per capita by 2000.\nAsis and Logs specifications show the largest decline in 1997 (30 fewer packs), with rescaled and rate specifications being the most conservative. The table also provides the pre-RMSE for all models. They suggest that using Rates provided the best fit, followed by the ReScaled data, using data as is, and using the log transformation."
  },
  {
    "objectID": "chatgpt/synth_05_09_2023/index.html#conclusions",
    "href": "chatgpt/synth_05_09_2023/index.html#conclusions",
    "title": "Synthetic Control: Role of rescaling",
    "section": "Conclusions",
    "text": "Conclusions\nThis was an interesting excercise driven by asking if it matter how the covariates are measured when applying SC. I was also curious about this approach, as I was trying to understand one of the extensions of the methodology: Synthetid Differences in Differences. This may require some testing, but that can be left for a future exploration.\nIt may seem, for this simplified example, that it doesnt really matter how covariates are transformed. The estimated effects were effectively the same. This conclusion, however, may not be valid in more complex settings.\nPerhaps the only concern at this point is that the “significance” level of the estimates did change considerably with the data rescaling approach. Granted, the change seems large, because the sample is small (25 observations), and a single change in ranking may look like a large change in the p-value of the statistic.\nIf you are reading this, and have some comments, please let me know."
  },
  {
    "objectID": "chatgpt/stata_08_06_2024/index.html",
    "href": "chatgpt/stata_08_06_2024/index.html",
    "title": "Stata Crash Course: Graphs and EDA",
    "section": "",
    "text": "The last part of the series of Stata Basics: Graphs! In this quick tutorial should provide you with a solid foundation for creating high-quality graphs in Stata. Granted, Stata’s graphing capabilities are not as flexible and fast as R or Python, but they are still powerful and can produce almost every type of graph you might need.\nBut before we start, a brief comment on graphs in Stata. One of the most important things when making good graphs is to choose good colors, and good schemes (or themes). Up to Stata 17, the default color scheme was s2color, which is not the best. In Stata 18, a new color scheme was introduced, which is more visually appealing. Of course, if you have an earlier version of Stata, you may want to consider other options.\nSo, to get you in the right path, you may want to first get few things installed in your Stata.\nnet install palettes , replace from(\"https://raw.githubusercontent.com/benjann/palettes/master/\")\nnet install colrspace, replace from(\"https://raw.githubusercontent.com/benjann/colrspace/master/\")\nssc install grstyle\nssc install color_style\npalette and colrspace, both by Ben Jann, are packages that allow you to easily create colors that can be used as part of your scheme. grstyle, also by Ben Jann, is another small tool that is very useful to make changes in the scheme files.\nFinally, a small wrapper of my own design: color_style. This package uses the previous ones for one main purpose. Change the color palettes used in the current scheme. That way, you do not need to make changes manually, and can easily change colors as needed. Now a quick setup:\n\n\nCode\nset scheme white2\ncolor_style tableau\n\n\n\n\n\nPERSONAL directory (see personal) does not exist; will create directory\nC:\\Users\\frios\\ado\\personal/\npress any key to continue, or Break to abort\n\n\n\n\nBefore we start with the guide, a quick word on why data visualization matters the data analysis process:\n\nExploratory Data Analysis (EDA): Visualizations help you understand the structure of your data, identify patterns, and spot anomalies or outliers.\nHypothesis Generation: Graphs can reveal relationships between variables that you might not have anticipated, leading to new research questions.\nCommunication: A well-designed graph can show information quickly and effectively to your audience.\nQuality Control: Visualizations can help you identify data quality issues or errors in your analysis.\n\nWith these benefits in mind, let’s begin!\n\n\n\nBefore we start making graphs, let’s start with a brief overview of Stata’s graphing syntax and some basic principles.\n\n\nMost Stata graph commands follow a similar structure:\ngraph_type y_variable(s) [x_variable] [if] [in] [weight] [, options]\nWhere: - graph_type is the type of graph you want to create (e.g., scatter, line, bar) - y_variable is typically the dependent variable - x_variable is typically the independent variable - [if] and [in] are optional conditions to subset your data - [weight] is an optional weighting specification - [, options] are additional specifications to customize your graph\nIn terms of Graph Types, we could possibly classify them into two main categories. you have the two way graphs, and the graph graphs.\nThe first type are graphs that can be overlaid on top of each other. The second, however, cannot be overlay. In addition to that, most community-contributed commands are typically two way graphs. However, they cannot be overlayed with the official ones. Which Its perhaps the biggest weakness of Stata’s graphical capabilities.\n\n\n\n\nLet’s create a simple scatter plot to get started. We’ll use the built-in “auto” dataset:\nsysuse auto, clear\nscatter mpg weight\n\nscatter mpg weight, title(\"Miles per Gallon vs. Weight\") /// Adds a title\n                    xtitle(\"Weight (lbs)\") /// label to the x-axis\n                    ytitle(\"Miles per Gallon\") // and y-axis \nDone!\nNow some points to consider. title is important, but often you may want to add a title in your editor, not the figure. Same if you add a note. Otherwise, the above code is a good start.\nAlso noticed that because of my setup using white2 scheme, the background is white, and minimalistic. The colors, from tableau are generally a good choice for most graphs.\nWait, but how does this look like? Let’s see:\n\n\nCode\nsysuse auto, clear\nscatter mpg weight, title(\"Miles per Gallon vs. Weight\") /// Adds a title\n                    xtitle(\"Weight (lbs)\") /// label to the x-axis\n                    ytitle(\"Miles per Gallon\") /// and y-axis \n                    scale(1.5)\n\n\n(1978 automobile data)\n\n\n\n\n\n\n\n\n\nFrom here on, I will produce the figures directly, and you can check the code to reproduce them in your own work."
  },
  {
    "objectID": "chatgpt/stata_08_06_2024/index.html#why-data-visualization-matters",
    "href": "chatgpt/stata_08_06_2024/index.html#why-data-visualization-matters",
    "title": "Stata Crash Course: Graphs and EDA",
    "section": "",
    "text": "Before we start with the guide, a quick word on why data visualization matters the data analysis process:\n\nExploratory Data Analysis (EDA): Visualizations help you understand the structure of your data, identify patterns, and spot anomalies or outliers.\nHypothesis Generation: Graphs can reveal relationships between variables that you might not have anticipated, leading to new research questions.\nCommunication: A well-designed graph can show information quickly and effectively to your audience.\nQuality Control: Visualizations can help you identify data quality issues or errors in your analysis.\n\nWith these benefits in mind, let’s begin!"
  },
  {
    "objectID": "chatgpt/stata_08_06_2024/index.html#getting-started-with-stata-graphs",
    "href": "chatgpt/stata_08_06_2024/index.html#getting-started-with-stata-graphs",
    "title": "Stata Crash Course: Graphs and EDA",
    "section": "",
    "text": "Before we start making graphs, let’s start with a brief overview of Stata’s graphing syntax and some basic principles.\n\n\nMost Stata graph commands follow a similar structure:\ngraph_type y_variable(s) [x_variable] [if] [in] [weight] [, options]\nWhere: - graph_type is the type of graph you want to create (e.g., scatter, line, bar) - y_variable is typically the dependent variable - x_variable is typically the independent variable - [if] and [in] are optional conditions to subset your data - [weight] is an optional weighting specification - [, options] are additional specifications to customize your graph\nIn terms of Graph Types, we could possibly classify them into two main categories. you have the two way graphs, and the graph graphs.\nThe first type are graphs that can be overlaid on top of each other. The second, however, cannot be overlay. In addition to that, most community-contributed commands are typically two way graphs. However, they cannot be overlayed with the official ones. Which Its perhaps the biggest weakness of Stata’s graphical capabilities."
  },
  {
    "objectID": "chatgpt/stata_08_06_2024/index.html#your-first-stata-graph",
    "href": "chatgpt/stata_08_06_2024/index.html#your-first-stata-graph",
    "title": "Stata Crash Course: Graphs and EDA",
    "section": "",
    "text": "Let’s create a simple scatter plot to get started. We’ll use the built-in “auto” dataset:\nsysuse auto, clear\nscatter mpg weight\n\nscatter mpg weight, title(\"Miles per Gallon vs. Weight\") /// Adds a title\n                    xtitle(\"Weight (lbs)\") /// label to the x-axis\n                    ytitle(\"Miles per Gallon\") // and y-axis \nDone!\nNow some points to consider. title is important, but often you may want to add a title in your editor, not the figure. Same if you add a note. Otherwise, the above code is a good start.\nAlso noticed that because of my setup using white2 scheme, the background is white, and minimalistic. The colors, from tableau are generally a good choice for most graphs.\nWait, but how does this look like? Let’s see:\n\n\nCode\nsysuse auto, clear\nscatter mpg weight, title(\"Miles per Gallon vs. Weight\") /// Adds a title\n                    xtitle(\"Weight (lbs)\") /// label to the x-axis\n                    ytitle(\"Miles per Gallon\") /// and y-axis \n                    scale(1.5)\n\n\n(1978 automobile data)\n\n\n\n\n\n\n\n\n\nFrom here on, I will produce the figures directly, and you can check the code to reproduce them in your own work."
  },
  {
    "objectID": "chatgpt/stata_08_06_2024/index.html#scatter-plots",
    "href": "chatgpt/stata_08_06_2024/index.html#scatter-plots",
    "title": "Stata Crash Course: Graphs and EDA",
    "section": "Scatter Plots",
    "text": "Scatter Plots\nScatter plots, what we just did, its perhaps the most basic plot you will use. They are excellent for visualizing the relationship between two continuous variables. They can reveal patterns, correlations, and outliers in your data.\nThe basic syntax for a scatter plot is:\nscatter [varlist y] [xvar] [if] [in] [weight] [, options]\nAs you may expect, varlist y can be a list of variables, all which will be plotted against xvar. However, I would recommend you to use only one variable at a time.\nIf multiple variables are required, you can use twoway to overlay them.\ntwoway (scatter [yvar1] [xvar] [if] [in] [weight] [, options]) /// &lt;-first\n       (scatter [yvar2] [xvar] [if] [in] [weight] [, options]) // &lt;-second\nEach scatter plot can have its own options, colors, and labels.\nFor example:\n\n\nCode\nsysuse auto, clear\ntwoway (scatter mpg weight if foreign==0) ///\n       (scatter mpg weight if foreign==1) ///\n        , title(\"Miles per Gallon vs. Weight: By foreign\") ///\n          xtitle(\"Weight (lbs)\") ///\n          ytitle(\"Miles per Gallon\") ///\n          scale(1.5) ///\n          legend(order(1 \"Domestic\" 2 \"Foreign\") )\n\n\n(1978 automobile data)\n\n\n\n\n\n\n\n\n\nNow this plot is combining various features.\n\nThe twoway command (two for short) is used to overlay the two scatter plots.\nlegend adds a legend to the graph, specifying the order of the labels. In white2 scheme, the legend is at the left of the graph.\nIf you would like to change the position of the legend, you can use position(#), where # is a number from 1 to 12. You may also want to use cols(#) or rows() to change the layout of the legend.\n\nlegend(order(1 \"Domestic\" 2 \"Foreign\") position(3) cols(1))\n\nCustomizing Markers\nYou can change the appearance of the markers using various options, like mcolor(), msymbol(), or msize(). From all of these, I prefer using pstyle() to change the colors, and other options for the size of shape, because pstyle() is connected to the color scheme you are using, and mcolor() for added options like transparency. Lets try this with the previous example:\n\n\nCode\nsysuse auto, clear\ntwoway (scatter mpg weight if foreign==0, pstyle(p3) msize(2)) ///\n       (scatter mpg weight if foreign==1, pstyle(p5) mcolor(%50)) ///\n        , title(\"Miles per Gallon vs. Weight: By foreign\") ///\n          xtitle(\"Weight (lbs)\") ///\n          ytitle(\"Miles per Gallon\") ///\n          scale(1.5) ///\n          legend(order(1 \"Domestic\" 2 \"Foreign\") )\n\n\n(1978 automobile data)\n\n\n\n\n\n\n\n\n\n\n\nOther interesting options\nThere are other options you may want to try. For example, mlabel() and mlabsize() to add labels to the points. Just be careful with the size of the labels, as the figure could become very busy. If you try the code below, you will see that the figure is not very informative.\nscatter mpg weight, mlabel(make) ///\n                    mlabsize(tiny)\nThis adds the car make as a label to each point.\n\n\nAdvanced Scatter Plot\nLet’s combine several elements to create a more informative scatter plot:\n\n\nCode\ntwo (scatter mpg weight, mcolor(%30) msize(5) ) ///\n    (scatter mpg weight in 1/5,  pstyle(p1) mlabel(make) mlabsize(small) ), ///\n                    title(\"MPG vs. Weight\") ///\n                    subtitle(\"1978 Automobile Data\") ///\n                    xtitle(\"Weight (lbs)\") ytitle(\"Miles per Gallon\") ///\n                    note(\"Source: Auto dataset\") ///\n                    legend(off) scale(1.4)\n\n\n\n\n\n\n\n\n\nThis plot includes: - Semi-transparent circles - Labels for few points (car make) - Informative title, subtitle, and axis labels - A note indicating the data source - No legend (turned off) (every time you have more than 1 plot, the legend will be displayed)\n\n\nA Different type of scatter Plot\nWhile you can always use scatter plot for plotting multiple multiple variables, any now and then you will need a more “brute force approach”, where a large set of variables are plotted at aonce. If you are interested in this, you can use a scatter plot matrix. It has the following syntax:\ngraph matrix [varlist], [half]\nWhere varlist is a list of variables to be plotted. This will make a matrix of scatter plots for all pairs of the specified variables. The half option will only plot the lower half of the matrix. Of course, such a plot can be very busy, so be careful with the number of variables you include. Here an example:\n\n\nCode\ngraph matrix price mpg weight length, half ///\n  xsize(12) ysize(8) title(\"Scatter Plot Matrix\")"
  },
  {
    "objectID": "chatgpt/stata_08_06_2024/index.html#line-plots",
    "href": "chatgpt/stata_08_06_2024/index.html#line-plots",
    "title": "Stata Crash Course: Graphs and EDA",
    "section": "Line Plots",
    "text": "Line Plots\nLine plots are the second most common type of plot you will use. They are excellent for visualizing trends over time or another continuous variable.\nIn general, I would classify them in three types. The basic line plot, and model based line plots.\nThe basic lineplot is the one that uses all the data to create a plot, going from one coordinate to the next. Think of it as a connect the dots plot. Because of this, the plot can be very different depending on the order of the data.\nThe model based plot is often more useful, as it uses some univariate model (linear, quadratic, etc) to create the line. This is often more useful, as it can show the trend of the data, without the noise.\nLets start with the basic line plot:\n\nBasic Line Plot\nSimilar to scatter plots, you can use multiple variables in a line plot, to be plotted against a single variable.\n\n\nCode\nsysuse uslifeexp, clear\nline le_w le_m year, ///\n  title(Life expectancy for men and women over time)\n\n\n(US life expectancy, 1900–1999)\n\n\n\n\n\n\n\n\n\nWhich works well, because the data was ordered by year. If i were to order it by Life expectancy, the plot would be very different, bordering non-sensical.\n\n\nCode\nsysuse uslifeexp, clear\nsort le_w\nline le_w le_m year, ///\n  title(Life expectancy for men and women over time)\n\n\n(US life expectancy, 1900–1999)\n\n\n\n\n\n\n\n\n\n\n\nCustomizing Line Styles\nThis plot allows for a few customizations. For example, you can change the line style, color, and width:\n\n\nCode\nsort year\nline le_w le_m year, lpattern(solid dash) ///\n            pstyle(p2 p3) ///\n            lwidth(thick medium) ///\n            xline(1930/1936, lw(2) lp(solid) lcolor(gs12)) ///\n            title(Life expectancy for men and women over time)\n\n\n\n\n\n\n\n\n\nIn addition to the line, I also added some shading for the years 1930 to 1936. Useful if you want to emphasize a particular period.\n\n\nAdvanced Customization\nAs with Scatter plot, and infact most two-way plots, you can customize the plot in many ways.\n\n\nCode\nline le_w le_m year, lpattern(solid dash) ///\n                     lcolor(red blue) ///\n                     lwidth(medthick medium) ///\n                     title(\"Life Expectancy Trends\") ///\n                     subtitle(\"United States, 1900-2000\") ///\n                     xtitle(\"Year\") ytitle(\"Life Expectancy (years)\") ///\n                     ylabel(40(10)80, angle(0)) ///\n                     xlabel(1900(20)2000) ///\n                     legend(order(1 \"Women\" 2 \"Men\")) ///\n                     note(\"Source: US Life Expectancy dataset\")\n\n\n\n\n\n\n\n\n\n\n\nModel based line plot\nSomething that its recommended to use more often are model-based line plots. That means that you are not plotting the data, but the predictions of a model. This is useful to better see patterns when the data is noisy.\nLets use the dui dataset and produce a combination of scatter plot with a line plot. On top of that, we will use confidence intervals. (typically means adding a ci to the command)\nLets see how they look like:\n\nLinear FitQuadratic FitNonParametric Fit\n\n\n\n\nCode\nwebuse dui, clear\ntwo (scatter citations fines) ///\n    (lfitci citations fines,  fcolor(%40)) , ///\n    legend(off) scale(1.4) ///\n    ytitle(\"Citations\") xtitle(\"Fines in 000$\")\n\n\n(Fictional data on monthly drunk driving citations)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nwebuse dui, clear\ntwo (scatter citations fines) ///\n    (qfitci citations fines,  fcolor(%40)) , ///\n    legend(off) scale(1.4) ///\n    ytitle(\"Citations\") xtitle(\"Fines in 000$\")\n\n\n(Fictional data on monthly drunk driving citations)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nwebuse dui, clear\ntwo (scatter citations fines) ///\n    (lpolyci citations fines,  fcolor(%40)) , ///\n    legend(off) scale(1.4) ///\n    ytitle(\"Citations\") xtitle(\"Fines in 000$\")\n\n\n(Fictional data on monthly drunk driving citations)"
  },
  {
    "objectID": "chatgpt/stata_08_06_2024/index.html#histograms-density-plots-and-box-plots",
    "href": "chatgpt/stata_08_06_2024/index.html#histograms-density-plots-and-box-plots",
    "title": "Stata Crash Course: Graphs and EDA",
    "section": "Histograms, Density Plots and Box Plots",
    "text": "Histograms, Density Plots and Box Plots\nWhen it comes to visualizing the distribution of a single variable, your main options are histograms, density plots, and box plots. Histograms and kdensity plots are both twoway plots that can be overlayed. Box plots, however, are graph plots, and are a bit more temperamental.\n\nHistograms\nThe first command we will see is the histogram command. This command is used to create histograms, which are bar charts that show the frequency distribution of a single variable.\nIn contrast with frequency tables, where you need to define the bins width or number of bins, histograms will do this for you. Not always in the best way, but it will do it.\nThe basic syntax for a histogram is:\nhistogram [varname] [if] [in] [weight] [, options]\nPerhaps the biggest complain I have about histograms is that they do not allow for aweights. So if your weights are fractional, you may need to consider other works around.\nLets see how this works\n\n\nBasic Histogram\nThe basic histogram plot will try to produce densities on the y-axis. It usually labels it correctly.\nYou can, however, request frequency or percent on the y-axis.\n\n\nCode\nwebuse dui, clear\n\nhistogram fines, ///\n  title(\"Histogram of Fines\") ///\n  xtitle(\"Fines in 000$\") \n\n\n(Fictional data on monthly drunk driving citations)\n(bin=22, start=7.4000001, width=.2090909)\n\n\n\n\n\n\n\n\n\nBy default, histogram will calculate the number of bins for you. However, you can specify the number of bins using the bin() option. Alternatively, you can use width() instead, making it either more or less granular.\n\nWider Width()Narrower Width()\n\n\n\n\nCode\nhistogram fines, ///\n  title(\"Histogram of Fines\") ///\n  xtitle(\"Fines in 000$\") width(.4)\n\n\n(bin=12, start=7.4000001, width=.4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nhistogram fines, ///\n  title(\"Histogram of Fines\") ///\n  xtitle(\"Fines in 000$\") width(.1)\n\n\n(bin=46, start=7.4000001, width=.1)\n\n\n\n\n\n\n\n\n\n\n\n\nWider bins will make the plot more general, but you lose some of the detail. Narrower bins will make the plot more detailed, but you may lose the overall picture. Its the classic bias-variance tradeoff.\n\n\nOverlaying Histograms\nAs mentioned before, because histograms are twoway plots, you can overlay them. This is useful when you want to compare the distribution of a variable across different groups (Low number of groups). Note that by default, histograms come with just one color, but you can change this using the pstyle() option.\nYou may want to use transparency to make the plot more readable. and even use other options to make the plot visually appealing.\nFor the last example, I will also use a global to pass options that are common to all subplots.\n\nSimple OverlapWith TransparencyFixing widths\n\n\n\n\nCode\ntwo (histogram citations if csize==1, pstyle(p1)) ///\n    (histogram citations if csize==2, pstyle(p2)) ///\n    (histogram citations if csize==3, pstyle(p3)) , ///\n    title(\"Histogram of citations\") ///\n    subtitle(\"By city Size\") ///\n    xtitle(\"Fines in 000$\") ///\n    legend(order(1 \"Small\" 2 \"Medium\" 3 \"Large\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntwo (histogram citations if csize==1, pstyle(p1) color(%75)) ///\n    (histogram citations if csize==2, pstyle(p2) color(%75)) ///\n    (histogram citations if csize==3, pstyle(p3) color(%75)), ///\n    title(\"Histogram of citations\") ///\n    subtitle(\"By city Size\") ///\n    xtitle(\"Fines in 000$\") ///\n    legend(order(1 \"Small\" 2 \"Medium\" 3 \"Large\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nglobal options width(3) start(0) color(%55)\ntwo (histogram citations if csize==1, pstyle(p1) $options) ///\n    (histogram citations if csize==2, pstyle(p2) $options) ///\n    (histogram citations if csize==3, pstyle(p3) $options), ///\n    title(\"Histogram of citations\") ///\n    subtitle(\"By city Size\") ///\n    xtitle(\"Fines in 000$\") ///\n    legend(order(1 \"Small\" 2 \"Medium\" 3 \"Large\"))\n\n\n\n\n\n\n\n\n\n\n\n\nOne more note. Histograms assume your dependent variable is continuous. However, you could also use them as discrete, by simply adding the discrete option. Unless width() is specified, each bar will represent a single value."
  },
  {
    "objectID": "chatgpt/stata_08_06_2024/index.html#kernel-density-plots",
    "href": "chatgpt/stata_08_06_2024/index.html#kernel-density-plots",
    "title": "Stata Crash Course: Graphs and EDA",
    "section": "Kernel Density Plots",
    "text": "Kernel Density Plots\nAs mentione before, histograms are a good way to visualize distributions of a single variable. Something that histograms do not do well is provide a smooth estimate of the probability density function. And, in Stata, its not possible to add fractional weights to histograms.\nThere is, however, another option. Kernel Density Plots. These plots provide a smooth estimate of the probability density function of a variable. As with Histograms, they can be very sensitive to the choice of bandwidth (how smooth or rugged the plot is).\nOne should also note that Kernel density plots cannot be used to plot distributions of discrete variables, and may provide misleading results if used with data that have constraints, or very relevant modes.\nThe basic syntax for a kernel density plot is:\nkdensity [varname] [if] [in] [weight] [, options]\nAnd because its a twoway plot, you can overlay them with other plots.\n\nBasic Kernel Density Plot\nLets see how this works with the dui dataset.\n\n\nCode\nwebuse dui, clear\nkdensity citation, ///\n  title(\"Kernel Density Estimate of Citations\") ///\n  xtitle(\"# of Citations\") ///\n  ytitle(\"Density\") scale(1.5) \n\n\n(Fictional data on monthly drunk driving citations)\n\n\n\n\n\n\n\n\n\nNoticed how smooth the plot is. The bwidth was set to 2.31, which is constructed using a rule of thumb. It also uses epanechnikov as the default kernel. While you can change the kernel() function, the default one is “optimal” for most cases. What would be more important is the bwidth() option.\n\n\nKernel Densities and bandwidth choice\nAs mentioned before, the bandwidth is a very important parameter. You can set it manually, with the understanding that a smaller bandwidth will make the plot more rugged, and a larger bandwidth will make it smoother. At the limit (infinite bandwidth), the plot will be a horizontal line, and on the other end, it would look like a histogram (each value its own curve)\n\nSmaller BandwidthLarger Bandwidth\n\n\n\n\nCode\nwebuse dui, clear\nkdensity citation, ///\n  title(\"Kernel Density Estimate of Citations\") ///\n  xtitle(\"# of Citations\") ///\n  ytitle(\"Density\") scale(1.5) bw(1)\n\n\n(Fictional data on monthly drunk driving citations)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nwebuse dui, clear\nkdensity citation, ///\n  title(\"Kernel Density Estimate of Citations\") ///\n  xtitle(\"# of Citations\") ///\n  ytitle(\"Density\") scale(1.5) bw(4)\n\n\n(Fictional data on monthly drunk driving citations)\n\n\n\n\n\n\n\n\n\n\n\n\nPerhaps of interest to some, the Kernel Density does not estimate the bandwidth for every point of data. Instead, it uses a map of points to reduce the computational burden. Then, simply makes a lineplot. If you want to see the curve for every point in your data, you have to use the at() option.\n\n\nCurves, Areas, and Overlays\nMany of the options you can use with histograms, scatterplots or line plots can also be used with Kernel Density plots. However, I will introduce one that some may find useful: recast. This option allows you to change the plot type from a line to something else. For example, you can use recast(area) to fill the area under the curve.\n\n\nCode\nwebuse dui, clear\nkdensity citation, ///\n  title(\"Kernel Density Estimate of Citations\") ///\n  xtitle(\"# of Citations\") ///\n  ytitle(\"Density\") scale(1.5) ///\n  recast(area) \n\n\n(Fictional data on monthly drunk driving citations)\n\n\n\n\n\n\n\n\n\nThis is not the most effective way of changing the plot type, but does the work if the data is small. With larger datasets, other methods may be more useful.\nWhat about overlaying? We can do that as well, as we did with histograms. Ohh, and in contrast with histograms, you do not need to specify colors, as you did with histograms.\n\nSimple OverlapAs an AreaAdding transparency\n\n\n\n\nCode\nglobal options bw(3)\ntwo (kdensity citations if csize==1, $options ) ///\n    (kdensity citations if csize==2, $options ) ///\n    (kdensity citations if csize==3, $options ) , ///\n    title(\"kdensity of citations\") ///\n    subtitle(\"By city Size\") ///\n    xtitle(\"Fines in 000$\") ///\n    legend(order(1 \"Small\" 2 \"Medium\" 3 \"Large\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nglobal options bw(3) recast(area  )\ntwo (kdensity citations if csize==1, $options ) ///\n    (kdensity citations if csize==2, $options ) ///\n    (kdensity citations if csize==3, $options ) , ///\n    title(\"kdensity of citations\") ///\n    subtitle(\"By city Size\") ///\n    xtitle(\"Fines in 000$\") ///\n    legend(order(1 \"Small\" 2 \"Medium\" 3 \"Large\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nglobal options bw(3) recast(area) color(%50)\ntwo (kdensity citations if csize==1, $options ) ///\n    (kdensity citations if csize==2, $options ) ///\n    (kdensity citations if csize==3, $options ) , ///\n    title(\"kdensity of citations\") ///\n    subtitle(\"By city Size\") ///\n    xtitle(\"Fines in 000$\") ///\n    legend(order(1 \"Small\" 2 \"Medium\" 3 \"Large\"))\n\n\n\n\n\n\n\n\n\n\n\n\nFew things to notice here.\n\nBecause im using less data, the default bandwidth makes a rougher plot. I increase it to 3 to make it smoother.\nSince the data by groups have “hard” limits, the plots are cut at the edges.\nThese plot cannot be used to compare the sizes of the groups, as the area under the curve is the same by definition."
  },
  {
    "objectID": "chatgpt/stata_08_06_2024/index.html#range-plots",
    "href": "chatgpt/stata_08_06_2024/index.html#range-plots",
    "title": "Stata Crash Course: Graphs and EDA",
    "section": "Range plots",
    "text": "Range plots\nThere is a fairly large list of plots that fall under this category. Range plots are mostly used to show, as the name implies, a particular range of values. Perhaps the most common of these would be to show confidence intervals.\nThey all have a similar syntax:\ntwo range_plot y_lower y_higher x_variable [if] , [options]\nAs you can see, the main difference is that you need to specify a lower and upper bound for the y-axis. And, as you can imagine, this will generate a nice rendition of a range.\nPerhaps the biggest issue with these plots is that you need to prepare the data beforehand. This is not a big issue.\nSo lets see how this works:\n\nRanges Galore\nTo show how this type of plots work, I will use the dataset cattaneo2. This dataset contains data for childrens weight at birth, in addition to other relevant information. I will do some data manipulation to create the data I need for the plots\n\nwebuse cattaneo2, clear\n// Constrain age of mother\ndrop if mage&lt;18 | mage&gt;35\n// Get summary for  Weight at birth\n// mean and p25 and p75 by age and smoking\ncollapse (mean) mean_bweight=bweight ///\n         (p25) p25_bweight=bweight ///\n         (p75) p75_bweight=bweight, by(mage mbsmoke)\n\n(Excerpt from Cattaneo (2010) Journal of Econometrics 155: 138–154)\n(488 observations deleted)\n\n\nNow that we have the data, lets create the plot(s). Each one will show the range of weights at birth for different groups of mothers. To add additional information, I will also add the mean “weight at birth” as a scatter plot.\n\nAreaBarSpikeCapCapSym\n\n\n\n\nCode\ntwo (rarea p25_bweight p75_bweight mage if mbsmoke==0, color(%50)) ///\n    (scatter mean_bweight mage if mbsmoke==0, pstyle(p1) connect(l)) , ///\n    title(\"Range of Birth Weight by Mother's Age\") ///\n    xtitle(\"Mother's Age\") ///\n    ytitle(\"Birth Weight (lbs)\") ///\n    legend(off) scale(1.4) ///\n    note(\"Note: The shaded area represents the 25th to 75th percentile range\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntwo (rbar p25_bweight p75_bweight mage if mbsmoke==0, color(%50)) ///\n    (scatter mean_bweight mage if mbsmoke==0, pstyle(p1) connect(l)) , ///\n    title(\"Range of Birth Weight by Mother's Age\") ///\n    xtitle(\"Mother's Age\") ///\n    ytitle(\"Birth Weight (lbs)\") ///\n    legend(off) scale(1.4) ///\n    note(\"Note: The shaded area represents the 25th to 75th percentile range\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntwo (rspike p25_bweight p75_bweight mage if mbsmoke==0, color(%50)) ///\n    (scatter mean_bweight mage if mbsmoke==0, pstyle(p1) connect(l)) , ///\n    title(\"Range of Birth Weight by Mother's Age\") ///\n    xtitle(\"Mother's Age\") ///\n    ytitle(\"Birth Weight (lbs)\") ///\n    legend(off) scale(1.4) ///\n    note(\"Note: The shaded area represents the 25th to 75th percentile range\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntwo (rcap p25_bweight p75_bweight mage if mbsmoke==0, color(%50)) ///\n    (scatter mean_bweight mage if mbsmoke==0, pstyle(p1) connect(l)) , ///\n    title(\"Range of Birth Weight by Mother's Age\") ///\n    xtitle(\"Mother's Age\") ///\n    ytitle(\"Birth Weight (lbs)\") ///\n    legend(off) scale(1.4) ///\n    note(\"Note: The shaded area represents the 25th to 75th percentile range\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntwo (rcapsym p25_bweight p75_bweight mage if mbsmoke==0, color(%50)) ///\n    (scatter mean_bweight mage if mbsmoke==0, pstyle(p1) connect(l)) , ///\n    title(\"Range of Birth Weight by Mother's Age\") ///\n    xtitle(\"Mother's Age\") ///\n    ytitle(\"Birth Weight (lbs)\") ///\n    legend(off) scale(1.4) ///\n    note(\"Note: The shaded area represents the 25th to 75th percentile range\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe rest (rscatter, rline and rconnected) are not very useful, as they are just a scatter plot, line plots, or both.\nAnd of course, you can combine them!\n\n\nCode\ntwo (rarea p25_bweight p75_bweight mage if mbsmoke==0, pstyle(p1) color(%50)) ///\n    (scatter mean_bweight mage if mbsmoke==0, pstyle(p1) connect(l)) ///\n    (rarea p25_bweight p75_bweight mage if mbsmoke==1, pstyle(p2) color(%50)) ///\n    (scatter mean_bweight mage if mbsmoke==1, pstyle(p2) connect(l)) , ///\n    title(\"Range of Birth Weight by Mother's Age\") ///\n    xtitle(\"Mother's Age\") ///\n    ytitle(\"Birth Weight (lbs)\") ///\n    legend(order(1 \"NonSmokers\" 3 \"Smokers\")) ///\n    scale(1.4) ///\n    note(\"Note: The shaded area represents the 25th to 75th percentile range\")"
  },
  {
    "objectID": "chatgpt/stata_08_02_2024/index.html",
    "href": "chatgpt/stata_08_02_2024/index.html",
    "title": "Stata Crash Course: Data management",
    "section": "",
    "text": "As you know, my weapon of choice for data analysis is Stata. However, even with my experience, I still find myself going back to the basics, especially when it comes to guiding students through their projects.\nSo, today, I wanted to take a moment to go over some of the fundamental aspects of data management in Stata, covering some of the fundamental toos on working with data. Even if you’re already familiar with Stata, a refresher never hurts!"
  },
  {
    "objectID": "chatgpt/stata_08_02_2024/index.html#creating-new-variables",
    "href": "chatgpt/stata_08_02_2024/index.html#creating-new-variables",
    "title": "Stata Crash Course: Data management",
    "section": "Creating New Variables",
    "text": "Creating New Variables\n\ngenerate : This is the most basic way to create new variables in Stata. All transformations are done observation by observation.\ngenerate [type] new_var = expression [if in]\nExamples:\ngenerate bmi = weight / (height/100)^2\ngenerate log_income = log(income)\ngenerate is_adult = (age &gt;= 18)\nUsing functions:\ngenerate fullname = strproper(firstname + \" \" + lastname)\ngenerate age_squared = age^2"
  },
  {
    "objectID": "chatgpt/stata_08_02_2024/index.html#modifying-existing-variables",
    "href": "chatgpt/stata_08_02_2024/index.html#modifying-existing-variables",
    "title": "Stata Crash Course: Data management",
    "section": "Modifying Existing Variables",
    "text": "Modifying Existing Variables\n\nreplace : Second most common command in Stata. Works like generate, but it modifies existing variables:\nreplace var = expression [if condition]\nExamples:\nreplace income = income * 1.1 if year == 2023\nreplace age = . if age &lt; 0  // Set negative ages to missing\nreplace education = \"High School\" if inlist(education, \"HS\", \"Secondary\")"
  },
  {
    "objectID": "chatgpt/stata_08_02_2024/index.html#more-on-generating-variables-egen",
    "href": "chatgpt/stata_08_02_2024/index.html#more-on-generating-variables-egen",
    "title": "Stata Crash Course: Data management",
    "section": "More on generating variables: egen",
    "text": "More on generating variables: egen\n\negen : This is a powerful command that allows you to generate new variables based on complex conditions by observations or across groups. You can even develop your own functions to use with egen. egen can work using information across observations, like the mean of a variable. Useful to have ssc install egenmore installed.\negen new_var = function(varlist) [if condition], [options]\negen can also have their own options.\nExamples:\negen mean_income = mean(income)\negen total_sales = total(sales)\negen max_age = max(age)\negen group_mean_income = mean(income), by(group_var)"
  },
  {
    "objectID": "chatgpt/stata_08_02_2024/index.html#recoding-variables",
    "href": "chatgpt/stata_08_02_2024/index.html#recoding-variables",
    "title": "Stata Crash Course: Data management",
    "section": "Recoding Variables",
    "text": "Recoding Variables\n\nrecode : Useful for transforming values into categories or changing values in categorical variables. The rules can include ranges, individual values, or missing values, as well as value labels.\nrecode var(s) (rule) (rule) ..., generate(new_var)\nExamples:\nrecode age (0/18=1 \"Child\") (19/65=2 \"Adult\") (66/max=3 \"Senior\"), gen(age_group)\nrecode income (0/1000=1) (1001/5000=2) (5001/max=3), gen(income_category)\nencode : Useful for converting string variables to numeric codes. It creates a new numeric variable that represents the original string variable, which is used as labels. The values are assigned in alphabetical order.\nencode str_var, gen(num_var)\ndecode : The reverse of encode. It replaces the numeric codes with the label values.\ndecode str_var, gen(num_var)"
  },
  {
    "objectID": "chatgpt/stata_08_02_2024/index.html#conditional-statements",
    "href": "chatgpt/stata_08_02_2024/index.html#conditional-statements",
    "title": "Stata Crash Course: Data management",
    "section": "Conditional Statements",
    "text": "Conditional Statements\n\nif condition: Most commands in Stata allow you to specify conditions on the data to be analyzed or modified.\ncommand ... if expression\nExamples:\nsummarize income if gender == \"Female\"\nlist name age if age &gt; 65 & !missing(income)\nWarning: When comparing numbers with missing values, remember that “missing” is always the highest value.\nUsing logical operators:\n\n& (and), | (or), ! (not) They can be combined to create complex conditions. Parentheses can be used to control the order of operations.\n\ngenerate high_income_male = (income &gt; 100000) & (gender == \"Male\")"
  },
  {
    "objectID": "chatgpt/stata_08_02_2024/index.html#labeling-variables-and-values",
    "href": "chatgpt/stata_08_02_2024/index.html#labeling-variables-and-values",
    "title": "Stata Crash Course: Data management",
    "section": "Labeling Variables and Values",
    "text": "Labeling Variables and Values\n\nVariable labels: Best way to provide a description of the variable.\nlabel variable var \"label\"\nLabels have a maximum length of 80 characters. If you need more, consider using notes.\nValue labels:\nlabel define label_name #1 \"label1\" #2 \"label2\" ... [, modify]\nlabel values var label_name\nOnce a label is defined, it can be applied to multiple variables. The modify option allows you to add new labels to an existing set, or modify existing ones.\nExamples:\nlabel variable bmi \"Body Mass Index\"\nlabel define bmi_cat 1 \"Underweight\" 2 \"Normal\" 3 \"Overweight\" 4 \"Obese\"\nlabel values bmi bmi_cat\nChecking labels:\nlabel list"
  },
  {
    "objectID": "chatgpt/stata_08_02_2024/index.html#working-with-missing-values",
    "href": "chatgpt/stata_08_02_2024/index.html#working-with-missing-values",
    "title": "Stata Crash Course: Data management",
    "section": "Working with Missing Values",
    "text": "Working with Missing Values\n\nStata’s missing value for numerical variables: .\nExtended missing values: .a to .z\nRemember .&gt;.z&gt;...&gt;.a&gt;any number\nIdentifying missing values:\nlist if missing(var)\ncount if missing(var)\nRecoding missing values:\nreplace var = 0 if missing(var)"
  },
  {
    "objectID": "chatgpt/stata_08_02_2024/index.html#renaming-variables",
    "href": "chatgpt/stata_08_02_2024/index.html#renaming-variables",
    "title": "Stata Crash Course: Data management",
    "section": "Renaming Variables",
    "text": "Renaming Variables\n\nrename : To change the name of a variable. You can rename multiple variables at once.\nrename old_name new_name\nExamples:\nrename income annual_income\nrename (var1 var2 var3) (new1 new2 new3)"
  },
  {
    "objectID": "chatgpt/stata_08_02_2024/index.html#reshaping-data",
    "href": "chatgpt/stata_08_02_2024/index.html#reshaping-data",
    "title": "Stata Crash Course: Data management",
    "section": "Reshaping Data",
    "text": "Reshaping Data\nAlso known as pivoting data. It involves changing the structure of the data from wide to long or vice versa.\n\nWide to Long:\nreshape long stub, i(id) j(time)\nstub is the common prefix of the variables to be reshaped. and time the new variable name that will hold the sufix of the variables. id is the identifier of the observations.\nLong to Wide: The same as before, but the data is reshaped in the opposite direction.\nreshape wide stub, i(id) j(time)\nstub is the variable that will be reshaped. Data should be uniquely identified by the i() and j() variables. The values of j() will be used to create the new variable names combining the stub."
  },
  {
    "objectID": "chatgpt/stata_08_02_2024/index.html#merging-datasets",
    "href": "chatgpt/stata_08_02_2024/index.html#merging-datasets",
    "title": "Stata Crash Course: Data management",
    "section": "Merging Datasets",
    "text": "Merging Datasets\nUseful to work with relational data. You can merge datasets based on common variables (keys).\nWhen using merge, the master dataset is the one currently in memory, and the using dataset is the one being added.\nThere are different types of merges:\n\nOne-to-one merge:\nmerge 1:1 key using filename\nMany-to-one merge:\nmerge m:1 key using filename\nOne-to-many merge:\nmerge 1:m key using filename\nThey key is the variable that is common to both datasets, and it should be unique in the “1” dataset. Master (1:1 and 1:m) or using (1:1 and m:1) datasets.\nExample:\nmerge 1:1 id using employee_details.dta\nmerge m:1 state using state_info.dta\nHandling merge results: Unless specified, Stata will create a new variable _merge to indicate the merge status. If one exists in the master or using dataset, the merge will fail.\ndrop if _merge == 2  // Drop unmatched from using"
  },
  {
    "objectID": "chatgpt/stata_08_02_2024/index.html#appending-datasets",
    "href": "chatgpt/stata_08_02_2024/index.html#appending-datasets",
    "title": "Stata Crash Course: Data management",
    "section": "Appending Datasets",
    "text": "Appending Datasets\n\nAdding observations: Useful when you have multiple datasets with the same structure and want to combine them, in the long way.\nappend using filename\n\nIn older versions, you needed to have a dataset already open in memory. Now, you can append without a dataset in memory.\n\nExample:\nuse survey2022.dta\nappend using survey2023.dta\nMake sure the variables are of the same type. (String and numeric do not mix)"
  },
  {
    "objectID": "chatgpt/stata_08_02_2024/index.html#sorting-data",
    "href": "chatgpt/stata_08_02_2024/index.html#sorting-data",
    "title": "Stata Crash Course: Data management",
    "section": "Sorting Data",
    "text": "Sorting Data\n\nsort : Some commands or procedures require the data to be sorted in a specific order. You can sort by multiple variables.\nsort var1 [var2 ...]\nWhen there are ties, Stata will sort records randomly, unless stable is specified.\ngsort is a variant of sort that can sort in ascending or descending order.\ngsort -var  // Sort in descending order\nExample:\nsort state year\ngsort -income  // Sort in descending order\nCreating a sorted index:\nsort state year\ngen index = _n"
  },
  {
    "objectID": "chatgpt/stata_08_02_2024/index.html#creating-complex-variables-with-by-and-bysort",
    "href": "chatgpt/stata_08_02_2024/index.html#creating-complex-variables-with-by-and-bysort",
    "title": "Stata Crash Course: Data management",
    "section": "Creating Complex Variables with by and bysort",
    "text": "Creating Complex Variables with by and bysort\n\nby command: Only works if data is already sorted. Operations are performed within groups defined by the variable(s) specified.\nby varlist: command\nbysort command (combines sorting and by):\nbysort varlist: command\nExamples:\n\nCalculate group means:\nbysort state: egen state_avg_income = mean(income)\negen state_avg_income = mean(income), by(state)\nCreate ranking within groups:\nbysort industry: egen income_rank = rank(income)\nGenerate group-specific indicators:\nbysort state: gen state_obs = _n\nbysort state: gen state_total_obs = _N\nCreate cumulative sums within groups:\nsort state date\nby state: gen cumulative_sales = sum(daily_sales)\nIdentify first and last observations in each group:\nbysort company_id (date): gen first_obs = _n == 1\nbysort company_id (date): gen last_obs = _n == _N"
  },
  {
    "objectID": "chatgpt/rep_03_31_2023/index.html",
    "href": "chatgpt/rep_03_31_2023/index.html",
    "title": "Why are my estimates changing every time I run my do-files?!",
    "section": "",
    "text": "I have read this problem quite a few times in Statalist as well as other forums to like Stackoverflow. You try to do the right thing. Write and document a do-file, to produce your results the same way every time, but, for some reason they key changing. The question is always Why?\nOf course, there could be a large number of reasons you this is happening, but I will narrow them to the most common causes:\n\nrandomness The first one is the most obvious. You may be trying to introduce in your program a procedure that relies on some random process. Bootstrap standard errors or creating of simulated data are the most common couses for this. Solution: set seed.\nsort The second most common problem is because you have some statement in your dofile that relies on sorting data with ties. When this happens, the default solution in Stata is to randomly assign an order to observations with the same value, which may affect procedures that are order dependent (matching). This is not a problem when you are sorting fully identified data. But if ties exists, it may kick in. Solution: set sortseed. Although one should be careful in analyzing if this is the true cause of replicability, or rather the next option\nmerging Most merging procedures in Stata are well defined, and won’t cause any problem: 1:m, m:1 and 1:1 will not create much of a problem. However, there is the infamous m:m. As many other people may have said this before…you NEVER-EVER do merge m:m. If you think you need to do it, think of jointby, because merge m:m is always wrong. In fact, this may trigger a problem that may “seem” caused and solved as mentionted before, so be mindful.\nMulticolinearity Something that occurs less often are problems related to multicolinearity. When two variables are collinear, and you are trying to estimate a linear or linear like model, Stata will try to fix the problem by dropping some observations before the estimation occurs. Most of the time, the excluded variable is the first one, but more often than not, the excluded variable would be at random. Solution: Revise your model specification.\n\nIf you have other reasons you have found to cause problems replicating your results, let me know, and we can add them to the list."
  },
  {
    "objectID": "chatgpt/quar_01_25_2024/reg_md.html",
    "href": "chatgpt/quar_01_25_2024/reg_md.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n(2)\n\n\n\n\n\neduc\n0.0848***\n(0.00519)\n0.0644***\n(0.00508)\n\n\nexper\n0.0111***\n(0.00154)\n-0.000311\n(0.00181)\n\n\ntenure\n0.00771***\n(0.00188)\n0.00672***\n(0.00190)\n\n\nfemale\n-0.0841***\n(0.0251)\n-0.160***\n(0.0243)\n\n\nage\n\n\n0.113***\n(0.00814)\n\n\nagesq\n\n\n-0.00126***\n(0.0000976)\n\n\nmarried\n\n\n-0.0224\n(0.0294)\n\n\ndivorced\n\n\n0.0719\n(0.0412)\n\n\n_cons\n2.213***\n(0.0683)\n0.311*\n(0.149)\n\n\nN\n1434\n\n1434\n\n\n\n\nStandard errors in parentheses * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "chatgpt/merge_04_20_2023/index.html",
    "href": "chatgpt/merge_04_20_2023/index.html",
    "title": "Merging Data 1:1, m:1, 1:m and m:m?",
    "section": "",
    "text": "When you can uniquely identify all observations in each dataset using a common identifier viariable.\nuse file1\nmerge 1:1 id using file2\n\n\n\n\n\nflowchart BT\n    subgraph File2\n    db1-1[\"id=1\"]\n    db1-2[\"id=2\"]\n    db1-3[\"id=3\"]\n    end\n    subgraph File1\n    db2-1[\"id=1\"]\n    db2-2[\"id=2\"]\n    db2-3[\"id=3\"]\n    end\n    \n    db1-1--&gt;db2-1\n    db1-2--&gt;db2-2\n    db1-3--&gt;db2-3"
  },
  {
    "objectID": "chatgpt/merge_04_20_2023/index.html#merging-11",
    "href": "chatgpt/merge_04_20_2023/index.html#merging-11",
    "title": "Merging Data 1:1, m:1, 1:m and m:m?",
    "section": "",
    "text": "When you can uniquely identify all observations in each dataset using a common identifier viariable.\nuse file1\nmerge 1:1 id using file2\n\n\n\n\n\nflowchart BT\n    subgraph File2\n    db1-1[\"id=1\"]\n    db1-2[\"id=2\"]\n    db1-3[\"id=3\"]\n    end\n    subgraph File1\n    db2-1[\"id=1\"]\n    db2-2[\"id=2\"]\n    db2-3[\"id=3\"]\n    end\n    \n    db1-1--&gt;db2-1\n    db1-2--&gt;db2-2\n    db1-3--&gt;db2-3"
  },
  {
    "objectID": "chatgpt/merge_04_20_2023/index.html#merging-m1",
    "href": "chatgpt/merge_04_20_2023/index.html#merging-m1",
    "title": "Merging Data 1:1, m:1, 1:m and m:m?",
    "section": "Merging m:1",
    "text": "Merging m:1\nWhen you are trying to transfer family level data to each family member. The family member file is uniquely identified\nuse family_members\nmerge m:1 hid using family\n\n\n\n\n\nflowchart BT\n    subgraph members [\"Family Members\"]\n    db1-1[\"hid=1,pid=1\"]\n    db1-2[\"hid=1,pid=2\"]\n    db1-3[\"hid=2,pid=1\"]\n    db1-4[\"hid=2,pid=2\"]\n    db1-5[\"hid=3,pid=1\"]\n    db1-6[\"hid=3,pid=2\"]\n    db1-7[\"hid=3,pid=3\"]\n    end\n    subgraph Family\n    db2-1[\"hid=1\"]\n    db2-2[\"hid=2\"]\n    db2-3[\"hid=3\"]\n    end\n    \n    db2-1--&gt;db1-1\n    db2-1--&gt;db1-2\n    db2-2--&gt;db1-3\n    db2-2--&gt;db1-4\n    db2-3--&gt;db1-5\n    db2-3--&gt;db1-6\n    db2-3--&gt;db1-7"
  },
  {
    "objectID": "chatgpt/merge_04_20_2023/index.html#merging-1m",
    "href": "chatgpt/merge_04_20_2023/index.html#merging-1m",
    "title": "Merging Data 1:1, m:1, 1:m and m:m?",
    "section": "Merging 1:m",
    "text": "Merging 1:m\nWhen you try to merge together data from various family members to a single family file. Each member is merged to its family.\nuse family\nmerge 1:m hid using family_members\n\n\n\n\n\nflowchart BT\n    subgraph members [\"Family Members\"]\n    db1-1[\"hid=1,pid=1\"]\n    db1-2[\"hid=1,pid=2\"]\n    db1-3[\"hid=2,pid=1\"]\n    db1-4[\"hid=2,pid=2\"]\n    db1-5[\"hid=3,pid=1\"]\n    db1-6[\"hid=3,pid=2\"]\n    db1-7[\"hid=3,pid=3\"]\n    end\n    subgraph Family\n    db2-1[\"hid=1\"]\n    db2-2[\"hid=2\"]\n    db2-3[\"hid=3\"]\n    end\n    \n    db1-1--&gt;db2-1\n    db1-2--&gt;db2-1\n    db1-3--&gt;db2-2\n    db1-4--&gt;db2-2\n    db1-5--&gt;db2-3\n    db1-6--&gt;db2-3\n    db1-7--&gt;db2-3"
  },
  {
    "objectID": "chatgpt/merge_04_20_2023/index.html#merging-mm",
    "href": "chatgpt/merge_04_20_2023/index.html#merging-mm",
    "title": "Merging Data 1:1, m:1, 1:m and m:m?",
    "section": "Merging m:m",
    "text": "Merging m:m\nSomething you NEVER want to do. It tries to merge to datasets based on a single set identification variable. This variable does not identify unique observations in either file. The merge is done by “id” based on the order they appear in the data. It will typically provide very odd results.\nuse family_mem1\nmerge m:m id using family_mem2\n\n\n\n\n\nflowchart BT\n    subgraph members [\"Family Members 1\"]\n    db1-1[\"hid=1,pid=1\"]\n    db1-2[\"hid=1,pid=2\"]\n    db1-3[\"hid=2,pid=1\"]\n    db1-4[\"hid=2,pid=2\"]\n    db1-5[\"hid=3,pid=1\"]\n    db1-6[\"hid=3,pid=2\"]\n    db1-7[\"hid=3,pid=3\"]\n    end\n    subgraph Family [\"Family Members 2\"]\n    db2-1[\"hid=1,pid=2\"]\n    db2-2[\"hid=1,pid=3\"]\n    db2-3[\"hid=1,pid=4\"]\n    db2-4[\"hid=2,pid=1\"]\n    db2-5[\"hid=2,pid=3\"]\n    db2-6[\"hid=3,pid=2\"]\n    db2-7[\"hid=3,pid=4\"]\n    end\n    \n    db2-1--&gt;db1-1\n    db2-2--&gt;db1-2\n    db2-4--&gt;db1-3\n    db2-5--&gt;db1-4\n    db2-6--&gt;db1-5\n    db2-7--&gt;db1-6"
  },
  {
    "objectID": "chatgpt/merge_04_20_2023/index.html#true-merge-m-x-m",
    "href": "chatgpt/merge_04_20_2023/index.html#true-merge-m-x-m",
    "title": "Merging Data 1:1, m:1, 1:m and m:m?",
    "section": "True Merge m x m",
    "text": "True Merge m x m\nIf you were trying m:m merge (which is probably wrong), means you want to joinby. This means merge both files using all combinations of individuals that have the same id. This will create a very large dataset, unless other restrictions are applied.\nuse family_mem1\njoinby id using family_mem2\n\n\n\n\n\nflowchart BT\n    subgraph members [\"Family Members 1\"]\n    db1-1[\"hid=1,pid=1\"]\n    db1-2[\"hid=1,pid=2\"]\n    db1-3[\"hid=2,pid=1\"]\n    db1-4[\"hid=2,pid=2\"]\n    db1-5[\"hid=3,pid=1\"]\n    db1-6[\"hid=3,pid=2\"]\n    db1-7[\"hid=3,pid=3\"]\n    end\n    subgraph Family [\"Family Members 2\"]\n    db2-1[\"hid=1,pid=2\"]\n    db2-2[\"hid=1,pid=3\"]\n    db2-3[\"hid=1,pid=4\"]\n    db2-4[\"hid=2,pid=1\"]\n    db2-5[\"hid=2,pid=3\"]\n    db2-6[\"hid=3,pid=2\"]\n    db2-7[\"hid=3,pid=4\"]\n    end\n    \n    db2-1--&gt;db1-1\n    db2-2--&gt;db1-1\n    db2-3--&gt;db1-1\n\n    db2-1--&gt;db1-2\n    db2-2--&gt;db1-2\n    db2-3--&gt;db1-2\n\n    db2-4--&gt;db1-3\n    db2-4--&gt;db1-4\n    db2-5--&gt;db1-4\n    db2-5--&gt;db1-3\n\n    db2-6--&gt;db1-5\n    db2-6--&gt;db1-6\n    db2-6--&gt;db1-7\n    db2-7--&gt;db1-5\n    db2-7--&gt;db1-6\n    db2-7--&gt;db1-7"
  },
  {
    "objectID": "chatgpt/fra_03_30_2023/index.html",
    "href": "chatgpt/fra_03_30_2023/index.html",
    "title": "How to write your own Installer",
    "section": "",
    "text": "If you are an avid Stata user, you know that THE place to search and access user-written, now community-contributed, programs is via SSC.\nSSC is a command that allows you to install community-contributed programs from the Boston College Statistical Software Components, provided by http://repec.org. This archive has become the “official” place to store all “unofficial” programs.\nWhile not official, most SSC programs are ment to be well documented, hard tested, and maintained set or programs that are of general interest. However, what if you want to share programs that are do not meet all this requirements. Say, you are writting small rutines for your own work, as well as your collegues.\nFor those cases, there is net. This program is what operates behind ssc to bring forward, and allows you to install programs from the internet, and why not, your own repository.\nSo here is where today’s post is about. How to write a wrapper around net that would easily install other programs from your personal repository."
  },
  {
    "objectID": "chatgpt/fra_03_30_2023/index.html#structure-of-the-program",
    "href": "chatgpt/fra_03_30_2023/index.html#structure-of-the-program",
    "title": "How to write your own Installer",
    "section": "Structure of the program",
    "text": "Structure of the program\nTo write your wrapping program you need two things:\n\nname for the program. I find this the hardest part, as you want something simple, memorable, that will connect to your research in general. In my case, I just decided to call it fra\nthe address for your repository. If you are archeving your programs online (outside of ssc) you already have this. In my case, it will be my stpackage repository.\n\nWith these two elements lets write the wrapper:\nprogram fra\n    syntax anything, [all replace force]\n    local from \"https://friosavila.github.io/stpackages\"\n    tokenize `anything'\n\n    if \"`1'`2'\"==\"\"  net from `from' \n    else if !inlist(\"`1'\",\"describe\", \"install\", \"get\") {\n        display as error \"`1' invalid subcommand\"\n    }\n    else {\n        net `1' `2', `all' `replace' from(`from')\n    }\n    qui:net from http://www.stata.com/\nend\nAs you can see, this is a minimal program. It will only use 3 options:\n\nall : the all the package contents are downloaded (in addition to the ado and helpfiles)\nreplace: to replace the currently available program in your computer\nforce: To force any action.\n\nLine 3 is equally important, because is where your repository address will go. That way, users won’t have to modify “from” everytime they want to get your packages.\nLines 6-12 will do the installing for you (or packages description).\nLastly, to be nice, line 13 will restore net to the default addres: Stata\nThe last step, of course, is to save this file with the name fra.ado. Excecute it once, and start redistributing your packages.\nIf you are doing this I would suggest trying the following:\nfra install fra, replace\nThat way, it will store the information in the right place, and you wont have to worry about it anymore!"
  },
  {
    "objectID": "chatgpt/drop_03_25_2023/index.html",
    "href": "chatgpt/drop_03_25_2023/index.html",
    "title": "Drop 2.0: Small Stata Programming",
    "section": "",
    "text": "If you are an avid Stata user, who works extensively either using the command window, or dofiles, and who does lots of data-management, you are probably used to create large number of new variables, as well as delete an equally long lists of variables.\nIf you are like me, however, you probably get frustrated when the following happens:\n\nYou drop variables by mistake, because they start with the same name as the one you aim to delete.\nA drop statement gives you an error, because a variable in the varlist does not exist.\n\nYes, both scenarios are easy to fix.\n\nOne can avoid the first problem using set varabbrev off.\nThe second problem can be partially partially fixed using capture in front of drop.\n\nWell, today I bring you drop2. This small program should fix the problems above, with a very simple code:\nprogram drop2\n    novarabbrev {\n        syntax anything \n        foreach i in `anything' {\n            capture noisily fvexpand `i'            \n            if _rc == 0 {\n                drop `r(varlist)' \n            }\n        }\n    }\nend\nIt handles the first problem by forcing you to use full variable names, rather than abreviations. And it handles the second problem by going over your list of variables one by one, dropping only the ones in your dataset, giving you a warning if a variable does not exists.\nTo use this, you can run this code, create an ado file with the name drop2.ado, and save it in your personal ado folder1, or save it in the plus\\d folder. You can also download the file from here\nSo, just for fun, a quick example!. You can run the following code to test the command.\n\nsysuse auto, clear\ndescribe, simple\ndrop2 p pr mpg\ndescribe, simple\ndrop2 pr* mpg displacement\ndescribe, simple\nAnd it should produce the following ouput:\n. sysuse auto, clear\n(1978 automobile data)\n\n. describe, simple\nmake          mpg           headroom      weight        turn          gear_ratio\nprice         rep78         trunk         length        displacement  foreign\n\n. drop2 p pr mpg\nvariable p not found\nvariable pr not found\n\n. describe, simple\nmake          rep78         trunk         length        displacement  foreign\nprice         headroom      weight        turn          gear_ratio\n\n. drop2 pr* mpg displacement\nvariable mpg not found\n\n. describe, simple\nmake        rep78       headroom    trunk       weight      length      turn        gear_ratio  foreign"
  },
  {
    "objectID": "chatgpt/drop_03_25_2023/index.html#drop2-upgrade",
    "href": "chatgpt/drop_03_25_2023/index.html#drop2-upgrade",
    "title": "Drop 2.0: Small Stata Programming",
    "section": "",
    "text": "If you are an avid Stata user, who works extensively either using the command window, or dofiles, and who does lots of data-management, you are probably used to create large number of new variables, as well as delete an equally long lists of variables.\nIf you are like me, however, you probably get frustrated when the following happens:\n\nYou drop variables by mistake, because they start with the same name as the one you aim to delete.\nA drop statement gives you an error, because a variable in the varlist does not exist.\n\nYes, both scenarios are easy to fix.\n\nOne can avoid the first problem using set varabbrev off.\nThe second problem can be partially partially fixed using capture in front of drop.\n\nWell, today I bring you drop2. This small program should fix the problems above, with a very simple code:\nprogram drop2\n    novarabbrev {\n        syntax anything \n        foreach i in `anything' {\n            capture noisily fvexpand `i'            \n            if _rc == 0 {\n                drop `r(varlist)' \n            }\n        }\n    }\nend\nIt handles the first problem by forcing you to use full variable names, rather than abreviations. And it handles the second problem by going over your list of variables one by one, dropping only the ones in your dataset, giving you a warning if a variable does not exists.\nTo use this, you can run this code, create an ado file with the name drop2.ado, and save it in your personal ado folder1, or save it in the plus\\d folder. You can also download the file from here\nSo, just for fun, a quick example!. You can run the following code to test the command.\n\nsysuse auto, clear\ndescribe, simple\ndrop2 p pr mpg\ndescribe, simple\ndrop2 pr* mpg displacement\ndescribe, simple\nAnd it should produce the following ouput:\n. sysuse auto, clear\n(1978 automobile data)\n\n. describe, simple\nmake          mpg           headroom      weight        turn          gear_ratio\nprice         rep78         trunk         length        displacement  foreign\n\n. drop2 p pr mpg\nvariable p not found\nvariable pr not found\n\n. describe, simple\nmake          rep78         trunk         length        displacement  foreign\nprice         headroom      weight        turn          gear_ratio\n\n. drop2 pr* mpg displacement\nvariable mpg not found\n\n. describe, simple\nmake        rep78       headroom    trunk       weight      length      turn        gear_ratio  foreign"
  },
  {
    "objectID": "chatgpt/drop_03_25_2023/index.html#footnotes",
    "href": "chatgpt/drop_03_25_2023/index.html#footnotes",
    "title": "Drop 2.0: Small Stata Programming",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo see where your personal ado folder is, just type sysdir. ↩︎"
  },
  {
    "objectID": "app_metrics/stata2023.html#what-do-quantile-regressions-do",
    "href": "app_metrics/stata2023.html#what-do-quantile-regressions-do",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "What do Quantile Regressions do?",
    "text": "What do Quantile Regressions do?\n\nQuantile Regressions are an alternative to standard linear regressions that help us to better understand the relationship between the distribution of \\(Y\\) and \\(X's\\).\nIn constrast with Linear regression models, where one focuses on explaining \\(E(y|X)\\) as a function of \\(X\\), quantile regressions aim to assess the relationship between \\(Q_\\tau(y|X)\\) with respect to \\(X\\).\n\nThis means that to interpret Q-regressions properly, one needs to “condition” on \\(X\\) but also on \\(\\tau\\).\n\nQuantile Regressions are nonlinear-models, that “look” and can be interpreted as linear ones."
  },
  {
    "objectID": "app_metrics/stata2023.html#section",
    "href": "app_metrics/stata2023.html#section",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "",
    "text": "Number of observations (_N) was 0, now 250."
  },
  {
    "objectID": "app_metrics/stata2023.html#estimation",
    "href": "app_metrics/stata2023.html#estimation",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Estimation",
    "text": "Estimation\n\nA Generalization of Quantile regression data generating function can be written as follows:\n\n\\[y_i = \\beta_0(\\tau)+\\beta_1(\\tau) X_{i,1}+\\beta_2(\\tau) X_{i,2}+...+\\beta_k(\\tau) X_{i,k}\n\\]\n\nBase on this specification, few characteristics should be considered:\n\nSlopes \\(\\beta_k(\\tau)\\) will vary across quantiles only if the model is Heteroskedastic.\nQreg can be considered as a semi-parametric-varying coefficient model, with unobserved running variable (\\(\\tau\\)).\nCoefficients are percentile specific, but functional forms can be used for more efficient estimation.\nIt is possible to separate location effects (mean) from scale effect (deviation from the mean)"
  },
  {
    "objectID": "app_metrics/stata2023.html#section-1",
    "href": "app_metrics/stata2023.html#section-1",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "",
    "text": "Standard\n\\[\\beta(\\tau) \\leftarrow \\frac 1 n \\sum \\left[ I(x_i\\beta(\\tau) \\geq y_i) - \\tau \\right] =0\n\\]\nSemiparamatric (Kaplan (2022))\n\\[\\beta(\\tau) \\leftarrow \\frac 1 n \\sum \\left[ F\\left(\\frac{x_i\\beta(\\tau)-y_i}{bw} \\right) - \\tau \\right] =0\n\\]\nFunctional (Bottai and Orsini (2019))\n\\[\\beta(\\tau) = \\theta_0 + \\theta_1 \\tau + \\theta_2 \\tau^2 +...\n\\]\nLocation-Scale \\[\\beta(\\tau) = \\beta + \\gamma(\\tau)\n\\]"
  },
  {
    "objectID": "app_metrics/stata2023.html#section-2",
    "href": "app_metrics/stata2023.html#section-2",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "",
    "text": "qregsivqrLocation-Scale"
  },
  {
    "objectID": "app_metrics/stata2023.html#how-to-account-for-fixed-effects",
    "href": "app_metrics/stata2023.html#how-to-account-for-fixed-effects",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "How to account for fixed effects?",
    "text": "How to account for fixed effects?\n\nAs said before, quantile regresions are nonlinear models. Thus, one cannot just add Dummies to address group fixed effects\n\n\\[Q_\\tau(y|x) = \\beta_0(\\tau) + \\beta_1 X(\\tau) + \\sum \\delta(\\tau)_g\n\\]\nThis creates an incidental parameter problem. Neither \\(\\delta(\\tau)'s\\) nor \\(\\beta's\\) would be consistently estimated.\n\nBut then how to solve it?"
  },
  {
    "objectID": "app_metrics/stata2023.html#some-solutions",
    "href": "app_metrics/stata2023.html#some-solutions",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Some Solutions",
    "text": "Some Solutions\nKoenker (2004): Assume Fixed effects only have an impact on Location, and Shrink invidual effects (LASSO) \\[y_i = \\beta(\\tau)X + \\delta_g\n\\]\nCanay (2011): Similar to Koenker (2004), but “eliminate them” before running Qreg\n\\[\\begin{aligned}\ny = \\beta X + \\delta_g + e \\\\\nQ_\\tau(y - \\delta_g|X) = \\beta(\\tau)X\n\\end{aligned}\n\\]\nAbrevaya and Dahl (2008): Correlated Random effects Model \\[Q_\\tau(y|X) = \\beta(\\tau)X + \\gamma(\\tau) E(X|g)\\]"
  },
  {
    "objectID": "app_metrics/stata2023.html#mss2019",
    "href": "app_metrics/stata2023.html#mss2019",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Machado and Santos Silva (2019)",
    "text": "Machado and Santos Silva (2019)\n\nThis method was developed (and implemented using xtqreg) to incorporate individual fixed effects (Useful when considering panel data)\nIn principle its an extension of He (1997), who proposed an strategy to estimate Qreg coefficients using a restricted location-scale model, assuming the following structure: \\[y_i = X_i\\beta + \\varepsilon X_i\\gamma\n\\]\n\nThus, Quantile regression model is given by:\n\\[Q_\\tau(y|X)=X\\left(\\beta + F^{-1}_\\varepsilon(\\tau) \\gamma \\right) = X \\beta(\\tau)\n\\]\n\nInstead of requiring \\(K\\times M\\) coefficients to identify the whole conditional distribution, it only requires \\(2\\times K + M\\).\nAlso, Quantile curves will not cross!"
  },
  {
    "objectID": "app_metrics/stata2023.html#how-does-simplified-he2019-and-mss2019-works",
    "href": "app_metrics/stata2023.html#how-does-simplified-he2019-and-mss2019-works",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "How does Simplified He (1997) and Machado and Santos Silva (2019) works?",
    "text": "How does Simplified He (1997) and Machado and Santos Silva (2019) works?\n\\[y_i = x_i \\beta + \\varepsilon x_i \\gamma \\rightarrow Q_\\tau(y|X)=X\\beta + X\\gamma F_\\varepsilon^{-1}(\\tau)\n\\]\nStep 1: Location model\n\\[E(X'(y-X\\beta))=0\n\\]\nStep 2: Scale model\n\\[E(X'(|y-X\\beta|-X\\gamma))=0\n\\]\nStep 3: Standardized Quantile\n\\[E\\left[ I\\left( \\frac{y-X\\beta}{X\\gamma} \\geq q_\\tau \\right) -\\tau  \\right]=0\n\\]\nStep 4: Aggregate \\(\\beta(\\tau) = \\beta + q_\\tau \\gamma\\)"
  },
  {
    "objectID": "app_metrics/stata2023.html#extension-multiple-fixed-effects",
    "href": "app_metrics/stata2023.html#extension-multiple-fixed-effects",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Extension: Multiple Fixed effects",
    "text": "Extension: Multiple Fixed effects\nMachado and Santos Silva (2019) propose an extension: To add a single set of fixed effect (panel) apply Frisch–Waugh–Lovell theorem.\nThis can be extended to controlling for multiple fixed effects:\n\\[\\tilde w_i = w_i +\\bar w - \\delta^w_{g1} - \\delta^w_{g2} - ... - \\delta^w_{gk} \\ \\forall \\  w \\in y,x\n\\]\nAnd use almost the same moments as before:\n\\[\\begin{aligned}\nE(\\tilde X(\\tilde y-\\tilde X\\beta)) & =0 \\\\\nE(\\tilde X(|\\tilde y- \\tilde X\\beta|-\\tilde X\\gamma)) &=0 \\\\\nX\\gamma + \\delta's &= y - (\\tilde y- \\tilde X\\gamma) \\\\\nE\\left[ I\\left( \\frac{\\tilde y- \\tilde X\\beta}{X\\gamma+ \\delta's } \\geq q_\\tau \\right) -\\tau  \\right] &=0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "app_metrics/stata2023.html#extension-standard-errors",
    "href": "app_metrics/stata2023.html#extension-standard-errors",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Extension: Standard Errors",
    "text": "Extension: Standard Errors\n\nAlong with the modified algorithm that allows for Panel Fixed effects, Machado and Santos Silva (2019) proposed a GLS type variance estimator.\n\nThe model assumes the existance of Heteroskedasticity.\nBased on Law of Iterated Expectations, and assuming Heteroskedasticity is correctly specified, its possible to construct a GLS type Variance Estimator.\n\nHow does this work?"
  },
  {
    "objectID": "app_metrics/stata2023.html#location-model",
    "href": "app_metrics/stata2023.html#location-model",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Location Model",
    "text": "Location Model\n\nConsider the location model: \\(y_i = x_i \\beta +  \\varepsilon_i x_i \\gamma\\)\nWhich has known heteroskedasticity: \\(Var(y_i - x_i\\beta)=(x_i\\gamma)^2 \\sigma^2_\\varepsilon\\)\nMachado and Santos Silva (2019) GLS Variance would be: \\[\\begin{aligned}\ne &= y_i - X\\beta = \\varepsilon_i x_i \\gamma\\\\\nVar(\\hat\\beta) &= (X'X)^{-1}\\sum x_i x_i' \\color{red}{e_i^2} (X'X)^{-1} \\\\\n&=\\color{red}{\\sigma_\\varepsilon^2} (X'X)^{-1} \\sum x_i x_i' \\color{red}{(x_i'\\gamma)^2} (X'X)^{-1}\n\\end{aligned}\n\\]\n\nHowever, this can be sensitive to the modeling of the Scale function.\nBut one can almost always Bootstrap…"
  },
  {
    "objectID": "app_metrics/stata2023.html#extension-robust-and-clustered-se",
    "href": "app_metrics/stata2023.html#extension-robust-and-clustered-se",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Extension: Robust and Clustered SE",
    "text": "Extension: Robust and Clustered SE\nAn extension offered in my implementation makes full use of the moment conditions, and the implicit Influence functions: \\[\\begin{aligned}\nIF'_{i,\\beta} &= N(X'X)^{-1} x_i R_i    \\\\\nIF'_{i,\\gamma} &= N(X'X)^{-1} x_i \\Big( \\tilde R_i  -x_i'\\gamma \\Big) \\\\\n\\color{blue}{IF_{i, q(\\tau)}} &= \\frac{\\tau -I(q_\\tau \\geq \\varepsilon_i) }{f_\\varepsilon(q_\\tau)}\n- \\frac{R_i}{\\bar x'\\gamma}\n- q_\\tau \\frac{\\tilde R_i -x_i'\\gamma}{\\bar x'\\gamma}\\\\\nR_i &= y_i-x_i'\\beta \\\\\n\\tilde R_i & =2 R_i \\big[1(R_i \\geq 0) - E(1(R_i \\geq 0)) \\big]\n\\end{aligned}\n\\]\nSo we just stack Influence functions to obtain Robust and Clustered Standard errors:"
  },
  {
    "objectID": "app_metrics/stata2023.html#section-3",
    "href": "app_metrics/stata2023.html#section-3",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "",
    "text": "Robust SE \\[Var_r(\\beta,\\gamma,q_\\tau) = \\frac{1}{N^2}\n\\left[ \\begin{matrix}\nIF_\\beta & IF_\\gamma & IF_{q_\\tau}\n\\end{matrix}\n\\right]'\\left[ \\begin{matrix}\nIF_\\beta & IF_\\gamma & IF_{q_\\tau}\n\\end{matrix}\n\\right]\n\\]\nCluster SE\n\\[Var_c(\\beta,\\gamma,q_\\tau) = \\frac{1}{N^2}\n\\left[ \\begin{matrix}\nSIF_\\beta & SIF_\\gamma & SIF_{q_\\tau}\n\\end{matrix}\n\\right]'\\left[ \\begin{matrix}\nSIF_\\beta & SIF_\\gamma & SIF_{q_\\tau}\n\\end{matrix}\n\\right]\n\\] \\[SIF_{w} = \\left[ \\begin{matrix}\nSIF_{g=1,w} \\\\\nSIF_{g=2,w}  \\\\ ... \\\\\\\nSIF_{g=G,w}  \n\\end{matrix}\\right] \\& SIF_{g=k, \\ w} = \\sum_{i \\in g=k} IF_{i,w}\n\\]\nEasily allows for simultaneous Qregressions and weights"
  },
  {
    "objectID": "app_metrics/stata2023.html#command-syntax-mmqreg",
    "href": "app_metrics/stata2023.html#command-syntax-mmqreg",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Command syntax mmqreg",
    "text": "Command syntax mmqreg\nmmqreg depvar indepvar [if in] [pw] , [Options] &lt;-- Standard Syntax\n\n** Variance Options\n   Default: MSS(2019) GLS SE\n  -robust-: Robust Standard errors, no DOF Correction (GMM)\n  -cluster(varname)-: Clustered Standard errors, no DOF Correction (GMM)\n  -dfadj-: Simple Adjustment (n-k-1)\n\n** Output\n  - q(numlist): List of Quantiles to be estimated (default q=50)\n  - ls        : Request Providing Location Scale coefficient models\nThere will be some differences with xtqreg because of couple of differences in the math and programming."
  },
  {
    "objectID": "app_metrics/stata2023.html#example",
    "href": "app_metrics/stata2023.html#example",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Example",
    "text": "Example\n\nwebuse nlswork, clear\nqui:mmqreg ln_w age ttl_exp tenure not_smsa south, ///\n    abs(idcode) \nest sto m1\nqui:mmqreg ln_w age ttl_exp tenure not_smsa south, ///\n    abs(idcode) robust\nest sto m2\nqui:mmqreg ln_w age ttl_exp tenure not_smsa south, ///\n    abs(idcode) cluster(idcode)\nest sto m3\nqui:mmqreg ln_w ttl_exp tenure not_smsa south, ///\n    abs(idcode age) cluster(idcode)\nest sto m4\nesttab m1 m2 m3 m4, se  nomtitle nogap \n\n(National Longitudinal Survey of Young Women, 14-24 years old in 1968)\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n----------------------------------------------------------------------------\nqtile                                                                       \nage              -0.00270**      -0.00270**      -0.00270*                  \n               (0.000868)      (0.000890)       (0.00127)                   \nttl_exp            0.0292***       0.0292***       0.0292***       0.0341***\n                (0.00178)       (0.00153)       (0.00225)       (0.00229)   \ntenure             0.0108***       0.0108***       0.0108***       0.0102***\n                (0.00191)      (0.000989)       (0.00145)       (0.00144)   \nnot_smsa          -0.0925***      -0.0925***      -0.0925***      -0.0880***\n                (0.00974)        (0.0104)        (0.0140)        (0.0138)   \nsouth             -0.0640***      -0.0640***      -0.0640***      -0.0599***\n                 (0.0116)        (0.0121)        (0.0167)        (0.0167)   \n_cons               1.611***        1.611***        1.611***        1.490***\n                 (0.0564)        (0.0194)        (0.0276)        (0.0146)   \n----------------------------------------------------------------------------\nN                   28093           28093           28093           28093   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "app_metrics/stata2023.html#section-4",
    "href": "app_metrics/stata2023.html#section-4",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "",
    "text": "sysuse auto, clear\nqui:mmqreg price mpg trunk  , \nest sto m1\nqui:mmqreg price mpg trunk  , robust\nest sto m2\nqui:bootstrap, reps(250):mmqreg price mpg trunk\nest sto m3\n\n(1978 automobile data)\n\n\n\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                  default          Robust       Bootstrap   \n------------------------------------------------------------\nqtile                                                       \nmpg                -172.7          -172.7***       -172.7*  \n                (38416.1)         (44.85)         (71.63)   \ntrunk               33.67           33.67           33.67   \n                (50437.7)         (49.04)         (56.11)   \n_cons              8457.3          8457.3***       8457.3***\n              (1454004.1)        (1617.9)        (2303.5)   \n------------------------------------------------------------\nN                      74              74              74   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "app_metrics/stata2023.html#plotting-qreg-vs-mmqreg",
    "href": "app_metrics/stata2023.html#plotting-qreg-vs-mmqreg",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Plotting: qreg vs mmqreg",
    "text": "Plotting: qreg vs mmqreg\n\nqregmmqreg"
  },
  {
    "objectID": "app_metrics/stata2023.html#conclusions",
    "href": "app_metrics/stata2023.html#conclusions",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Conclusions",
    "text": "Conclusions\n\nIn this presentation I present a quick review of quantile regressions, with emphasis on solutions for adding fixed effects.\nThe Location and Scale model help with the problem because it reduces the number of coefficients to needed to be estimated for consistent estimates.\n\nIn MSS2019, inconsistencies are still observed with Small T. (get long data)\nIn my context, this may be less biding, because is more likely to have many obs per Fixed characteristic.\n\nWhile the model may be restrictive (multiplicative heteroskedasticity) it may still be useful for exploration, and analysis when other methodologies are not feasible."
  },
  {
    "objectID": "app_metrics/stata2023.html#references",
    "href": "app_metrics/stata2023.html#references",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "References",
    "text": "References\n\n\nAbrevaya, Jason, and Christian M Dahl. 2008. “The Effects of Birth Inputs on Birthweight.” Journal of Business & Economic Statistics 26 (4): 379–97. https://doi.org/10.1198/073500107000000269.\n\n\nBottai, Matteo, and Nicola Orsini. 2019. “Qmodel: A Command for Fitting Parametric Quantile Models.” The Stata Journal 19 (2): 261–93. https://doi.org/10.1177/1536867X19854002.\n\n\nCanay, Ivan A. 2011. “A Simple Approach to Quantile Regression for Panel Data.” The Econometrics Journal 14 (3): 368–86. https://doi.org/https://doi.org/10.1111/j.1368-423X.2011.00349.x.\n\n\nHe, Xuming. 1997. “Quantile Curves Without Crossing.” The American Statistician 51 (2): 186–92. https://doi.org/10.1080/00031305.1997.10473959.\n\n\nKaplan, David M. 2022. “Smoothed Instrumental Variables Quantile Regression.” The Stata Journal 22 (2): 379–403. https://doi.org/10.1177/1536867X221106404.\n\n\nKoenker, Roger. 2004. “Quantile Regression for Longitudinal Data.” Journal of Multivariate Analysis 91 (1): 74–89. https://doi.org/https://doi.org/10.1016/j.jmva.2004.05.006.\n\n\nMachado, José A. F., and J. M. C. Santos Silva. 2019. “Quantiles via Moments.” Journal of Econometrics 213 (1): 145–73. https://doi.org/https://doi.org/10.1016/j.jeconom.2019.04.009."
  },
  {
    "objectID": "app_metrics/stata2023.html#short-intro",
    "href": "app_metrics/stata2023.html#short-intro",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Short intro",
    "text": "Short intro\n\nCorrelated Random Effects model can also be applied for the estimation of Q-regressions with large set of fixed effects.\n\nIn Linear models, point estimates are identical to panel FE (or Dummy inclusion appraoch)\n\nEstimating Standard errors, however, can be tricky.\n\nIf only 1 set of FE is used, clustered SE by the Fixed effects will give identical results. But equivalances dissapear with 2 or more sets of fixed effects\n\nMay not work with Qreg if FE are not a function of conditional means of all other explanatory variables. (compound error will not be related to \\(\\tau\\) of the original ranking)\nBut may still be a good approximation !"
  },
  {
    "objectID": "app_metrics/stata2023.html#cre-for-correlated-random-effects-model",
    "href": "app_metrics/stata2023.html#cre-for-correlated-random-effects-model",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "cre: For Correlated Random Effects Model",
    "text": "cre: For Correlated Random Effects Model\nfrom -fra install-\nsyntax: \n  cre, abs(varlist) [keep compact]: [regression command]\n- a prefix command. Gathers all controls and obtains conditional means\n  based on FE groups\n- Adds them to model specification, and estimates the model.\n- SE should be accounted for in the command.\n\n** Example\ncre, abs(idcode):qreg ln_w age ttl_exp tenure not_smsa south, ///\n    cluster(idcode)"
  },
  {
    "objectID": "app_metrics/stata2023.html#comparing-methods",
    "href": "app_metrics/stata2023.html#comparing-methods",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Comparing Methods",
    "text": "Comparing Methods\n\n\nCode\nclear\nset seed 101\nset obs 5000\ngen g = runiformint(1,250)\ngen rnd1 = rchi2(4)/4\ngen rnd2 = rchi2(4)/4\ngen rnd3 = rchi2(4)/4 - 1\n\nbysort g:gen fe = rnd1[1]\nbysort g:gen fe2 = rnd1[2] + fe\nbysort g:gen x1  =2 - fe + rchi2(4)/4\nbysort g:gen x2  = fe + rnd2[1] \n\ngen tau = runiform()\n\ngen y = 2+ tau + log(tau) + tau^2 * x1 - log(tau) * x2 + 2*tau*fe2\n\nqreg y x1 x2 fe2\nqregplot x1, name(m1, replace) title(Benchmark) graphregion( margin(r=5)) estore(ss)\nest restore ss\nmatrix qq=e(qq)\nmatrix bb=e(bs)\nmatrix ll=e(ll)\nmatrix uu=e(ul)\nsvmat qq\nsvmat bb\nsvmat ll\nsvmat uu\ncre , abs(g) keep: qreg y x1 x2 \nqregplot x1, name(m2, replace) title(CRE) graphregion( margin(r=5))\naddplot : rarea ll1 uu1 qq1, color(%30) lcolor(%0) pstyle(p4) || line bb1 qq1 , color(%40)  pstyle(p4)\n\nmmqreg y x1 x2, abs(g)\nqregplot x1, name(m3, replace) title(mmqreg) graphregion( margin(r=5))\naddplot : rarea ll1 uu1 qq1, color(%30) lcolor(%0) pstyle(p4) || line bb1 qq1 , color(%40)  pstyle(p4)\n \n reghdfe y x1 x2, abs(fex=g)\ngen y2 = y-fex\nqreg y2 x1 x2\nqregplot x1, name(m4, replace) title(Canay2011) graphregion( margin(r=5))\naddplot : rarea ll1 uu1 qq1, color(%30) lcolor(%0) pstyle(p4) || line bb1 qq1 , color(%40)  pstyle(p4)\n\nqreg y x1 x2 fex\nqregplot x1, name(m5, replace) title(M-Canay2011) graphregion( margin(r=5))\naddplot : rarea ll1 uu1 qq1, color(%30) lcolor(%0) pstyle(p4) || line bb1 qq1 , color(%40)  pstyle(p4)\n\nqreg y x1 x2\nqregplot x1, name(m6, replace) title(qreg no FE) graphregion( margin(r=5))\naddplot : rarea ll1 uu1 qq1, color(%30) lcolor(%0) pstyle(p4) || line bb1 qq1 , color(%40)  pstyle(p4)\n\ngraph combine m1 m2 m3 m4 m5 m6, col(3) ycommon nocopies \n\ngraph export fig9.png, width(2000) replace"
  },
  {
    "objectID": "app_metrics/app_metrics9.html",
    "href": "app_metrics/app_metrics9.html",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "",
    "text": "Quantile Regressions are an alternative to standard linear regressions that help us to better understand the relationship between the distribution of \\(Y\\) and \\(X's\\).\nIn constrast with Linear regression models, where one focuses on explaining \\(E(y|X)\\) as a function of \\(X\\), quantile regressions aim to assess the relationship between \\(Q_\\tau(y|X)\\) with respect to \\(X\\).\n\nThis means that to interpret Q-regressions properly, one needs to “condition” on \\(X\\) but also on \\(\\tau\\).\n\nQuantile Regressions are nonlinear-models, that “look” and can be interpreted as linear ones.\n\n\n\n\n\n\n\n\n\nNumber of observations (_N) was 0, now 250.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Generalization of Quantile regression data generating function can be written as follows:\n\n\\[y_i = \\beta_0(\\tau)+\\beta_1(\\tau) X_{i,1}+\\beta_2(\\tau) X_{i,2}+...+\\beta_k(\\tau) X_{i,k}\n\\]\n\nBase on this specification, few characteristics should be considered:\n\nSlopes \\(\\beta_k(\\tau)\\) will vary across quantiles only if the model is Heteroskedastic.\nQreg can be considered as a semi-parametric-varying coefficient model, with unobserved running variable (\\(\\tau\\)).\nCoefficients are percentile specific, but functional forms can be used for more efficient estimation.\nIt is possible to separate location effects (mean) from scale effect (deviation from the mean)\n\n\n\n\n\nStandard\n\\[\\beta(\\tau) \\leftarrow \\frac 1 n \\sum \\left[ I(x_i\\beta(\\tau) \\geq y_i) - \\tau \\right] =0\n\\]\nSemiparamatric (Kaplan (2022))\n\\[\\beta(\\tau) \\leftarrow \\frac 1 n \\sum \\left[ F\\left(\\frac{x_i\\beta(\\tau)-y_i}{bw} \\right) - \\tau \\right] =0\n\\]\nFunctional (Bottai and Orsini (2019))\n\\[\\beta(\\tau) = \\theta_0 + \\theta_1 \\tau + \\theta_2 \\tau^2 +...\n\\]\nLocation-Scale \\[\\beta(\\tau) = \\beta + \\gamma(\\tau)\n\\]\n\n\n\n\nqregsivqrLocation-Scale",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#what-do-quantile-regressions-do",
    "href": "app_metrics/app_metrics9.html#what-do-quantile-regressions-do",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "",
    "text": "Quantile Regressions are an alternative to standard linear regressions that help us to better understand the relationship between the distribution of \\(Y\\) and \\(X's\\).\nIn constrast with Linear regression models, where one focuses on explaining \\(E(y|X)\\) as a function of \\(X\\), quantile regressions aim to assess the relationship between \\(Q_\\tau(y|X)\\) with respect to \\(X\\).\n\nThis means that to interpret Q-regressions properly, one needs to “condition” on \\(X\\) but also on \\(\\tau\\).\n\nQuantile Regressions are nonlinear-models, that “look” and can be interpreted as linear ones.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#section",
    "href": "app_metrics/app_metrics9.html#section",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "",
    "text": "Number of observations (_N) was 0, now 250.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#estimation",
    "href": "app_metrics/app_metrics9.html#estimation",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "",
    "text": "A Generalization of Quantile regression data generating function can be written as follows:\n\n\\[y_i = \\beta_0(\\tau)+\\beta_1(\\tau) X_{i,1}+\\beta_2(\\tau) X_{i,2}+...+\\beta_k(\\tau) X_{i,k}\n\\]\n\nBase on this specification, few characteristics should be considered:\n\nSlopes \\(\\beta_k(\\tau)\\) will vary across quantiles only if the model is Heteroskedastic.\nQreg can be considered as a semi-parametric-varying coefficient model, with unobserved running variable (\\(\\tau\\)).\nCoefficients are percentile specific, but functional forms can be used for more efficient estimation.\nIt is possible to separate location effects (mean) from scale effect (deviation from the mean)",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#section-1",
    "href": "app_metrics/app_metrics9.html#section-1",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "",
    "text": "Standard\n\\[\\beta(\\tau) \\leftarrow \\frac 1 n \\sum \\left[ I(x_i\\beta(\\tau) \\geq y_i) - \\tau \\right] =0\n\\]\nSemiparamatric (Kaplan (2022))\n\\[\\beta(\\tau) \\leftarrow \\frac 1 n \\sum \\left[ F\\left(\\frac{x_i\\beta(\\tau)-y_i}{bw} \\right) - \\tau \\right] =0\n\\]\nFunctional (Bottai and Orsini (2019))\n\\[\\beta(\\tau) = \\theta_0 + \\theta_1 \\tau + \\theta_2 \\tau^2 +...\n\\]\nLocation-Scale \\[\\beta(\\tau) = \\beta + \\gamma(\\tau)\n\\]",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#section-2",
    "href": "app_metrics/app_metrics9.html#section-2",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "",
    "text": "qregsivqrLocation-Scale",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#how-to-account-for-fixed-effects",
    "href": "app_metrics/app_metrics9.html#how-to-account-for-fixed-effects",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "How to account for fixed effects?",
    "text": "How to account for fixed effects?\n\nAs said before, quantile regresions are nonlinear models. Thus, one cannot just add Dummies to address group fixed effects\n\n\\[Q_\\tau(y|x) = \\beta_0(\\tau) + \\beta_1 X(\\tau) + \\sum \\delta(\\tau)_g\n\\]\nThis creates an incidental parameter problem. Neither \\(\\delta(\\tau)'s\\) nor \\(\\beta's\\) would be consistently estimated.\n\nBut then how to solve it?",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#some-solutions",
    "href": "app_metrics/app_metrics9.html#some-solutions",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Some Solutions",
    "text": "Some Solutions\nKoenker (2004): Assume Fixed effects only have an impact on Location, and Shrink invidual effects (LASSO) \\[y_i = \\beta(\\tau)X + \\delta_g\n\\]\nCanay (2011): Similar to Koenker (2004), but “eliminate them” before running Qreg\n\\[\\begin{aligned}\ny = \\beta X + \\delta_g + e \\\\\nQ_\\tau(y - \\delta_g|X) = \\beta(\\tau)X\n\\end{aligned}\n\\]\nAbrevaya and Dahl (2008): Correlated Random effects Model \\[Q_\\tau(y|X) = \\beta(\\tau)X + \\gamma(\\tau) E(X|g)\\]",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#mss2019",
    "href": "app_metrics/app_metrics9.html#mss2019",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Machado and Santos Silva (2019)",
    "text": "Machado and Santos Silva (2019)\n\nThis method was developed (and implemented using xtqreg) to incorporate individual fixed effects (Useful when considering panel data)\nIn principle its an extension of He (1997), who proposed an strategy to estimate Qreg coefficients using a restricted location-scale model, assuming the following structure: \\[y_i = X_i\\beta + \\varepsilon X_i\\gamma\n\\]\n\nThus, Quantile regression model is given by:\n\\[Q_\\tau(y|X)=X\\left(\\beta + F^{-1}_\\varepsilon(\\tau) \\gamma \\right) = X \\beta(\\tau)\n\\]\n\nInstead of requiring \\(K\\times M\\) coefficients to identify the whole conditional distribution, it only requires \\(2\\times K + M\\).\nAlso, Quantile curves will not cross!",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#how-does-simplified-he2019-and-mss2019-works",
    "href": "app_metrics/app_metrics9.html#how-does-simplified-he2019-and-mss2019-works",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "How does Simplified He (1997) and Machado and Santos Silva (2019) works?",
    "text": "How does Simplified He (1997) and Machado and Santos Silva (2019) works?\n\\[y_i = x_i \\beta + \\varepsilon x_i \\gamma \\rightarrow Q_\\tau(y|X)=X\\beta + X\\gamma F_\\varepsilon^{-1}(\\tau)\n\\]\n\nStep 1: Location model \\(E(X'(y-X\\beta))=0\\)\n\n\nStep 2: Scale model \\(E(X'(|y-X\\beta|-X\\gamma))=0\\)\n\n\nStep 3: Standardized Quantile \\(E\\left[ I\\left( \\frac{y-X\\beta}{X\\gamma} \\geq q_\\tau \\right) -\\tau  \\right]=0\\)\n\n\nStep 4: Aggregate \\(\\beta(\\tau) = \\beta + q_\\tau \\gamma\\)",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#extension-multiple-fixed-effects",
    "href": "app_metrics/app_metrics9.html#extension-multiple-fixed-effects",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Extension: Multiple Fixed effects",
    "text": "Extension: Multiple Fixed effects\nMachado and Santos Silva (2019) propose an extension: To add a single set of fixed effect (panel) apply Frisch–Waugh–Lovell theorem.\nThis can be extended to controlling for multiple fixed effects:\n\\[\\tilde w_i = w_i +\\bar w - \\delta^w_{g1} - \\delta^w_{g2} - ... - \\delta^w_{gk} \\ \\forall \\  w \\in y,x\n\\]\nAnd use almost the same moments as before:\n\\[\\begin{aligned}\nE(\\tilde X(\\tilde y-\\tilde X\\beta)) & =0 \\\\\nE(\\tilde X(|\\tilde y- \\tilde X\\beta|-\\tilde X\\gamma)) &=0 \\\\\nX\\gamma + \\delta's &= y - (\\tilde y- \\tilde X\\gamma) \\\\\nE\\left[ I\\left( \\frac{\\tilde y- \\tilde X\\beta}{X\\gamma+ \\delta's } \\geq q_\\tau \\right) -\\tau  \\right] &=0\n\\end{aligned}\n\\]",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#extension-standard-errors",
    "href": "app_metrics/app_metrics9.html#extension-standard-errors",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Extension: Standard Errors",
    "text": "Extension: Standard Errors\n\nAlong with the modified algorithm that allows for Panel Fixed effects, Machado and Santos Silva (2019) proposed a GLS type variance estimator.\n\nThe model assumes the existance of Heteroskedasticity.\nBased on Law of Iterated Expectations, and assuming Heteroskedasticity is correctly specified, its possible to construct a GLS type Variance Estimator.\n\nHow does this work?",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#location-model",
    "href": "app_metrics/app_metrics9.html#location-model",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Location Model",
    "text": "Location Model\n\nConsider the location model: \\(y_i = x_i \\beta +  \\varepsilon_i x_i \\gamma\\)\nWhich has known heteroskedasticity: \\(Var(y_i - x_i\\beta)=(x_i\\gamma)^2 \\sigma^2_\\varepsilon\\)\nMachado and Santos Silva (2019) GLS Variance would be: \\[\\begin{aligned}\ne &= y_i - X\\beta = \\varepsilon_i x_i \\gamma\\\\\nVar(\\hat\\beta) &= (X'X)^{-1}\\sum x_i x_i' \\color{red}{e_i^2} (X'X)^{-1} \\\\\n&=\\color{red}{\\sigma_\\varepsilon^2} (X'X)^{-1} \\sum x_i x_i' \\color{red}{(x_i'\\gamma)^2} (X'X)^{-1}\n\\end{aligned}\n\\]\n\nHowever, this can be sensitive to the modeling of the Scale function.\nBut one can almost always Bootstrap…",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#extension-robust-and-clustered-se",
    "href": "app_metrics/app_metrics9.html#extension-robust-and-clustered-se",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Extension: Robust and Clustered SE",
    "text": "Extension: Robust and Clustered SE\nAn extension offered in my implementation makes full use of the moment conditions, and the implicit Influence functions: \\[\\begin{aligned}\nIF'_{i,\\beta} &= N(X'X)^{-1} x_i R_i    \\\\\nIF'_{i,\\gamma} &= N(X'X)^{-1} x_i \\Big( \\tilde R_i  -x_i'\\gamma \\Big) \\\\\n\\color{blue}{IF_{i, q(\\tau)}} &= \\frac{\\tau -I(q_\\tau \\geq \\varepsilon_i) }{f_\\varepsilon(q_\\tau)}\n- \\frac{R_i}{\\bar x'\\gamma}\n- q_\\tau \\frac{\\tilde R_i -x_i'\\gamma}{\\bar x'\\gamma}\\\\\nR_i &= y_i-x_i'\\beta \\\\\n\\tilde R_i & =2 R_i \\big[1(R_i \\geq 0) - E(1(R_i \\geq 0)) \\big]\n\\end{aligned}\n\\]\nSo we just stack Influence functions to obtain Robust and Clustered Standard errors:",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#section-3",
    "href": "app_metrics/app_metrics9.html#section-3",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "",
    "text": "Robust SE \\[Var_r(\\beta,\\gamma,q_\\tau) = \\frac{1}{N^2}\n\\left[ \\begin{matrix}\nIF_\\beta & IF_\\gamma & IF_{q_\\tau}\n\\end{matrix}\n\\right]'\\left[ \\begin{matrix}\nIF_\\beta & IF_\\gamma & IF_{q_\\tau}\n\\end{matrix}\n\\right]\n\\]\nCluster SE\n\\[Var_c(\\beta,\\gamma,q_\\tau) = \\frac{1}{N^2}\n\\left[ \\begin{matrix}\nSIF_\\beta & SIF_\\gamma & SIF_{q_\\tau}\n\\end{matrix}\n\\right]'\\left[ \\begin{matrix}\nSIF_\\beta & SIF_\\gamma & SIF_{q_\\tau}\n\\end{matrix}\n\\right]\n\\] \\[SIF_{w} = \\left[ \\begin{matrix}\nSIF_{g=1,w} \\\\\nSIF_{g=2,w}  \\\\ ... \\\\\\\nSIF_{g=G,w}  \n\\end{matrix}\\right] \\& SIF_{g=k, \\ w} = \\sum_{i \\in g=k} IF_{i,w}\n\\]\nEasily allows for simultaneous Qregressions and weights",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#command-syntax-mmqreg",
    "href": "app_metrics/app_metrics9.html#command-syntax-mmqreg",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Command syntax mmqreg",
    "text": "Command syntax mmqreg\nmmqreg depvar indepvar [if in] [pw] , [Options] &lt;-- Standard Syntax\n\n** Variance Options\n   Default: MSS(2019) GLS SE\n  -robust-: Robust Standard errors, no DOF Correction (GMM)\n  -cluster(varname)-: Clustered Standard errors, no DOF Correction (GMM)\n  -dfadj-: Simple Adjustment (n-k-1)\n\n** Output\n  - q(numlist): List of Quantiles to be estimated (default q=50)\n  - ls        : Request Providing Location Scale coefficient models\nThere will be some differences with xtqreg because of couple of differences in the math and programming.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#example",
    "href": "app_metrics/app_metrics9.html#example",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Example",
    "text": "Example\n\nwebuse nlswork, clear\nqui:mmqreg ln_w age ttl_exp tenure not_smsa south, ///\n    abs(idcode) \nest sto m1\nqui:mmqreg ln_w age ttl_exp tenure not_smsa south, ///\n    abs(idcode) robust\nest sto m2\nqui:mmqreg ln_w age ttl_exp tenure not_smsa south, ///\n    abs(idcode) cluster(idcode)\nest sto m3\nqui:mmqreg ln_w ttl_exp tenure not_smsa south, ///\n    abs(idcode age) cluster(idcode)\nest sto m4\nesttab m1 m2 m3 m4, se  nomtitle nogap \n\n(National Longitudinal Survey of Young Women, 14-24 years old in 1968)\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n----------------------------------------------------------------------------\nqtile                                                                       \nage              -0.00270**      -0.00270**      -0.00270*                  \n               (0.000868)      (0.000890)       (0.00127)                   \nttl_exp            0.0292***       0.0292***       0.0292***       0.0341***\n                (0.00178)       (0.00153)       (0.00225)       (0.00229)   \ntenure             0.0108***       0.0108***       0.0108***       0.0102***\n                (0.00191)      (0.000989)       (0.00145)       (0.00144)   \nnot_smsa          -0.0925***      -0.0925***      -0.0925***      -0.0880***\n                (0.00974)        (0.0104)        (0.0140)        (0.0138)   \nsouth             -0.0640***      -0.0640***      -0.0640***      -0.0599***\n                 (0.0116)        (0.0121)        (0.0167)        (0.0167)   \n_cons               1.611***        1.611***        1.611***        1.490***\n                 (0.0564)        (0.0194)        (0.0276)        (0.0146)   \n----------------------------------------------------------------------------\nN                   28093           28093           28093           28093   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#section-4",
    "href": "app_metrics/app_metrics9.html#section-4",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "",
    "text": "sysuse auto, clear\nqui:mmqreg price mpg trunk  , \nest sto m1\nqui:mmqreg price mpg trunk  , robust\nest sto m2\nqui:bootstrap, reps(250):mmqreg price mpg trunk\nest sto m3\n\n(1978 automobile data)\n\n\n\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                  default          Robust       Bootstrap   \n------------------------------------------------------------\nqtile                                                       \nmpg                -172.7          -172.7***       -172.7*  \n                (38416.1)         (44.85)         (71.63)   \ntrunk               33.67           33.67           33.67   \n                (50437.7)         (49.04)         (56.11)   \n_cons              8457.3          8457.3***       8457.3***\n              (1454004.1)        (1617.9)        (2303.5)   \n------------------------------------------------------------\nN                      74              74              74   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#plotting-qreg-vs-mmqreg",
    "href": "app_metrics/app_metrics9.html#plotting-qreg-vs-mmqreg",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Plotting: qreg vs mmqreg",
    "text": "Plotting: qreg vs mmqreg\n\nqregmmqreg",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#conclusions",
    "href": "app_metrics/app_metrics9.html#conclusions",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Conclusions",
    "text": "Conclusions\n\nIn this presentation I present a quick review of quantile regressions, with emphasis on solutions for adding fixed effects.\nThe Location and Scale model help with the problem because it reduces the number of coefficients to needed to be estimated for consistent estimates.\n\nIn MSS2019, inconsistencies are still observed with Small T. (get long data)\nIn my context, this may be less biding, because is more likely to have many obs per Fixed characteristic.\n\nWhile the model may be restrictive (multiplicative heteroskedasticity) it may still be useful for exploration, and analysis when other methodologies are not feasible.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#references",
    "href": "app_metrics/app_metrics9.html#references",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "References",
    "text": "References\n\n\nAbrevaya, Jason, and Christian M Dahl. 2008. “The Effects of Birth Inputs on Birthweight.” Journal of Business & Economic Statistics 26 (4): 379–97. https://doi.org/10.1198/073500107000000269.\n\n\nBottai, Matteo, and Nicola Orsini. 2019. “Qmodel: A Command for Fitting Parametric Quantile Models.” The Stata Journal 19 (2): 261–93. https://doi.org/10.1177/1536867X19854002.\n\n\nCanay, Ivan A. 2011. “A Simple Approach to Quantile Regression for Panel Data.” The Econometrics Journal 14 (3): 368–86. https://doi.org/https://doi.org/10.1111/j.1368-423X.2011.00349.x.\n\n\nHe, Xuming. 1997. “Quantile Curves Without Crossing.” The American Statistician 51 (2): 186–92. https://doi.org/10.1080/00031305.1997.10473959.\n\n\nKaplan, David M. 2022. “Smoothed Instrumental Variables Quantile Regression.” The Stata Journal 22 (2): 379–403. https://doi.org/10.1177/1536867X221106404.\n\n\nKoenker, Roger. 2004. “Quantile Regression for Longitudinal Data.” Journal of Multivariate Analysis 91 (1): 74–89. https://doi.org/https://doi.org/10.1016/j.jmva.2004.05.006.\n\n\nMachado, José A. F., and J. M. C. Santos Silva. 2019. “Quantiles via Moments.” Journal of Econometrics 213 (1): 145–73. https://doi.org/https://doi.org/10.1016/j.jeconom.2019.04.009.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#short-intro",
    "href": "app_metrics/app_metrics9.html#short-intro",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Short intro",
    "text": "Short intro\n\nCorrelated Random Effects model can also be applied for the estimation of Q-regressions with large set of fixed effects.\n\nIn Linear models, point estimates are identical to panel FE (or Dummy inclusion appraoch)\n\nEstimating Standard errors, however, can be tricky.\n\nIf only 1 set of FE is used, clustered SE by the Fixed effects will give identical results. But equivalances dissapear with 2 or more sets of fixed effects\n\nMay not work with Qreg if FE are not a function of conditional means of all other explanatory variables. (compound error will not be related to \\(\\tau\\) of the original ranking)\nBut may still be a good approximation !",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#cre-for-correlated-random-effects-model",
    "href": "app_metrics/app_metrics9.html#cre-for-correlated-random-effects-model",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "cre: For Correlated Random Effects Model",
    "text": "cre: For Correlated Random Effects Model\nfrom -fra install-\nsyntax: \n  cre, abs(varlist) [keep compact]: [regression command]\n- a prefix command. Gathers all controls and obtains conditional means\n  based on FE groups\n- Adds them to model specification, and estimates the model.\n- SE should be accounted for in the command.\n\n** Example\ncre, abs(idcode):qreg ln_w age ttl_exp tenure not_smsa south, ///\n    cluster(idcode)",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics9.html#comparing-methods",
    "href": "app_metrics/app_metrics9.html#comparing-methods",
    "title": "Quantile regressions with multiple fixed effects",
    "section": "Comparing Methods",
    "text": "Comparing Methods\n\n\nCode\nclear\nset seed 101\nset obs 5000\ngen g = runiformint(1,250)\ngen rnd1 = rchi2(4)/4\ngen rnd2 = rchi2(4)/4\ngen rnd3 = rchi2(4)/4 - 1\n\nbysort g:gen fe = rnd1[1]\nbysort g:gen fe2 = rnd1[2] + fe\nbysort g:gen x1  =2 - fe + rchi2(4)/4\nbysort g:gen x2  = fe + rnd2[1] \n\ngen tau = runiform()\n\ngen y = 2+ tau + log(tau) + tau^2 * x1 - log(tau) * x2 + 2*tau*fe2\n\nqreg y x1 x2 fe2\nqregplot x1, name(m1, replace) title(Benchmark) graphregion( margin(r=5)) estore(ss)\nest restore ss\nmatrix qq=e(qq)\nmatrix bb=e(bs)\nmatrix ll=e(ll)\nmatrix uu=e(ul)\nsvmat qq\nsvmat bb\nsvmat ll\nsvmat uu\ncre , abs(g) keep: qreg y x1 x2 \nqregplot x1, name(m2, replace) title(CRE) graphregion( margin(r=5))\naddplot : rarea ll1 uu1 qq1, color(%30) lcolor(%0) pstyle(p4) || line bb1 qq1 , color(%40)  pstyle(p4)\n\nmmqreg y x1 x2, abs(g)\nqregplot x1, name(m3, replace) title(mmqreg) graphregion( margin(r=5))\naddplot : rarea ll1 uu1 qq1, color(%30) lcolor(%0) pstyle(p4) || line bb1 qq1 , color(%40)  pstyle(p4)\n \n reghdfe y x1 x2, abs(fex=g)\ngen y2 = y-fex\nqreg y2 x1 x2\nqregplot x1, name(m4, replace) title(Canay2011) graphregion( margin(r=5))\naddplot : rarea ll1 uu1 qq1, color(%30) lcolor(%0) pstyle(p4) || line bb1 qq1 , color(%40)  pstyle(p4)\n\nqreg y x1 x2 fex\nqregplot x1, name(m5, replace) title(M-Canay2011) graphregion( margin(r=5))\naddplot : rarea ll1 uu1 qq1, color(%30) lcolor(%0) pstyle(p4) || line bb1 qq1 , color(%40)  pstyle(p4)\n\nqreg y x1 x2\nqregplot x1, name(m6, replace) title(qreg no FE) graphregion( margin(r=5))\naddplot : rarea ll1 uu1 qq1, color(%30) lcolor(%0) pstyle(p4) || line bb1 qq1 , color(%40)  pstyle(p4)\n\ngraph combine m1 m2 m3 m4 m5 m6, col(3) ycommon nocopies \n\ngraph export fig99.png, width(2000) replace",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Quantile regressions with multiple fixed effects"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics7.html",
    "href": "app_metrics/app_metrics7.html",
    "title": "Linear Regressions, OLS and Standard Errors",
    "section": "",
    "text": "Linear regressions (LR) are a fundamental tool for economists to analyze data. Among the various methods for estimating the corresponding parameters, the most commonly used is Ordinary Least Squares (OLS). When certain assumptions are met1 , OLS can provide unbiased and consistent estimates of how changes in independent variables (\\(X's\\)) affect the dependent variable (\\(y\\)), assuming that all other factors remain constant. One advantage of OLS is that it is easy to apply and provides a straightforward solution.\nConsider the following linear model (in matrix form), and assume all Classical LR assumtions hold:\n\\[y =X  \\beta  + e\n\\]\nIf that is the case, the OLS estimator for \\(\\beta's\\) is simply given by:\n\\[\n\\hat \\beta_{OLS}=(X'X)^{-1}X'y\n\\]\nWhich is one of those formulas you probably know by memory by know. Something else you can derive from here is the relationship between the estimate \\(\\hat\\beta\\) and the true \\(\\beta\\):\n\\[\n\\begin{aligned}\n\\hat \\beta_{OLS}&=(X'X)^{-1}X'(X\\beta + e) \\\\\n&=(X'X)^{-1}X'X\\beta  + (X'X)^{-1}X'e \\\\\n&= \\beta + (X'X)^{-1}X'e\n\\end{aligned}\n\\]\nWhich simply states that \\(\\hat\\beta\\) will be unbiased if \\(E((X'X)^{-1}X'e)=0\\). But other wise, when using samples, there will be small differences because of the sampling error.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Linear Regressions, OLS and Standard Errors"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics7.html#introduction",
    "href": "app_metrics/app_metrics7.html#introduction",
    "title": "Linear Regressions, OLS and Standard Errors",
    "section": "",
    "text": "Linear regressions (LR) are a fundamental tool for economists to analyze data. Among the various methods for estimating the corresponding parameters, the most commonly used is Ordinary Least Squares (OLS). When certain assumptions are met1 , OLS can provide unbiased and consistent estimates of how changes in independent variables (\\(X's\\)) affect the dependent variable (\\(y\\)), assuming that all other factors remain constant. One advantage of OLS is that it is easy to apply and provides a straightforward solution.\nConsider the following linear model (in matrix form), and assume all Classical LR assumtions hold:\n\\[y =X  \\beta  + e\n\\]\nIf that is the case, the OLS estimator for \\(\\beta's\\) is simply given by:\n\\[\n\\hat \\beta_{OLS}=(X'X)^{-1}X'y\n\\]\nWhich is one of those formulas you probably know by memory by know. Something else you can derive from here is the relationship between the estimate \\(\\hat\\beta\\) and the true \\(\\beta\\):\n\\[\n\\begin{aligned}\n\\hat \\beta_{OLS}&=(X'X)^{-1}X'(X\\beta + e) \\\\\n&=(X'X)^{-1}X'X\\beta  + (X'X)^{-1}X'e \\\\\n&= \\beta + (X'X)^{-1}X'e\n\\end{aligned}\n\\]\nWhich simply states that \\(\\hat\\beta\\) will be unbiased if \\(E((X'X)^{-1}X'e)=0\\). But other wise, when using samples, there will be small differences because of the sampling error.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Linear Regressions, OLS and Standard Errors"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics7.html#how-precise-are-our-estimates",
    "href": "app_metrics/app_metrics7.html#how-precise-are-our-estimates",
    "title": "Linear Regressions, OLS and Standard Errors",
    "section": "How precise are our estimates?",
    "text": "How precise are our estimates?\nThe last expression has an additional value. It shows that the estimated coefficient \\(\\hat\\beta\\) can vary across samples, it also provides us a clue of how to measure the precision of that estimate.\nSpecifically, if we apply \\(Var()\\) to both sides of the equation, we will have:\n\\[\n\\begin{aligned}\nVar(\\hat\\beta_{OLS})=Var(\\beta + (X'X)^{-1}X'e) \\\\\nVar(\\hat\\beta_{OLS})=Var(\\beta) +Var((X'X)^{-1}X'e)  \n\\end{aligned}\n\\]\nAssuming that \\(X's\\) are fixed, we can rewrite this equation into the Variance covariance matrix we are accostume to: \\[Var(\\hat\\beta_{OLS})=Var(\\beta) +(X'X)^{-1}X' Var(e) X (X'X)^{-1}\n\\]\nWhere \\(Var(e)\\) is not necessarity the ovarall variance of \\(e_i\\). Instead is the \\(N\\times N\\) variance convariance matrix that combines the information of all observations in a sample.\n\\[\nVar(e) = \\left(\n\\begin{matrix}\n\\sigma^2_1 & \\sigma_{1,2} & \\sigma_{1,3} & ... &  \\sigma_{1,n} \\\\\n\\sigma_{1,2} & \\sigma_2^2 & \\sigma_{2,3} & ... &  \\sigma_{2,n} \\\\\n\\sigma_{1,3} & \\sigma_{2,3} & \\sigma^2_{3} & ...  & \\sigma_{3,n} \\\\\n... & ... & ... & ...& ...  \\\\\n\\sigma_{1,n} & \\sigma_{n,2} & \\sigma_{n,3} & ...  & \\sigma^2_{n} \\\\\n\\end{matrix}\\right)\n\\]\nThis may be somewhat less intuive, and will require a bit of an stretch. In a crossectional sample, we only observe a unit once, but in theory, what we observe is just one of many possible states that unit could have been at. At each one of those, that unit may have received a different \\(e_i\\). Thus, if we could see all those states, it would be possible to obtain units own unobservable variance \\(\\sigma^2_i\\), and its correlation with other units \\(\\sigma_{ij}\\).\nBut let me make one last change and substitute \\(Var(e)\\) with \\(\\Omega\\), so we have the more traditional formula: \\[\nVar(\\hat\\beta_{OLS}) = \\color{brown}{(X'X)^{-1}} \\color{green}{X'} \\color{red} \\Omega \\color{green}{X}\\color{brown}{(X'X)^{-1}}\n\\]\nWhich is usually described as the Sandwich formula. Because of this, I color coded the components.\n\n\\(\\color{brown}{(X'X)^{-1}}\\): Is the Bread\n\\(\\color{green}{X'}\\): Is the Lettuce\n\\(\\color{red} \\Omega\\): The best part of the Sandwich, the meat! And Of course, depending on the kind of meat we use, we would have different kind of sandwiches, or in this case different types of Standard errors.\n\nSo let us go ahead and revise the different types of Standard erros we can get from this expression. In what follows, however, I will omit any degrees of freedom correction, to simplify some of the expressions",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Linear Regressions, OLS and Standard Errors"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics7.html#homoskedastic-standard-errors",
    "href": "app_metrics/app_metrics7.html#homoskedastic-standard-errors",
    "title": "Linear Regressions, OLS and Standard Errors",
    "section": "Homoskedastic Standard errors",
    "text": "Homoskedastic Standard errors\nThe first type of Standard errors we learn in Econometrics 101, is the one based on the assumption of homoskedasticity and no cross-units correlation. This means that:\n\nUnobservables \\(e\\) that affect unit \\(i\\) are completely independent from the unobservables that affect unit \\(j\\).\nAnd that the magnitud or variance of the unobservables is the same across all individuals.\n\nMathematically this means: \\[\n\\begin{aligned}\n\\sigma_i^2 &=\\sigma_j^2 = \\sigma^2 \\\\\n\\sigma_{ij} &=0 \\ \\forall \\ i\\neq j\n\\end{aligned}\n\\]\nWhich simplifies greatly the \\(\\Omega\\) matrix: \\[\n\\Omega_0 =\n\\left(\n\\begin{matrix}\n\\sigma^2 & 0 & 0 & ... &  0 \\\\\n0 & \\sigma^2 & 0 & ... &  0 \\\\\n0 & 0 & \\sigma^2 & ...  & 0 \\\\\n... & ... & ... & ...& ...  \\\\\n0 & 0 & 0 & ...  & \\sigma^2 \\\\\n\\end{matrix}\\right)= \\sigma^2 I(N)\n\\]\nAnd of course simplyfies the Variance-covariance of the \\(\\beta\\) coefficients to:\n\\[Var(\\hat\\beta_{OLS})_0 = (X'X)^{-1} X'\\sigma^2 I(N) X (X'X)^{-1}=\\sigma^2 (X'X)^{-1}\n\\]",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Linear Regressions, OLS and Standard Errors"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics7.html#robust-standard-errors",
    "href": "app_metrics/app_metrics7.html#robust-standard-errors",
    "title": "Linear Regressions, OLS and Standard Errors",
    "section": "Robust Standard errors",
    "text": "Robust Standard errors\nNow, Robust standard errors, also known as Huber-White Robust standard errors, lifts one of the assumptions compared to the previous case. It explicitly allows for the variance of the errors to be different across individuals, while still imposing no correlation across individuals. \\[\n\\begin{aligned}\n\\sigma_{ij} &=0 \\ \\forall \\ i\\neq j \\\\\n\\exists i,j &:\\sigma^2_{i}\\neq \\sigma^2_{j}\n\\end{aligned}\n\\]\nThis does not mean that the variance will always be different when comparing two units, just that they could be different. With this conditions, we can partially simply the \\(\\Omega\\) matrix. However, I will not show any further simplications for the variance of \\(\\beta's\\) \\[\n\\Omega_1 =\n\\left(\n\\begin{matrix}\n\\sigma_1^2 & 0 & 0 & ... &  0 \\\\\n0 & \\sigma_2^2 & 0 & ... &  0 \\\\\n0 & 0 & \\sigma_3^2 & ...  & 0 \\\\\n... & ... & ... & ...& ...  \\\\\n0 & 0 & 0 & ...  & \\sigma_n^2 \\\\\n\\end{matrix}\\right)\n\\]",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Linear Regressions, OLS and Standard Errors"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics7.html#way-cluster-standard-errors",
    "href": "app_metrics/app_metrics7.html#way-cluster-standard-errors",
    "title": "Linear Regressions, OLS and Standard Errors",
    "section": "1-way Cluster Standard errors",
    "text": "1-way Cluster Standard errors\nCluster standard errors lifts an additional assumption to how the variance covariance matrix should be estimated. In contrast with robust standard errors, cluster standard errors assumes that the unobservables are not necessarily independent across units.\nFor example, people within the same family will share similar experiences, thus their unobservables will be correlated. Same for students in the same classrooms, because they share the same “teacher exprience”, or workers in the same firm, because they face the same work enviroment.\nWe, of course, still need to impose some restrictions on the data.\n\nFirst, as most software was imposing until few years ago, we assume that there is only 1 dimension that individuals could be related to each other.\nSecond, we still need to assume that if individuals do not belong to the same “group”, their errors are still independent.\n\nTo show this mathematically, let me define the function \\(g()\\), which gives me the group membership of unit \\(i\\). In other words, if unit \\(i\\) belongs to family \\(k\\), then \\(g(i)=k\\). With this function, we can define the correlations across individuals to be defined as: \\[\n\\begin{aligned}\n\\sigma_{i,j} \\neq 0 \\text{  if  } g(i)=g(j)\n\\end{aligned}\n\\]\nIn this case, the \\(\\Omega\\) matrix will look very similar to the one we use for robust standard errors. It will still be block diagonal, with the main diagonal having block elements different from zero (\\(\\Sigma_j\\)), and all other elements outside the diagonal to be zero: \\[\n\\Omega_2 =\n\\left(\n\\begin{matrix}\n\\sigma_1^2     & \\sigma_{1,2} & 0         & ...  &  0 \\\\\n\\sigma_{1,2} & \\sigma_2^2     & 0         & ...  &  0 \\\\\n0            & 0            & \\sigma_3^2  & ...  & \\sigma_{3,n} \\\\\n...          & ...          & ...       & ...  & ...  \\\\\n0            & 0            & \\sigma_{3,n}     & ...  & \\sigma_n^2 \\\\\n\\end{matrix}\\right) =\n\\left(\n\\begin{matrix}\n\\Sigma_1     & 0            \\\\\n0            & \\Sigma_2      \\\\\n\\end{matrix}\\right)\n\\]\nVisually, this would look like the following\n\n\nCode\nclear\nset scheme white2\ncolor_style tableau\nset seed 1\nset obs 50\ngen r1=runiformint(1,4)\ngen r2=runiformint(1,4)\ngen id=_n\nsort r1  r2\nqui:mata:\nr1=st_data(.,\"r1\")\nr2=st_data(.,\"r2\")\nrr1=J(rows(r1)*rows(r2),4,0)\nk=0\nfor(i=1;i&lt;=50;i++){\n    for(j=1;j&lt;=50;j++){\n        if ((r1[i]==r1[j]) | (r2[i]==r2[j])) {\n            k++\n            rr1[k,]=(51-i,j,(r1[i]==r1[j]),(r2[i]==r2[j]) )         \n        }\n    }   \n}\nrr1=rr1[1..k,]\nend\ngetmata rr1*=rr1, replace force\n\ntwo (scatter rr11 rr12 if rr13==1,  ms(s) msize(2.1))  ///\n    (scatter rr11 rr12 if 51-rr11 == rr12, ms(s) msize(2.1) color(gs1)  ) ///\n    , aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off)\n\n\n\nCorrelation matrix \\(\\Omega\\)\n\n\n\n\n\n\n\n\n\n\n\n\nIn this figure, the blue blocks shows that we are allowing for some non-zero correlation among observations that belong to the same group or cluster. Implicily, we are also allowing for all elements of the main diagonal (in black) to be different from zero, which is why Cluster standard errors are also robust to heteroskedasticity.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Linear Regressions, OLS and Standard Errors"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics7.html#way-cluster-standard-errors-1",
    "href": "app_metrics/app_metrics7.html#way-cluster-standard-errors-1",
    "title": "Linear Regressions, OLS and Standard Errors",
    "section": "2-way Cluster Standard Errors",
    "text": "2-way Cluster Standard Errors\nWhat about 2-way, or for even M-way, clustered standard errors?\nAs I mentioned above, 1-way clustered standard errors allows for individuals unobserved errors to be correlated across each other, but only if they belong to the same group. If they are from different groups, then no correlation should exists. 1-Way cluster standard errors also assumes that there is only one set of groups that people can be connected through.\nThis assumption is also quite strong. People, for example, are interconnected through many chanels. My unobserved factors are likely to be related to my wife’s and daughter’s unobservables, because we belong to the same household. However, they may also be related to my co-workers, because we all have the same workplace, thus share similar experience. I cannot discard my grad-school peers, since we have shared experiences (and thus correlated unobservables). Some of this connections would be new and unique, but some others may overlap.\nThe good news is that 2-way cluster standard errors, or M-way, does allow for this more complex scenario. Specifically, if two individuals are part of at least 1 common group, then we will assume the correlation of unobserved factors is different from zero. Unfortunately, we cannot differentiate which connection is the one that is driven that correlation.\nTo show this mathematically, let me define the function \\(g_h()\\), which indicates the group, among the set \\(h\\) (cluster variable), that a unit belongs to, and \\(gg(i,j)\\) which takes the value of zero if \\(i\\) and \\(j\\) are not connected to each other in any way (based on the cluster variables), and 1 otherwise. \\[\n\\begin{aligned}\ngg(i,j)&=0 \\text{  if  } \\forall h:g_h(i)\\neq  g_h(j) \\\\\n&\\text{ and 1 otherwise}\n\\end{aligned}\n\\]\nIf this is the case, \\(\\sigma_{ij}=0\\) is assumed zero if \\(gg(i,j)=0\\), but allow to vary otherwise.\nThe implications that it has on the \\(\\Omega\\) matrix is that it will no longer be block diagonal, because elements outside the main diagonal (and block diagonal) will be different from zero. \\[\\Omega_3 =\n\\left(\n\\begin{matrix}\n\\sigma_1^2     & \\sigma_{1,2} & 0         & ...  &  \\sigma_{1,n} \\\\\n\\sigma_{1,2} & \\sigma_2^2     & \\sigma_{2,3}         & ...  &  0 \\\\\n0            & \\sigma_{2,3}   & \\sigma_3^2  & ...  & \\sigma_{3,n} \\\\\n...          & ...            & ...       & ...  & ...  \\\\\n\\sigma_{1,n} & 0            & \\sigma_{3,n}     & ...  & \\sigma_n^2 \\\\\n\\end{matrix}\\right)\n\\]\nIts perhaps easier to show this visually:\n\n\nCode\ntwo (scatter rr11 rr12 if rr13==1,  ms(s) msize(2.1))  ///\n    , aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off) name(m1, replace) \n\ntwo (scatter rr11 rr12 if rr14==1,  ms(s) msize(2.1))  ///\n    , aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off) name(m2, replace)    \n\ntwo (scatter rr11 rr12 if rr14==1 | rr13==1,  ms(s) msize(2.1))  ///\n    , aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off) name(m3, replace)\n\n\n\n\n\n\n\n\n\n\n\n(a) 1-way cluster\n\n\n\n\n\n\n\n\n\n\n\n(b) 2nd 1-way cluster\n\n\n\n\n\n\n\n\n\n\n\n(c) 2-way cluster\n\n\n\n\n\n\nFigure 1: Correlation matrix \\(\\Omega\\)\n\n\n\n\nFigure 1 shows the correlation structure we would observe considering the 2 clustered variables. Figure 1 (a) and Figure 1 (b) show the interconnections based on a first and a second cluster variables, whereas Figure 1 (c) considers the combination of both. To estimate Standard errors we need to allow the blue cells to be different from zero, impossing the zero condition only on the “unconnected” pairs.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Linear Regressions, OLS and Standard Errors"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics7.html#how-to-estimate-sigma_ij",
    "href": "app_metrics/app_metrics7.html#how-to-estimate-sigma_ij",
    "title": "Linear Regressions, OLS and Standard Errors",
    "section": "How to estimate \\(\\sigma_{ij}\\)?",
    "text": "How to estimate \\(\\sigma_{ij}\\)?\nUp to this point, I have shown how one should think about the estimation of Standard errors. But you may also be interested in the practical approach. Specifically, how do we come up with estimates for \\(\\sigma_i^2\\) and \\(\\sigma_{ij}\\).?\nThis is actually straight forward!. Since we only see each unit once in crossection, our best estimation of terms is given by: \\[\n\\hat \\sigma_{ij} = \\varepsilon_i*\\varepsilon_j\n\\]\nwhere \\(\\varepsilon_i\\) is the unit specific unobserved component. So that is pretty much it! once you have \\(\\hat \\sigma_{ij}\\) you simply need to define which assumption you want to use (homoskedastic errors, heteroskedastic, or clustered), identify which elements you will assume to be zero, and voila!…\nOr almost. While what I describe here is technically correct, there are two limitations to its application compared to how is it done in most software.\n\nI’m disregarding completely the role of degrees of freedom and corresponding corrections.\nThe estimation of \\(\\hat\\Omega\\) as described here is only feasible for small datasets, because it suggest creating a \\(N\\times N\\) matrix. This would easily consume all the resources of your computer.\n\nRegarding point 2. While there are other ways to simply the math, thinking in terms of the unrestricted \\(\\Omega\\) is useful to understand the role of clustering and two way clustering in the data.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Linear Regressions, OLS and Standard Errors"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics7.html#stata-example",
    "href": "app_metrics/app_metrics7.html#stata-example",
    "title": "Linear Regressions, OLS and Standard Errors",
    "section": "Stata Example",
    "text": "Stata Example\nHere I will work on a small example that would implement the 4 approaches to estimating standard errors. Because I will use the unrestrict \\(\\hat\\Omega\\) matrix, I will work with a very small dataset: `auto’. The next contains lots of code, so feel free to skip it.\n\n** Load Data, create clusters and a constant\nsysuse auto, clear\nset seed 1\nreplace price = price / 1000\ngen one = 1\ngen c1  = runiformint(1,5)\ngen c2  = runiformint(1,5)\n** Load Data to Mata to easy manipulation\nmata: y = st_data(.,\"price\")\nmata: x = st_data(.,\"mpg one\")\n** estimate betas, ixx, and erros\nmata: xx = cross(x,x); ixx = invsym(xx);xy=cross(x,y)\nmata: b  = ixx * xy ;  e   = y:-x*b\n\n(1978 automobile data)\nvariable price was int now float\n(74 real changes made)\n\n\nHomoskedastic errors:\n\nmata: omega_0 = I(74)*mean(e:^2)\nmata: vcv_0   = ixx * x' * omega_0 * x * ixx\nmata: svcv_0  = diagonal(vcv_0):^.5;svcv_0\n\n                 1\n    +---------------+\n  1 |  .0523545211  |\n  2 |  1.154882583  |\n    +---------------+\n\n\nRobust Standard errors\n\nmata: omega_1 = (e*e'):*I(74)\nmata: vcv_1   = ixx * x' * omega_1 * x * ixx\nmata: svcv_1  = diagonal(vcv_1):^.5;svcv_1\n\n                 1\n    +---------------+\n  1 |  .0566949707  |\n  2 |   1.35766526  |\n    +---------------+\n\n\n1-way Cluster Standard errors:\n\nmata: cc = st_data(.,\"c1 c2\")\nmata: i1 = J(1,74,cc[,1]):==J(74,1,cc[,1]') \nmata: i2 = J(1,74,cc[,2]):==J(74,1,cc[,2]') \nmata: omega_2a = (e*e'):*i1; omega_2b = (e*e'):*i2\nmata: vcv_2a   = ixx * x' * omega_2a * x * ixx\nmata: vcv_2b   = ixx * x' * omega_2b * x * ixx\nmata: svcv_2a  = diagonal(vcv_2a):^.5\nmata: svcv_2b  = diagonal(vcv_2b):^.5\nmata: svcv_2a, svcv_2b \n\n                 1             2\n    +-----------------------------+\n  1 |  .0816625514   .0293288459  |\n  2 |  2.010064686   .6935766918  |\n    +-----------------------------+\n\n\n2-way Cluster Standard errors\n\nmata: i3 = i1:|i2\nmata: omega_3 = (e*e'):*i3\nmata: vcv_3   = ixx * x' * omega_3 * x * ixx\nmata: svcv_3  = diagonal(vcv_3):^.5; svcv_3\n\n                 1\n    +---------------+\n  1 |  .0595359543  |\n  2 |  1.523244536  |\n    +---------------+\n\n\nBut for the 2-way Cluster Standard errors you also have the more commonly used formula:\n\n** First we need to create a variable that combines both clusters\nmata: i4 = i1:&i2\nmata: omega_4 = (e*e'):*i4\nmata: vcv_aux   = ixx * x' * omega_4 * x * ixx\nmata: vcv_4 = vcv_2a + vcv_2b - vcv_aux\nmata: svcv_4  = diagonal(vcv_4):^.5; svcv_4\n\n                 1\n    +---------------+\n  1 |  .0595359543  |\n  2 |  1.523244536  |\n    +---------------+",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Linear Regressions, OLS and Standard Errors"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics7.html#conclusions",
    "href": "app_metrics/app_metrics7.html#conclusions",
    "title": "Linear Regressions, OLS and Standard Errors",
    "section": "Conclusions",
    "text": "Conclusions\nIn this note, I provide a walkthrough the estimation of different types of standard errors from a linear regression model.\nWhile most textbooks already cover the basics of standard errors under homoskedasticity, heteroskedasticity, and oneway clustered errors, I have not seen an intuitive approach to understanding w-way clustered standard errors.\nAs I shown here, the idea of 2-way, or M-way, clustered standard errors is to allow arbitrary correlations across individuals, if they have to at least one common group (one cluster variable).\nNow, one may be tempted to try and use this approach to allow for unconstrained correlation across all units. After all we are all connected to some way. However, because we do not see the true unobserved variance-covariance matrix, the variance of \\(\\beta\\) would not be identified without impossing restrictions on \\(\\Omega\\).\nIf you are interested in learning more about this topic, I would recommend you to read Cameron, Gelbach, and Miller (2011), MacKinnon, Nielsen, and Webb (2023) and Abadie et al. (2022).",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Linear Regressions, OLS and Standard Errors"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics7.html#footnotes",
    "href": "app_metrics/app_metrics7.html#footnotes",
    "title": "Linear Regressions, OLS and Standard Errors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee for example Introductory Econometrics: A Modern Approach by Jeffery Wooldridge↩︎",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Linear Regressions, OLS and Standard Errors"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics5.html#simple-2x2-did",
    "href": "app_metrics/app_metrics5.html#simple-2x2-did",
    "title": "2x2 DID: Sant’Anna and Zhao (2020)",
    "section": "Simple 2x2 DID",
    "text": "Simple 2x2 DID",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "2x2 DID: Sant'Anna and Zhao (2020)"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics5.html#level-up-adding-controls",
    "href": "app_metrics/app_metrics5.html#level-up-adding-controls",
    "title": "2x2 DID: Sant’Anna and Zhao (2020)",
    "section": "Level up: Adding controls",
    "text": "Level up: Adding controls",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "2x2 DID: Sant'Anna and Zhao (2020)"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics5.html#going-doubly-robust",
    "href": "app_metrics/app_metrics5.html#going-doubly-robust",
    "title": "2x2 DID: Sant’Anna and Zhao (2020)",
    "section": "Going Doubly Robust",
    "text": "Going Doubly Robust",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "2x2 DID: Sant'Anna and Zhao (2020)"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics5.html#drdid-the-stata-way",
    "href": "app_metrics/app_metrics5.html#drdid-the-stata-way",
    "title": "2x2 DID: Sant’Anna and Zhao (2020)",
    "section": "drdid: The Stata way",
    "text": "drdid: The Stata way",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "2x2 DID: Sant'Anna and Zhao (2020)"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics5.html#references",
    "href": "app_metrics/app_metrics5.html#references",
    "title": "2x2 DID: Sant’Anna and Zhao (2020)",
    "section": "References",
    "text": "References\n\n\nSant’Anna, Pedro H. C., and Jun Zhao. 2020. “Doubly Robust Difference-in-Differences Estimators.” Journal of Econometrics 219 (1): 101–22. https://doi.org/10.1016/j.jeconom.2020.06.003.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "2x2 DID: Sant'Anna and Zhao (2020)"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics3.html",
    "href": "app_metrics/app_metrics3.html",
    "title": "DID: The Fall",
    "section": "",
    "text": "Differences-in-Differences has been a popular approach for identifying causal effects of treatments, utilizing data from both treated and untreated units across time. However, in recent years (5? as of 2023), this methodology has come under scrutiny due to several flaws that have been identified, leading to what some refer to as the DID-Revolution. Numerous econometricians, both established and new, have worked on this topic, highlighting the problems associated with the methodology and proposing potential solutions. Additionally, they have suggested extensions to the model to address a wide range of scenarios (The literature has grown…alot).\nAs someone who was initially perplexed by these changes, problems, and solutions, I started reading in detail on the topic, and had the opportunity to develop two Stata commands:\n\ncsdid and csdid2, which implement Callaway and Sant’Anna (2021) approach\njwdid, which implements the approach suggested by Wooldridge (2021).\n\nThrough this process of coding and reading, I gained a better understanding of several concepts, particularly why certain strategies, such as the Two-way-Fixed-effect (TWFE), may not yield accurate estimates.\nTo simplify things and improve intuition, I will discuss the methodology and its problems and solutions.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: The Fall"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics3.html#introduction",
    "href": "app_metrics/app_metrics3.html#introduction",
    "title": "DID: The Fall",
    "section": "",
    "text": "Differences-in-Differences has been a popular approach for identifying causal effects of treatments, utilizing data from both treated and untreated units across time. However, in recent years (5? as of 2023), this methodology has come under scrutiny due to several flaws that have been identified, leading to what some refer to as the DID-Revolution. Numerous econometricians, both established and new, have worked on this topic, highlighting the problems associated with the methodology and proposing potential solutions. Additionally, they have suggested extensions to the model to address a wide range of scenarios (The literature has grown…alot).\nAs someone who was initially perplexed by these changes, problems, and solutions, I started reading in detail on the topic, and had the opportunity to develop two Stata commands:\n\ncsdid and csdid2, which implement Callaway and Sant’Anna (2021) approach\njwdid, which implements the approach suggested by Wooldridge (2021).\n\nThrough this process of coding and reading, I gained a better understanding of several concepts, particularly why certain strategies, such as the Two-way-Fixed-effect (TWFE), may not yield accurate estimates.\nTo simplify things and improve intuition, I will discuss the methodology and its problems and solutions.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: The Fall"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics3.html#did-2x2-canonical-design",
    "href": "app_metrics/app_metrics3.html#did-2x2-canonical-design",
    "title": "DID: The Fall",
    "section": "DiD 2x2 Canonical Design",
    "text": "DiD 2x2 Canonical Design\nLet’s start with the fundamental building block of the Differences-in-Differences methodology, which is the 2x2 canonical design that is widely used in many papers related to DID models.\nThe basic 2x2 DiD design involves observing two groups of observations across two periods in time. During the first period, neither of these groups receive a treatment, and thus, it can be assumed that they have similar experiences up to that point.\nIn the second period, one of the groups receives a treatment, such as a training program, medicine, or other types of treatments, while the other remains untreated. By comparing the changes in outcomes between the treated and untreated groups over time, we can estimate the causal effect of the treatment.\nTo analyze the data, a simple 2x2 matrix can be designed after the experiment is finalized, which helps to quantify the impact of the treatment.\n\n\n\nTime Treatment\nTR=0\nTR=1\n\n\n\n\nT=0\n\\(Y(0,0)\\)\n\\(Y(1,0)\\)\n\n\nT=1\n\\(Y(0,1)\\)\n\\(Y(1,1)\\)\n\n\n\nTo simplify notation, I’ll assume that all the \\(Y(TR,T)\\) represent the average outcome for the group -TR- (treated or untreated) at time T (Pre=0 and Post =1).\n\nHow are treatment effects estimated?\nWe can begin by noting that the treatment occurred in T=1. One naive estimator that we could use to estimate the treatment effect (TE) would be to simply obtain the difference in outcome between the treated and untreated groups, which is expressed as “\\(Y(1,1)-Y(0,1)\\)” (First D).\nHowever, this approach is incorrect because it fails to account for the self-selection bias that may exist between the two groups. For instance, the individuals who received the treatment may have been those who would benefit from it the most or needed it the most, which could lead to differences in their average outcomes.\nAnother naive approach would be to only examine the treated unit and evaluate how much its outcome has changed after the treatment, which is expressed as “\\(Y(1,1)-Y(1,0)\\)” (the other D). This approach would also be incorrect because the observed change could be due to some natural growth or trend that the unit experienced independent of the treatment.\nTo properly estimate the treatment effect (TE) , we need to combine both strategies and obtain the Differences in Differences (DiD). This approach helps to identify the causal effect of the treatment by comparing the changes in outcomes between the treated and untreated groups over time.\nIntuitively, this could be thought in two ways:\n\nEstimate the TE by comparing the treated-untreated outcomes change in the post period to the pre-period outcome difference.\n\nTT1=  (y11-y01)  Post-period   Treated vs untreated\n     -(y10-y00)  pre -period   Treated vs untreated\nIn essence this captures the TE if the selection bias of the second period is the same as the selection bias observed in the first period.\n\nEstimate the TE by comparing the outcome change for the treated group across time to the outcome change experienced by the not-treated group.\n\nTT2=  (y11-y10)  Treated    Post vs pre\n     -(y01-y00)  Untreated  Post vs pre\nIn this case, the TE is estimated if both groups would have experienced the same trends in their outcomes across time.\nBoth strategies estimate the same treatment effect, but they rely on different assumptions.\n\nThe first strategy assumes “bias-stability,” which implies that any factors that explain the difference between treated and untreated outcomes after the treatment are also present before the treatment and can be eliminated.\n\n-The second strategy relies on the “parallel-trends” assumption, which assumes that the treated outcome would have experienced a similar and parallel change in outcome as the untreated units experience.\nThis is the vanilla 2x2, piece-of-cake strategy. If the basic assumptions hold, you cannot go wrong with this. The problem, surprisingly or not, is what happens when you have more data.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: The Fall"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics3.html#multiple-periods-did-txg-design",
    "href": "app_metrics/app_metrics3.html#multiple-periods-did-txg-design",
    "title": "DID: The Fall",
    "section": "Multiple Periods DID TxG Design:",
    "text": "Multiple Periods DID TxG Design:\n\nWhere things went wrong\nAs I mentioned earlier, the simple case of DiD is straightforward to estimate and get right. We can even use a simple linear regression analysis to calculate the treatment effect (TE) using the formula:\n\\[\ny_{it} = a_0 + a_1*tr + a_2*t + \\delta * (tr*t) +e_{it}\n\\]\nHowever, the challenge arises when the design has multiple time periods and groups, and the treatment occurs at different times. This is known as treatment timing heterogeneity. In the following discussion, I will refer to insights from Callaway and Sant’Anna (2021) and Goodman-Bacon (2021) to explain the concepts.\n\n\nSetup\nAssume that there are four time periods T=1, 2, 3, and 4, and five “groups” G=Never Treated (NT), 1, 2, 3 and 4.\nIn the 2x2 design, the treatment identifier “TR” distinguishes treated (1) from untreated (0) units. However, in a design with more time periods, the possibility arises that units can be treated at different points in time or never treated at all.\nCallaway and Sant’Anna (2021) use the letter “G” to identify this variable and represent observations that are never treated as “infinity \\(\\infty\\)”. This is done because those units are never treated in the window of time we have access to.1\nFor simplicity, I will use “NT” to represent never treated units. It is worth noting that groups that are always treated cannot be identified in this design.\nThe TxG design can be represented in matrix form as follows:\n\n\n\nTxG Design\n\n\nIn this TxG design, orange cells represent units that have effectivly received treatment, while the blue cells represent potential control units that have not-yet received treatment. The green cells correspond to never treated units that can be used as perfect controls, assuming there are no spill-over effects.\nThis design can be thought of as a combination of multiple 2x2 designs, but not all possible combinations are good ones for estimating the treatment effect. This is, infact, where the traditional TWFE model get things wrong.\nA good reference that explains the problem is Goodman-Bacon (2021). The intuition behind the paper is that he traditional TWFE model obtains a parameter for TE that is the average of all possible 2x2 designs that could be constructed from the above matrix. However, not all of them are good ones!.\nSome of the 2x2 combinations will provide interesting and meaningful results:\n\nYou can use some of them to calculate Treatment effects.\nSome can be used for testing assumptions regarding parallel trends.\nSome, however, will be combinations that are not useful, and may lead you to incorrect conclusions.\n\n\n\nWhat is the TWFE?\nTo account for the many periods (T) and treatment-cohorts (G) in the data structure, an extension to the simple linear model was proposed. This involved estimating a linear model that includes controls (fixed effects/dummies) for all available periods in the data, as well as for all panel individuals (or treatment-cohort groups). To capture the treatment effect, researchers would include a dummy variable \\(D\\) that takes the value of 1 after a unit has been treated, and 0 if it was never or not yet treated. This specification can be written as: \\[\ny_{it} = a_i + a_t + \\delta D_{it}+e_{it}\n\\]\nwhere \\(a_i\\) captures the individual fixed effect (or cohort), \\(a_t\\) captures the time fixed effect, and \\(\\delta\\) which was meant to capture an average treatment effect for treated units, across all time periods.\nGoodman-Bacon (2021) discovered that the parameter of interest \\(\\delta\\) is a weighted average of all feasible DiD designs that could be constructed from the data. This is because the Ordinary Least Squares method attempts to leverage all possible variation, without taking into account the nuances of which ones are actually useful for identifying causal effects, and which ones are not adequate to do so.\n\n\nFirst the Good\nLets say, we are interested in the treatment effect only for the group that was first treated in period 2 (G=2).\nThe first thing to consider is which groups could be used as a “control”, to appropriately identify the TE. The first and easy choice is to compare the G=1 units with observations that were never treated (NT), the “perfect” control group.\nUsing the NT as control units, we can construct at least 3 2x2 DiD setups to identify the TE, at periods 2, 3 and 4:\n\n\n\nfig3\n\n\nA very similar exercise could be done if one is interested in analyzing the TE for other groups (say G3 and G4):\nIf a Never treated group were not available, it is also possible, and valid given limited information, to use units from other “treated” groups as controls, as long as they have not yet been treated:\n\n\n\nfig4\n\n\nAverage treatment effects could then be derived by “averaging” this individual treatment effects.\n\n\nSecond the Bad\nJust as there are good, there are also bad designs. As described in Goodman-Bacon (2021), one of the pitfalls of the TWFE is that it may also try to identify TE by comparing units that are already treated, but at different times.\n\n\n\nfig5\n\n\nConsider the first panel. It compares data for G3 to those in G2. At T=2, G3 is untreated, whereas G2 is treated both at T=2 and T=3. If the treatment effect were homogenous (a location shift), this may work. However, if the treatment affected both G2 and G3 differently, it would violate the parallel trend assumption, and this 2x2 design will not identify Treatment effects.\nNow, why are these units used as controls?\nFrom an OLS viewpoint, the reason is that the treatment status for a given unit remains constant across time, thereby making it appear as though the previously treated unit is a good control. Nonetheless, if the treatment effect varies over time, then the parallel trends assumption may be invalidated, and treatment effects will not be identified.\n\n\nBad controls and Negative weights: How bad can it be?\nThe problem describe above can be summarized with a single idea. WHen using OLS, sometimes one uses bad controls to estimete Treatment effects, which will affect the identification.\nThe other concept that has been used to describe the problem is one about Negative weights, which are closely related.\nFirst the technical part.\nAssume we have balanced panel data, and estimate the following model: \\[\ny_{it} = a_i + a_t + \\delta D_{it}+e_{it}\n\\]\nOne way to estimate this effects easily is to apply Frisch–Waugh–Lovell theorem, demean \\(D_{it}\\), and estimate \\(\\delta\\) using the new data:\n\\[\n\\begin{aligned}\n    \\delta^{twfe} &=\\frac{\\sum \\bar{\\bar D}_{it} y_{it}}{\\sum \\bar{\\bar D}_{it}^2} \\\\\n    \\bar{\\bar D}_{it} &= D_{it}+\\bar D-(\\bar D_i + \\bar D_t)\n\\end{aligned}    \n\\]\nWhen estimating treatment effects, treated units are expected to receive a positive weight. However, after applying FWL, there is no assurance that \\(\\bar{\\bar D}_{it}\\) will be positive for already treated units. This happens because \\(\\bar D_t\\) is larger in later periods, and \\(\\bar D_i\\) is larger for units that were treated earlier. In consequence, units that were treated earlier could incorrectly receive a negative weight, when estimating the treatment effect, it could also be that units that were never treated are incorrectedly assigned a possitive weight, when they should receive negative weights.\nHow bad could it be? Consider the following excercise/code:\n\n\nCode\nclear\nset seed 101\nset obs 100\ngen id = _n\ngen gv = runiformint(2,10)\nreplace gv = 0 if gv&gt;=8\nexpand 10\nbysort id:gen t=_n\ngen trt = gv&lt;=t & gv!=0\nssc install hdfe\nhdfe trt, abs(t id) gen(trt_d)\ngen y = 0\nreplace y = 1 if trt==1 & trt_d &lt;0 \n\n\nIn this case, y is zero for everyone, but for selected treated units at some point in time. You would expect average treatment effects to be possitive. TWFE, would dissagree:\n\n\nCode\ncapture ssc install reghdfe\nreghdfe y trt, abs(id t)\n\n\n(MWFE estimator converged in 2 iterations)\n\nHDFE Linear regression                            Number of obs   =      1,000\nAbsorbing 2 HDFE groups                           F(   1,    890) =      44.48\n                                                  Prob &gt; F        =     0.0000\n                                                  R-squared       =     0.5367\n                                                  Adj R-squared   =     0.4799\n                                                  Within R-sq.    =     0.0476\n                                                  Root MSE        =     0.2275\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         trt |  -.1770374   .0265457    -6.67   0.000    -.2291368    -.124938\n       _cons |   .1962698   .0145409    13.50   0.000     .1677313    .2248083\n------------------------------------------------------------------------------\n\nAbsorbed degrees of freedom:\n-----------------------------------------------------+\n Absorbed FE | Categories  - Redundant  = Num. Coefs |\n-------------+---------------------------------------|\n          id |       100           0         100     |\n           t |        10           1           9     |\n-----------------------------------------------------+\n\n\nBut why is that? The way treatment was assigned, only the units that were treated earlier have a positive treatment, but those are the ones used as controls later on, receiving negative weights. Thus, TWFE uses that and estimates negative TE.\n\n\n\n\n\nFWL-Weights",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: The Fall"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics3.html#these-were-the-problems-but-solutions",
    "href": "app_metrics/app_metrics3.html#these-were-the-problems-but-solutions",
    "title": "DID: The Fall",
    "section": "These were the problems, but solutions?",
    "text": "These were the problems, but solutions?\nWhat I provided you here was a brief review of DID, with some of the intution on why TWFE fails. Now, it doesn’t mean there is no solutions moving forward. In fact, by the time you are reading it, you will probaly be aware of many of them.\nOver the last years, there have been many propositions trying provide solutions that would tackle the problems related to TWFE DID. I have been lucky enough to participate in the after programming of couple of them, but also worked closely to understand some of the others: csdid/csdid2; jwdid; did2s; did_imputation; did_multiplegt.\nThese new strategies have one thing in commong. They aim to solve the problems of TWFE by either using good 2x2 designs, avoiding wrong 2x2 designs, or adjusting weights to reduce the impact of negative weights.\nI dive a bit into the possible solutions (and may reiterated the problems) in other posts. For now, hope you enjoy this.\nComments and questions welcome.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: The Fall"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics3.html#references",
    "href": "app_metrics/app_metrics3.html#references",
    "title": "DID: The Fall",
    "section": "References",
    "text": "References\n\n\nCallaway, Brantly, and Pedro H. C. Sant’Anna. 2021. “Difference-in-Differences with Multiple Time Periods.” Journal of Econometrics, Themed Issue: Treatment Effect 1, 225 (2): 200–230. https://doi.org/10.1016/j.jeconom.2020.12.001.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with Variation in Treatment Timing.” Journal of Econometrics, Themed Issue: Treatment Effect 1, 225 (2): 254–77. https://doi.org/10.1016/j.jeconom.2021.03.014.\n\n\nWooldridge, Jeffrey M. 2021. “Two-Way Fixed Effects, the Two-Way Mundlak Regression, and Difference-in-Differences Estimators.” {SSRN} {Scholarly} {Paper}. Rochester, NY. https://doi.org/10.2139/ssrn.3906345.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: The Fall"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics3.html#footnotes",
    "href": "app_metrics/app_metrics3.html#footnotes",
    "title": "DID: The Fall",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis suggests they will be treated at some point in the far future↩︎",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: The Fall"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics12.html",
    "href": "app_metrics/app_metrics12.html",
    "title": "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools",
    "section": "",
    "text": "Its been a while since I last posted. Last year it was classes and research and new family member, this year was a new job, new challenges, and new skills to get. However, While my Python-fu is getting better, my weapon of choice is still Stata. Today I dediced to vibe-write a post about the new cate command in Stata 19. I just got my hands on it, and I am excited to share my thoughts. What is the twist?\nNot everyone can have Stata 19, for one reason or another. So, I will show you how to do similar things with two basic commands: reg and margins. This comes with a plus, with regress, you can easily get CATE, but also CATT!! (Conditional Average Treatment on the Treated). So lets do it!",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics12.html#prologue",
    "href": "app_metrics/app_metrics12.html#prologue",
    "title": "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools",
    "section": "",
    "text": "Its been a while since I last posted. Last year it was classes and research and new family member, this year was a new job, new challenges, and new skills to get. However, While my Python-fu is getting better, my weapon of choice is still Stata. Today I dediced to vibe-write a post about the new cate command in Stata 19. I just got my hands on it, and I am excited to share my thoughts. What is the twist?\nNot everyone can have Stata 19, for one reason or another. So, I will show you how to do similar things with two basic commands: reg and margins. This comes with a plus, with regress, you can easily get CATE, but also CATT!! (Conditional Average Treatment on the Treated). So lets do it!",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics12.html#introduction",
    "href": "app_metrics/app_metrics12.html#introduction",
    "title": "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools",
    "section": "Introduction",
    "text": "Introduction\nTreatment effect heterogeneity is a critical concept in modern causal inference. Stata 19, just lunched last week, has introduced specialized commands for estimating Conditional Average Treatment Effects (CATE), with doubly robust, machine learning methods. This is very powerful, and going over the command, there is a lot behind the command, it cannot fit in a single blog post entry.\nWhat is quite interesting, and I shared my thoughts way back with some developers at Stata, is that one could already get similar results using tools Stata already provided. Of course, comparable results can be obtained using basic regression commands with careful implementation. This post demonstrates how powerful Stata can be when you understand its fundamentals, while also exploring the convenience of the new specialized tools.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics12.html#the-math-behind-treatment-effects",
    "href": "app_metrics/app_metrics12.html#the-math-behind-treatment-effects",
    "title": "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools",
    "section": "The Math Behind Treatment Effects",
    "text": "The Math Behind Treatment Effects\nBefore diving into implementation, let’s review the key mathematical concepts:\n\nPotential Outcomes Framework\nIn the potential outcomes framework, we define: - \\(y_i(1)\\) as the potential outcome if unit \\(i\\) is treated - \\(y_i(0)\\) as the potential outcome if unit \\(i\\) is not treated - \\(x_i\\) as a vector of characteristics for unit \\(i\\)\nThe Average Treatment Effect (ATE) is:\n\\[ATE \\equiv E[y_i(1) - y_i(0)]\\]\nThe Conditional Average Treatment Effect (CATE) is:\n\\[CATE \\equiv \\tau(x) = E[y_i(1) - y_i(0) | x_i = x]\\]\nThis represents the expected treatment effect for units with characteristics \\(x\\).\nIn the same line \\(CATT\\) or Conditional Average Treatment on the Treated can be defined as:\n\\[CATT \\equiv \\tau(x) = E[y_i(1) - y_i(0) | x_i = x, TrT= 1]\\]",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics12.html#implementation-in-stata",
    "href": "app_metrics/app_metrics12.html#implementation-in-stata",
    "title": "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools",
    "section": "Implementation in Stata",
    "text": "Implementation in Stata\nThe new CATE command in Stata 19 provides a streamlined way to estimate CATEs using advanced machine learning methods. My first look at the command ouput shows me that it applieds a partialing out estimator, with potential of other doubly robust methods. Interestingly, the core of CATE can be achieved using standard regression techniques with interactions.\nLet’s examine two approaches to estimate CATEs in Stata: using the new cate command and using standard regression with interactions.\n\nData Context: 401(k) Eligibility Effect on Net Financial Wealth\nWe’ll analyze the effect of 401(k) program eligibility on net financial assets, examining whether this effect varies across income categories.\n\n\nApproach 1: Using the New cate Command\nStata 19’s new cate command implements sophisticated methods for CATE estimation:\nwebuse assets3\nglobal catecovars age educ i.(incomecat pension married twoearn ira ownhome)\ncate po (assets $catecovars) (e401k), group(incomecat) nolog\nResults:\nConditional average treatment effects     Number of observations       = 9,913\nEstimator:       Partialing out           Number of folds in cross-fit =    10\nOutcome model:   Linear lasso             Number of outcome controls   =    17\nTreatment model: Logit lasso              Number of treatment controls =    17\nCATE model:      Random forest            Number of CATE variables     =    17\n\n---------------------------------------------------------------------------------------------\n                            |               Robust\n                     assets | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n----------------------------+----------------------------------------------------------------\nGATE                        |\n                  incomecat |\n                         0  |   4133.402   990.7697     4.17   0.000     2191.529    6075.275\n                         1  |   1499.306   1651.826     0.91   0.364    -1738.214    4736.827\n                         2  |   5094.275   1348.127     3.78   0.000     2451.996    7736.555\n                         3  |   8663.574   2286.673     3.79   0.000     4181.778    13145.37\n                         4  |   20509.95   4698.348     4.37   0.000     11301.36    29718.55\n----------------------------+----------------------------------------------------------------\nATE                         |\n                      e401k |\n(Eligible vs Not eligible)  |   7980.516   1148.136     6.95   0.000      5730.21    10230.82\n----------------------------+----------------------------------------------------------------\nThe cate po command uses the Partialing Out estimator with machine learning methods (random forest and lasso) to estimate both the ATE and Group Average Treatment Effects (GATEs).\n\n\nApproach 2: Using Standard Regression with Interactions\nWe can achieve comparable results using standard regression with full interactions:\n* you do not want to see all interactions.\nqui:reg assets i.incomecat##i.e401k##c.($catecovars), robust\n\n* Average Treatment Effect (ATE)\nmargins, at(e401k=(0 1)) noestimcheck contrast(atcontrast(r))\n\n* Group Average Treatment Effects (GATEs)\nmargins, at(e401k=(0 1)) noestimcheck contrast(atcontrast(r)) over(incomecat)\nResults for ATE:\n--------------------------------------------------------------\n             |            Delta-method\n             |   Contrast   std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n         _at |\n   (2 vs 1)  |   7642.407   1142.797      5402.291    9882.524\n--------------------------------------------------------------\nResults for GATEs:\n---------------------------------------------------------------\n              |            Delta-method\n              |   Contrast   std. err.     [95% conf. interval]\n--------------+------------------------------------------------\n_at@incomecat |\n  (2 vs 1) 0  |   3594.182   896.3298      1837.191    5351.172\n  (2 vs 1) 1  |     1283.3   1565.151     -1784.718    4351.317\n  (2 vs 1) 2  |   5056.144   1262.728      2580.938     7531.35\n  (2 vs 1) 3  |   8610.622   2320.156      4062.641     13158.6\n  (2 vs 1) 4  |   19665.61   4733.551      10386.88    28944.34\n---------------------------------------------------------------\n\n\nBonus: CATT Estimation\nBecause we have full control of margins, we can also estimate Treatment Effects on the Treated!\n* you do not want to see all interactions.\nqui:reg assets i.incomecat##i.e401k##c.($catecovars), robust\n\n* Average Treatment Effect (ATE)\nmargins,  subpop(if e401k==1 ) at(e401k=(0 1)) noestimcheck contrast(atcontrast(r))\n\n* Group Average Treatment Effects (GATEs)\nmargins,  subpop(if e401k==1 ) at(e401k=(0 1)) noestimcheck contrast(atcontrast(r)) over(incomecat)\nResults for ATE:\n--------------------------------------------------------------\n             |            Delta-method\n             |   Contrast   std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n         _at |\n   (2 vs 1)  |   10420.85   1618.697      7247.875    13593.83\n--------------------------------------------------------------\nResults for GATEs:\n---------------------------------------------------------------\n              |            Delta-method\n              |   Contrast   std. err.     [95% conf. interval]\n--------------+------------------------------------------------\n_at@incomecat |\n  (2 vs 1) 0  |   3666.243   1113.918      1482.736    5849.751\n  (2 vs 1) 1  |   2118.117   1437.205     -699.1003    4935.334\n  (2 vs 1) 2  |   5224.821   1229.951      2813.864    7635.778\n  (2 vs 1) 3  |   8780.722   2238.042        4393.7    13167.74\n  (2 vs 1) 4  |   20408.31   4633.656       11325.4    29491.23\n---------------------------------------------------------------",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics12.html#comparing-the-two-approaches",
    "href": "app_metrics/app_metrics12.html#comparing-the-two-approaches",
    "title": "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools",
    "section": "Comparing the Two Approaches",
    "text": "Comparing the Two Approaches\nLet’s compare the ATE and GATE estimates from both methods:\n\n\n\nIncome Category\nCATE Command\nBasic Regression\n\n\n\n\nOverall ATE\n7,980.52\n7,642.41\n\n\nIncome Cat 0\n4,133.40\n3,594.18\n\n\nIncome Cat 1\n1,499.31\n1,283.30\n\n\nIncome Cat 2\n5,094.28\n5,056.14\n\n\nIncome Cat 3\n8,663.57\n8,610.62\n\n\nIncome Cat 4\n20,509.95\n19,665.61\n\n\n\nThe estimates are remarkably similar! Both approaches reveal substantial heterogeneity in treatment effects across income categories, with the highest income group (category 4) experiencing treatment effects nearly 4-5 times larger than the lowest income groups.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics12.html#the-power-of-statas-fundamentals",
    "href": "app_metrics/app_metrics12.html#the-power-of-statas-fundamentals",
    "title": "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools",
    "section": "The Power of Stata’s Fundamentals",
    "text": "The Power of Stata’s Fundamentals\nWhile the new cate command offers sophisticated machine learning techniques, our traditional regression approach using margins yields comparable results. This demonstrates several important points:\n\nConceptual understanding matters: When you understand the underlying causal framework, you can implement sophisticated analyses using fundamental tools.\nFlexibility of regression with interactions: Full interaction models can capture complex heterogeneity patterns.\nPower of margins: Stata’s margins command is incredibly versatile, allowing for the calculation of various treatment effects with proper standard errors.\nTransparency: The regression approach makes the modeling assumptions explicit and transparent.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics12.html#conclusion",
    "href": "app_metrics/app_metrics12.html#conclusion",
    "title": "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools",
    "section": "Conclusion",
    "text": "Conclusion\nStata’s power lies not just in its specialized commands but in the flexibility and depth of its fundamental tools. While the new cate command provides a streamlined approach to modern causal inference techniques, understanding how to implement these concepts with basic commands gives you greater control and insight into your analysis.\nWhether you choose the specialized cate command or build your analysis with regression and margins depends on your specific needs, the dimensionality of your data, and your comfort with different estimation approaches. Either way, Stata provides powerful tools for uncovering treatment effect heterogeneity and informing policy decisions.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics12.html#appendix-the-math-behind-margins",
    "href": "app_metrics/app_metrics12.html#appendix-the-math-behind-margins",
    "title": "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools",
    "section": "Appendix: The Math Behind Margins",
    "text": "Appendix: The Math Behind Margins\nWhen using margins after our interaction model, Stata is computing:\n\\[E[Y_i | X_i = x, D_i = 1] - E[Y_i | X_i = x, D_i = 0]\\]\nFor each subgroup defined by incomecat. This is equivalent to estimating:\n\\[\\tau(x) = E[Y_i(1) - Y_i(0) | X_i = x]\\]\nUnder the conditional independence assumption. The margins command handles the complex delta-method calculations for standard errors automatically, accounting for the full covariance structure of the coefficient estimates.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Avg Treatment Effect Heterogeneity in Stata: Basic Commands vs. New CATE Tools"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html",
    "href": "app_metrics/app_metrics10.html",
    "title": "Nonlinear DID",
    "section": "",
    "text": "Differences in Differences (DiD) design is one of the most popular methods in applied microeconomics, because it requires relatively few assumptions to identify treatment effects.\n\nNo anticipation,\nParallel trends,\nNo spillovers\n\nThe canonical DiD, a 2x2 design, simply compares means (or conditional means) of the outcome variable (before after x treated non-treated) to identify treatment effects.\n\nThus it can be used even if outcome is a limited dependent variable (binary, count, etc) (parallel to the linear regression case)",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#introduction",
    "href": "app_metrics/app_metrics10.html#introduction",
    "title": "Nonlinear DID",
    "section": "",
    "text": "Differences in Differences (DiD) design is one of the most popular methods in applied microeconomics, because it requires relatively few assumptions to identify treatment effects.\n\nNo anticipation,\nParallel trends,\nNo spillovers\n\nThe canonical DiD, a 2x2 design, simply compares means (or conditional means) of the outcome variable (before after x treated non-treated) to identify treatment effects.\n\nThus it can be used even if outcome is a limited dependent variable (binary, count, etc) (parallel to the linear regression case)",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#section",
    "href": "app_metrics/app_metrics10.html#section",
    "title": "Nonlinear DID",
    "section": "",
    "text": "Because the Canonical design is rather limited, many extensions have been proposed to handle more complex scenarios: Staggered Treatment with GxT DID.\n\nEarly Extensions (infamous TWFE) have been shown to be problematic. (negative weights and bad controls)\nbut more recent approaches (See Roth et al. (2023)) have shown how can one better use these designs to identify treatment effects, avoiding the simple-TWFE problems.\n\nLinear models, however, have limitations:\n\nLinear models do a poor job interpolating and predicting LDV outcomes\nParallel trends assumptions may only be credible under specific functional forms",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#not-andrew-goodman-bacon",
    "href": "app_metrics/app_metrics10.html#not-andrew-goodman-bacon",
    "title": "Nonlinear DID",
    "section": "Not Andrew Goodman-Bacon",
    "text": "Not Andrew Goodman-Bacon\n\n\n\n\n\nAmong others, Andrew showed the problems of using TWFE in the presence of staggered adoption of the treatment.\nBecause of treatment timing, later treated units are compared to bad controls (early treated ones), in potential violation of the parallel trends assumption.\nThis also relates to negative weights.\nSee Goodman-Bacon (2021)",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#not-pedro-santanna",
    "href": "app_metrics/app_metrics10.html#not-pedro-santanna",
    "title": "Nonlinear DID",
    "section": "Not Pedro Sant’Anna",
    "text": "Not Pedro Sant’Anna\n\n\n\n\n\nPedro and Brantly proposed deconstructing the GxT problem. Consider only good 2x2 DD designs to identify Treatment effects in DID.\nAgregate them as needed to obtain the desired treatment effect (weighted Average). (dynamic, average, across time, across groups, etc)\nAlong with Jonathan Roth, discuss the problem of PTA and functional forms. Not all may be valid.\nsee Callaway and Sant’Anna (2021) and Roth and Sant’Anna (2023)",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#not-jeffrey-wooldridge",
    "href": "app_metrics/app_metrics10.html#not-jeffrey-wooldridge",
    "title": "Nonlinear DID",
    "section": "Not Jeffrey Wooldridge",
    "text": "Not Jeffrey Wooldridge\n\n\n\n\n\nJeff Wooldridge brought back life to the TWFE.\nThe problem was not the TWFE part of the analysis, but the model specification.\n\n(\\(post \\times treated\\) instead of \\(G \\times T\\))\n\nThis insights, can be extended to nonlinear cases.\nSee Wooldridge (2021) and Wooldridge (2023)",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#fernando-rios-avila",
    "href": "app_metrics/app_metrics10.html#fernando-rios-avila",
    "title": "Nonlinear DID",
    "section": "Fernando Rios-Avila",
    "text": "Fernando Rios-Avila\n\n\n\n\n\nI have followed some of the developments in DID with staggered adoption of the treatment.\n\nImplemented few things (drdid/csdid/csdid2/jwdid)\nUnderstood few others (comparison groups, efficiency, negative weights, nonlinear models)\n\nAnd today, I will be providing some of my insights regarding the empirical analysis of nonlinear DID.\n\nI will rely heavily on Wooldridge (2023),",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#basics-2x2-did",
    "href": "app_metrics/app_metrics10.html#basics-2x2-did",
    "title": "Nonlinear DID",
    "section": "Basics: 2x2 DiD",
    "text": "Basics: 2x2 DiD\n\nIn the 2x2 DID design, we have 2 groups:\n\nControl (\\(D=0\\)) and treated (\\(D=1\\)),\n\nWhich are observed for two periods of time:\n\nBefore (\\(T=0\\)) and after (\\(T=1\\)) the treatment.\n\nFor all groups and periods of time, we observe the realized outcome \\(Y_{i,t}\\), but cannot observe all potential outcomes \\(Y_{i,t}(D)\\).\nRealized outcomes are determined by the following equation:\n\n\\[Y_{i,t}=D_i Y_{i,t}(1) + (1-D_i) Y_{i,t}(0)\n\\]",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#section-1",
    "href": "app_metrics/app_metrics10.html#section-1",
    "title": "Nonlinear DID",
    "section": "",
    "text": "If treatment occured at some point between T0 and T1, and we could observe all potential outcomes, the estimate of interested, Average Treatment effect, could be estimated as follows:\n\\[ATT = E(Y_{i,1}(1) - Y_{i,1}(0)|D_i=1)\n\\]\nFor the treated, we observe \\(Y_{i,1}(1)\\), but cannot observe \\(Y_{i,t}(0)\\) (counterfactual), thus, to identify Treatment Effects, we need to impose some assumptions.\n\nPTA:\n\n\\[\\begin{aligned}\nE(Y_{i,1}(0) - Y_{i,0}(0)|D_i=1) &= E(Y_{i,1} - Y_{i,0}|D_i=0) \\\\\nE(Y_{i,1}(0)|D_i=1) &= E(Y_{i,0}(0)|D_i=1) + E(Y_{i,1} - Y_{i,0}|D_i=0)\n\\end{aligned}\n\\]\n\nNo Anticipation: \\[Y_{i,0}(1) =  Y_{i,0}(0)=Y_{i,0}\\]",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#section-2",
    "href": "app_metrics/app_metrics10.html#section-2",
    "title": "Nonlinear DID",
    "section": "",
    "text": "Thus, ATT can be estimated as follows:\n\\[\\begin{aligned}\nATT &= E(Y_{i,1}(1)|D_i=1)-E(Y_{i,1}(0)|D_i=1) \\\\\n&= E(Y_{i,1}|D_i=1)- \\Big(E(Y_{i,0}|D_i=1) + E(Y_{i,1} - Y_{i,0}|D_i=0)  \\Big) \\\\\n&= E(Y_{i,1}-Y_{i,0}|D_i=1)- E(Y_{i,1} - Y_{i,0}|D_i=0)  \n\\end{aligned}\n\\]\n\nAnd the Same could be done via Regressions:\n\n\\[y_{i,t} = \\beta_0 + \\beta_1 T + \\beta_2 D_i + \\theta (D_i \\times T) + \\epsilon_{i,t}\n\\]\n\nATT identification relies on the Parallel trend assumption.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#how-to-test-for-pta",
    "href": "app_metrics/app_metrics10.html#how-to-test-for-pta",
    "title": "Nonlinear DID",
    "section": "How to test for PTA?",
    "text": "How to test for PTA?\n\nPTA is a non-testable assumption, because we do not observe all potential outcomes.\nHowever, if we “move”, from the 2x2 design, it may be possible to test if PTA hold Before treatment.\nConsider a case of T periods of time, and that treatment happen at period G.\nSay we estimate the ATT comparing periods T and T-1, for any T&lt;G.\n\n\\[ATT(T) = E(Y_{i,T} - Y_{i,T-1}|D_i=1) - E(Y_{i,T} - Y_{i,T-1}|D_i=0)\n\\]\n\nIf there is no anticipation, and Parallel trends hold, then \\(ATT(T)=0 \\text{ if } T&lt;G\\)\n\nThis is what Callaway and Sant’Anna (2021) uses for PTA testing",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#how-to-test-for-pta-1",
    "href": "app_metrics/app_metrics10.html#how-to-test-for-pta-1",
    "title": "Nonlinear DID",
    "section": "How to test for PTA?",
    "text": "How to test for PTA?\n\nAlternatively, one could simply estimate all ATT’s using period G-1 as baseline period (long2 differences):\n\n\\[ATT^2(T) = E(Y_{i,T} - Y_{i,G-1}|D_i=1) - E(Y_{i,T} - Y_{i,G-1}|D_i=0)\n\\]\n\nAnd use all post-treatment periods to estimate the ATT (\\(T\\geq G\\))\nand use all pre-treatment periods to test for PTA (\\(T&lt;G\\))",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#section-3",
    "href": "app_metrics/app_metrics10.html#section-3",
    "title": "Nonlinear DID",
    "section": "",
    "text": "What if PTA does not hold?Wage PTA in levelsWage PTA in logs\n\n\n\nAs suggested by Wooldridge (2023), one of the reasons PTA may not hold is because we may be analyzing the wrong model.\n\nconsider two groups of workers, high and low earners, that experience the same wage growth. (parallel trends in relative terms)\nIf we observe wages at levels, parallel trends would be violated\nAnd Post treatment estimates will be missleading",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#pta-may-hold-for-gbar-y",
    "href": "app_metrics/app_metrics10.html#pta-may-hold-for-gbar-y",
    "title": "Nonlinear DID",
    "section": "PTA may hold for \\(G(\\bar Y)\\)",
    "text": "PTA may hold for \\(G(\\bar Y)\\)\nA similar story could be told about other types of transformations.\nIn general, it is possible that PTA hold for some other monotonic transformation of the outcome variable.\n\\[G^{-1}\\Big( E_1 [Y_{i,1}(0)] \\Big) -G^{-1}\\Big(E_1[ Y_{i,0}(0) ] \\Big)  \n= G^{-1}\\Big(E_0[Y_{i,1}]\\Big) - G^{-1}\\Big(E_0[Y_{i,0}]\\Big)\n\\]\nThis is very similar to the PTA assumption explored in Roth and Sant’Anna (2023).\n\\[E_1\\Big[g(Y_{i,1}(0))\\Big] - E_1\\Big[g(Y_{i,0}(0))\\Big] = E_0\\Big[g(Y_{i,1})\\Big] - E_0\\Big[g(Y_{i,0})\\Big]\n\\]\nWooldridge idea: It may be possible to identify ATTs using correct functional forms, through the latent index.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#how-do-things-change",
    "href": "app_metrics/app_metrics10.html#how-do-things-change",
    "title": "Nonlinear DID",
    "section": "How do things Change?",
    "text": "How do things Change?\n\nUsing this insight, we can go back to the 2x2 design, and see how things change.\n\nBefore:\n\\[E(y_{i,t}|D,T)=\\beta_0 + \\beta_1 T + \\beta_2 D_i + \\theta (D_i \\times T)\n\\]\nAfter:\n\\[E(y_{i,t}|D,T)=G\\Big(\\beta_0 + \\beta_1 T + \\beta_2 D_i + \\theta (D_i \\times T)\\Big)\n\\]\n\nFor the practitioner, the extended Nonlinear DID simply requires choosing a functional form that better fits the data.\n\nPoisson, logit, fractional regression, multinomial model, Linear model, etc",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#extension-i-adding-covariates",
    "href": "app_metrics/app_metrics10.html#extension-i-adding-covariates",
    "title": "Nonlinear DID",
    "section": "Extension I: Adding Covariates",
    "text": "Extension I: Adding Covariates\n\nMany papers in the literature consider the use of covariates in the estimation of the ATT.\nThe lessons from Sant’Anna and Zhao (2020):\n\nThe choosen covariates should be time fixed, to avoid contamination of the treatment effect.\nUsing covariates allows relaxing the parallel trends assumption: PTA hold for specific groups of individuals. (if not for the whole population due to compositional changes)\n\nIn the 2x2 DID-Regression, covariates can be added with interactions:\n\n\\[\\begin{aligned}\ny_{i,t} &= \\beta_0 + \\beta_1 D + \\beta_2 T + \\theta (D \\times T) \\\\\n&+ X\\gamma +  D \\times X\\gamma_d + T \\times X\\gamma_T + D \\times T (X-\\bar X )\\lambda + \\epsilon_{i,t}\n\\end{aligned}\n\\]\n\nThe same could be done in the nonlinear case",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#extension-ii-gxt-did",
    "href": "app_metrics/app_metrics10.html#extension-ii-gxt-did",
    "title": "Nonlinear DID",
    "section": "Extension II: GxT DiD",
    "text": "Extension II: GxT DiD\n\nThe 2x2 design is rather limited, because often people have access to multiple periods of time, with differential treatment timing. (staggered adoption of the treatment)\n\nI call this the GxT design (G groups, T periods of time)\n\nThe majority of the papers that analyze this case impose an additional assumption:\n\nTreatment is not Reversable: Once treated always treated\n\n\nNOTE: Because of the interactions required, adding covariates would rapidily “consume” degrees of freedom. (may be a problem with nonlinear models).\nHow to see this? --&gt; tab tvar gvar",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#section-4",
    "href": "app_metrics/app_metrics10.html#section-4",
    "title": "Nonlinear DID",
    "section": "",
    "text": "The Problem\n\nEarly extensions of the 2x2 design to the GxT design, relied on the TWFE estimator. \\[y_{i,t} = \\delta_i + \\delta_t + \\theta^{fe} D_{i,t} + e_{i,t}\\]\n\nwhere \\(D_{i,t}=1\\) only after treatment is in place, and zero otherwise.\n\nThis model has been shown to be problematic, because of How OLS estimates the \\(\\theta^{fe}\\) parameter.\n\n\\(\\theta^{fe}\\) is a weighted average of all possible 2x2 DID designs. Goodman-Bacon (2021)\nSome designs use early treated units as controls for late treated units, which might be a violation of the parallel trends assumption.\n\n(treated units effectively receiving negative weights) Goodman-Bacon (2021), Chaisemartin and D’Haultfœuille (2020) and Borusyak, Jaravel, and Spiess (2023).",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#section-5",
    "href": "app_metrics/app_metrics10.html#section-5",
    "title": "Nonlinear DID",
    "section": "",
    "text": "Avoiding the Problem\n\nCallaway and Sant’Anna (2021) proposes a simple solution: Deconstruct the problem into smaller pieces (2x2 DIDs), and aggregate them as needed.\nWooldridge (2021), however, proposes a different solution: Use the correct functional form to estimate the ATTs.\n\n\\[\\text{Instead of: }Y_{i,t} = \\delta_i + \\gamma_t  + \\theta^{fe} PT_{i,t} + \\epsilon_{i,t}\\]\n\\[\\text{Use: }Y_{i,t} = \\delta_i + \\gamma_t  + \\sum_{g \\in \\mathbb{G}} \\sum_{t=g}^T \\theta_{g,t} \\mathbb{1}(G=g,T=t) + \\epsilon_{i,t}\\]\n\nTheir Message: Embrace heterogeneity across time and cohorts.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#an-added-insight",
    "href": "app_metrics/app_metrics10.html#an-added-insight",
    "title": "Nonlinear DID",
    "section": "An added Insight",
    "text": "An added Insight\n\nThe approach proposed by Wooldridge (2021), is more efficient than Callaway and Sant’Anna (2021), because it uses all pre-treatment data to estimate the ATTs. (Callaway and Sant’Anna (2021) uses only one pre-treatment period)\nHowever, doing this doesn’t allow you to test for PTA directly, unless we use an alternative approach:\n\n\\[Y_{i,t} = \\delta_i + \\gamma_t  +\n\\sum_{g \\in \\mathbb{G}} \\sum_{t=t_0}^{g-2} \\theta_{g,t}^{pre} \\mathbb{1}(G=g,T=t) +\n\\sum_{g \\in \\mathbb{G}} \\sum_{t=g}^T \\theta_{g,t}^{post} \\mathbb{1}(G=g,T=t) + \\epsilon_{i,t}\\]\n\nThis specification is equivalent to Callaway and Sant’Anna (2021) and to Sun and Abraham (2021).\n\nIts explicily a regression (Wooldridge)\nand uses actual, instead of relative, time.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#implementing-nl-did-the-jw-way",
    "href": "app_metrics/app_metrics10.html#implementing-nl-did-the-jw-way",
    "title": "Nonlinear DID",
    "section": "Implementing NL-DID the JW way",
    "text": "Implementing NL-DID the JW way\n\nOne of the advantages of the approaches proposed by Wooldridge (2021) and Wooldridge (2023), is that they can be directly estimated using regressions.\nThe hard part is to construct all the interactions required for the model to work.\nAnd a second challenge is to aggregate the results.\n\njwdid\n\njwdid is a simple command that helps with the construction of all required interactions that could be used to implement Wooldridge approach.\nIt is flexible enough, in that it allows you to choose different estimators that would better fit your data.\nit comes with its own post estimation commands that can help you aggregate the results into simple ATT, dynamics effects, across periods, across years, etc.\nLets take it for a spin",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#command-syntax",
    "href": "app_metrics/app_metrics10.html#command-syntax",
    "title": "Nonlinear DID",
    "section": "Command Syntax",
    "text": "Command Syntax\n\njwdid - Command Name. In charge of getting all interactions -right-\n\ndepvar [indepvar] - Declare the dependent variables. Independent variables are optional. They should be time fixed.\n[if] [in] [weight], Declares sample and weights. Only PW is allowed.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#command-main-options",
    "href": "app_metrics/app_metrics10.html#command-main-options",
    "title": "Nonlinear DID",
    "section": "Command Main Options",
    "text": "Command Main Options\n\njwdid: main options\n\nivar(varname): Panel indicator. If not declared, command assumes one is using repeated cross sections.\ncluster(varname): Cluster variable. To request a clustered standard error other than at ivar level. Recommended with RC.\ntvar(varname) or time(varname): Required, Time variable. There are two ways to call it for compatability with csdid.\ngvar(varname): Group variable. Indicates the timing of when a unit has ben treated.\ntrtvar(varname): If using Panel data, one could instead provide the post-treatment dummy.\n\nIf data is repeated crossection, one requires using trgvar(varname) (Pseudo panel indicator).",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#extra-options",
    "href": "app_metrics/app_metrics10.html#extra-options",
    "title": "Nonlinear DID",
    "section": "Extra Options",
    "text": "Extra Options\n\ngroup: Requests using group fixed effects, instead of individual fixed effects (default)\nnever: Request to use alternative specification that allows to test for PTA. (default is to use the standard specification)\nLinear and Nonlinear models:\n\nmethod(command, options): Request to use a specific method to model the data. Default is using linear regression via reghdfe.\nthe option part allows you do include specific options for the method. (e.g. method(glm, link() family()))",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#example-1-min-wages-on-employment-cs-data",
    "href": "app_metrics/app_metrics10.html#example-1-min-wages-on-employment-cs-data",
    "title": "Nonlinear DID",
    "section": "Example 1: Min Wages on Employment CS data",
    "text": "Example 1: Min Wages on Employment CS data\n\nclear all\nqui:ssc install frause\nqui:frause mpdta, clear\nfrause mpdta, clear\njwdid lemp, ivar(county) tvar(year) gvar(first)\n\n(Written by R.              )\nWARNING: Singleton observations not dropped; statistical significance is biased\n&gt;  (link)\n(MWFE estimator converged in 2 iterations)\n\nHDFE Linear regression                            Number of obs   =      2,500\nAbsorbing 2 HDFE groups                           F(   7,    499) =       3.82\nStatistics robust to heteroskedasticity           Prob &gt; F        =     0.0005\n                                                  R-squared       =     0.9933\n                                                  Adj R-squared   =     0.9915\n                                                  Within R-sq.    =     0.0101\nNumber of clusters (countyreal) =        500      Root MSE        =     0.1389\n\n                           (Std. err. adjusted for 500 clusters in countyreal)\n------------------------------------------------------------------------------\n             |               Robust\n        lemp | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n first_treat#|\n        year#|\n    c.__tr__ |\n  2004 2004  |  -.0193724   .0223818    -0.87   0.387    -.0633465    .0246018\n  2004 2005  |  -.0783191   .0304878    -2.57   0.010    -.1382195   -.0184187\n  2004 2006  |  -.1360781   .0354555    -3.84   0.000    -.2057386   -.0664177\n  2004 2007  |  -.1047075   .0338743    -3.09   0.002    -.1712613   -.0381536\n  2006 2006  |   .0025139   .0199328     0.13   0.900    -.0366487    .0416765\n  2006 2007  |  -.0391927   .0240087    -1.63   0.103    -.0863634     .007978\n  2007 2007  |   -.043106   .0184311    -2.34   0.020    -.0793182   -.0068938\n             |\n       _cons |    5.77807    .001544  3742.17   0.000     5.775036    5.781103\n------------------------------------------------------------------------------\n\nAbsorbed degrees of freedom:\n-----------------------------------------------------+\n Absorbed FE | Categories  - Redundant  = Num. Coefs |\n-------------+---------------------------------------|\n  countyreal |       500         500           0    *|\n        year |         5           0           5     |\n-----------------------------------------------------+\n* = FE nested within cluster; treated as redundant for DoF computation\n\n\n\ngen emp = exp(lemp)\njwdid emp, ivar(county) tvar(year) gvar(first) method(poisson) \n\n\nIteration 0:   log pseudolikelihood = -2980537.4  \nIteration 1:   log pseudolikelihood = -2980526.5  \nIteration 2:   log pseudolikelihood = -2980526.5  \n\nPoisson regression                                      Number of obs =  2,500\n                                                        Wald chi2(14) = 225.38\nLog pseudolikelihood = -2980526.5                       Prob &gt; chi2   = 0.0000\n\n                           (Std. err. adjusted for 500 clusters in countyreal)\n------------------------------------------------------------------------------\n             |               Robust\n         emp | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n first_treat#|\n        year#|\n    c.__tr__ |\n  2004 2004  |  -.0080499   .0100858    -0.80   0.425    -.0278177    .0117178\n  2004 2005  |  -.0252131   .0176754    -1.43   0.154    -.0598562    .0094299\n  2004 2006  |   -.051965   .0197745    -2.63   0.009    -.0907222   -.0132077\n  2004 2007  |  -.0672208   .0192207    -3.50   0.000    -.1048926    -.029549\n  2006 2006  |    .055212   .0330023     1.67   0.094    -.0094714    .1198954\n  2006 2007  |   .0109993     .04294     0.26   0.798    -.0731617    .0951602\n  2007 2007  |   -.060675   .0149793    -4.05   0.000    -.0900339   -.0313161\n             |\n first_treat |\n       2004  |   .4789133   .4347691     1.10   0.271    -.3732184    1.331045\n       2006  |   .6010118   .3248861     1.85   0.064    -.0357532    1.237777\n       2007  |   .1269293   .2472938     0.51   0.608    -.3577576    .6116163\n             |\n        year |\n       2004  |  -.0459369   .0064592    -7.11   0.000    -.0585966   -.0332771\n       2005  |  -.0301284   .0094457    -3.19   0.001    -.0486416   -.0116152\n       2006  |  -.0030985   .0122001    -0.25   0.800    -.0270104    .0208133\n       2007  |   .0350031   .0118264     2.96   0.003     .0118238    .0581824\n             |\n       _cons |    6.84775   .1555219    44.03   0.000     6.542932    7.152567\n------------------------------------------------------------------------------\n\n\n\njwdid emp, ivar(county) tvar(year) gvar(first) method(poisson) never\n\n\nIteration 0:   log pseudolikelihood = -2980450.1  \nIteration 1:   log pseudolikelihood = -2980438.8  \nIteration 2:   log pseudolikelihood = -2980438.8  \n\nPoisson regression                                      Number of obs =  2,500\n                                                        Wald chi2(19) = 262.50\nLog pseudolikelihood = -2980438.8                       Prob &gt; chi2   = 0.0000\n\n                           (Std. err. adjusted for 500 clusters in countyreal)\n------------------------------------------------------------------------------\n             |               Robust\n         emp | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n first_treat#|\n        year#|\n    c.__tr__ |\n  2004 2004  |  -.0063802   .0111509    -0.57   0.567    -.0282356    .0154752\n  2004 2005  |   -.027483   .0190781    -1.44   0.150    -.0648753    .0099094\n  2004 2006  |  -.0641446   .0224733    -2.85   0.004    -.1081913   -.0200978\n  2004 2007  |  -.0704859   .0204208    -3.45   0.001    -.1105099   -.0304619\n  2006 2003  |  -.0081647   .0366458    -0.22   0.824     -.079989    .0636597\n  2006 2004  |  -.0289763   .0258763    -1.12   0.263    -.0796929    .0217403\n  2006 2006  |   .0310817   .0186723     1.66   0.096    -.0055153    .0676788\n  2006 2007  |  -.0042165   .0335939    -0.13   0.900    -.0700593    .0616263\n  2007 2003  |   .0358582   .0236868     1.51   0.130    -.0105671    .0822835\n  2007 2004  |   .0517571   .0164417     3.15   0.002      .019532    .0839822\n  2007 2005  |   .0237174   .0112134     2.12   0.034     .0017395    .0456952\n  2007 2007  |  -.0329817    .009941    -3.32   0.001    -.0524657   -.0134976\n             |\n first_treat |\n       2004  |   .4821784   .4354246     1.11   0.268    -.3712381    1.335595\n       2006  |   .6162276   .3252255     1.89   0.058    -.0212028    1.253658\n       2007  |    .099236    .244468     0.41   0.685    -.3799124    .5783844\n             |\n        year |\n       2004  |  -.0476066   .0080213    -5.94   0.000     -.063328   -.0318852\n       2005  |  -.0278586   .0118649    -2.35   0.019    -.0511134   -.0046038\n       2006  |    .009081    .016213     0.56   0.575    -.0226959     .040858\n       2007  |   .0382682   .0136908     2.80   0.005     .0114347    .0651017\n             |\n       _cons |   6.844485   .1573451    43.50   0.000     6.536094    7.152875\n------------------------------------------------------------------------------",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#example-1-aggregations",
    "href": "app_metrics/app_metrics10.html#example-1-aggregations",
    "title": "Nonlinear DID",
    "section": "Example 1: Aggregations",
    "text": "Example 1: Aggregations\n\nestat event\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   __event__ |\n         -4  |   37.84578   27.92442     1.36   0.175    -16.88508    92.57665\n         -3  |   36.91665   23.13761     1.60   0.111     -8.43224    82.26554\n         -2  |   7.462364   14.88422     0.50   0.616    -21.71018    36.63491\n         -1  |          0  (omitted)\n          0  |  -13.33389   12.32746    -1.08   0.279    -37.49527    10.82749\n          1  |  -18.42686   40.35552    -0.46   0.648    -97.52222     60.6685\n          2  |   -95.3188   36.54186    -2.61   0.009    -166.9395   -23.69806\n          3  |  -107.5067   44.45653    -2.42   0.016    -194.6399   -20.37347\n------------------------------------------------------------------------------\n\n\n\nestat group\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   __group__ |\n       2004  |  -63.03151   28.14208    -2.24   0.025     -118.189   -7.874052\n       2006  |   23.89072   46.37715     0.52   0.606    -67.00682    114.7883\n       2007  |  -34.94374   13.54265    -2.58   0.010    -61.48686   -8.400625\n------------------------------------------------------------------------------\n\n\n\nestat calendar\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n__calendar__ |\n       2004  |  -9.219455   15.81277    -0.58   0.560    -40.21192    21.77301\n       2005  |  -40.08113   29.74368    -1.35   0.178    -98.37767    18.21541\n       2006  |   5.147843   33.13277     0.16   0.877     -59.7912    70.08689\n       2007  |  -36.81546   16.84608    -2.19   0.029    -69.83316   -3.797752\n------------------------------------------------------------------------------\n\n\n\nqui:estat event, plot\ngraph export event.png, replace width(1000)",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#example-2-wooldridge-simulation-data",
    "href": "app_metrics/app_metrics10.html#example-2-wooldridge-simulation-data",
    "title": "Nonlinear DID",
    "section": "Example 2: Wooldridge Simulation data",
    "text": "Example 2: Wooldridge Simulation data\n\nclear all\nuse did_common_6_binary, clear\nqui {\nreg y i.w#c.d#c.f04 i.w#c.d#c.f05 i.w#c.d#c.f06 ///\n    i.w#c.d#c.f04#c.x i.w#c.d#c.f05#c.x i.w#c.d#c.f06#c.x ///\n    f02 f03 f04 f05 f06 ///\n    c.f02#c.x c.f03#c.x c.f04#c.x c.f05#c.x c.f06#c.x /// \n    d x c.d#c.x, noomitted vce(cluster id)\n  est sto m1\n}  \nqui:margins, dydx(w) at(f02 = 0 f03 = 0 f04 = 1 f05 = 0 f06 = 0) ///\n    subpop(if d == 1) noestimcheck vce(uncond)  post\nereturn display  \nest restore m1\nqui:margins, dydx(w) at(f02 = 0 f03 = 0 f04 = 0 f05 = 1 f06 = 0) ///\n    subpop(if d == 1) noestimcheck vce(uncond) post\nereturn display  \nest restore m1\nqui:margins, dydx(w) at(f02 = 0 f03 = 0 f04 = 0 f05 = 0 f06 = 1) ///\n    subpop(if d == 1) noestimcheck vce(uncond) post\nereturn display \n\n                                 (Std. err. adjusted for 1,000 clusters in id)\n------------------------------------------------------------------------------\n             |            Unconditional\n             | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         1.w |   .0462206   .0367939     1.26   0.209    -.0259816    .1184228\n------------------------------------------------------------------------------\n(results m1 are active now)\n                                 (Std. err. adjusted for 1,000 clusters in id)\n------------------------------------------------------------------------------\n             |            Unconditional\n             | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         1.w |   .0755069   .0376124     2.01   0.045     .0016985    .1493153\n------------------------------------------------------------------------------\n(results m1 are active now)\n                                 (Std. err. adjusted for 1,000 clusters in id)\n------------------------------------------------------------------------------\n             |            Unconditional\n             | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         1.w |   .0445457   .0379857     1.17   0.241    -.0299952    .1190866\n------------------------------------------------------------------------------\n\n\nUsing jwdid:\n\nclear all\nuse did_common_6_binary, clear\nqui: jwdid y x, ivar(id) tvar(year) trtvar(w) method(regress)\nestat event, vce(unconditional)\n\n                                 (Std. err. adjusted for 1,000 clusters in id)\n------------------------------------------------------------------------------\n             |            Unconditional\n             | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   __event__ |\n          0  |   .0462206   .0367939     1.26   0.209    -.0259816    .1184228\n          1  |   .0755069   .0376124     2.01   0.045     .0016985    .1493153\n          2  |   .0445457   .0379857     1.17   0.241    -.0299952    .1190866\n------------------------------------------------------------------------------\n\n\nUsing Logit\n\nclear all\nuse did_common_6_binary, clear\nqui {\nlogit y i.w#c.d#c.f04 i.w#c.d#c.f05 i.w#c.d#c.f06 ///\n    i.w#c.d#c.f04#c.x i.w#c.d#c.f05#c.x i.w#c.d#c.f06#c.x ///\n    f02 f03 f04 f05 f06 c.f02#c.x c.f03#c.x c.f04#c.x c.f05#c.x c.f06#c.x ///\n    d x c.d#c.x, noomitted vce(cluster id)\n  est store m1\n}  \n\nqui:margins, dydx(w) at(f02 = 0 f03 = 0 f04 = 1 f05 = 0 f06 = 0) ///\n    subpop(if d == 1) noestimcheck vce(uncond)  post\nereturn display  \nest restore m1\nqui:margins, dydx(w) at(f02 = 0 f03 = 0 f04 = 0 f05 = 1 f06 = 0) ///\n    subpop(if d == 1) noestimcheck vce(uncond) post\nereturn display  \nest restore m1\nqui:margins, dydx(w) at(f02 = 0 f03 = 0 f04 = 0 f05 = 0 f06 = 1) ///\n    subpop(if d == 1) noestimcheck vce(uncond) post\nereturn display \n\n                                 (Std. err. adjusted for 1,000 clusters in id)\n------------------------------------------------------------------------------\n             |            Unconditional\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         1.w |   .0886639   .0326848     2.71   0.007     .0246029    .1527249\n------------------------------------------------------------------------------\n(results m1 are active now)\n                                 (Std. err. adjusted for 1,000 clusters in id)\n------------------------------------------------------------------------------\n             |            Unconditional\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         1.w |   .1217999   .0355845     3.42   0.001     .0520556    .1915441\n------------------------------------------------------------------------------\n(results m1 are active now)\n                                 (Std. err. adjusted for 1,000 clusters in id)\n------------------------------------------------------------------------------\n             |            Unconditional\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         1.w |   .1073639   .0371242     2.89   0.004     .0346018    .1801261\n------------------------------------------------------------------------------\n\n\nUsing jwdid:\n\nclear all\nuse did_common_6_binary, clear\nqui: jwdid y x, ivar(id) tvar(year) trtvar(w) method(logit)\nestat event, vce(unconditional)\n\n                                 (Std. err. adjusted for 1,000 clusters in id)\n------------------------------------------------------------------------------\n             |            Unconditional\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   __event__ |\n          0  |   .0886639   .0326848     2.71   0.007     .0246029    .1527249\n          1  |   .1217999   .0355845     3.42   0.001     .0520556    .1915441\n          2  |   .1073639   .0371242     2.89   0.004     .0346018    .1801261\n------------------------------------------------------------------------------",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics10.html#conclusion",
    "href": "app_metrics/app_metrics10.html#conclusion",
    "title": "Nonlinear DID",
    "section": "Conclusion",
    "text": "Conclusion\n\nDID is a popular method for analyzing policy interventions.\nThanks to the contributions of Wooldridge, Roth and Sant’Anna among others, we have a better understanding of how to implement DID in more complex scenarios.\nOne of this important extensions is the use of nonlinear models to better fit the data, and better estimate treatment effects.\nThe jwdid command is a simple tool that can help you implement the Wooldridge approach to nonlinear DID.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Nonlinear DID"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the Site",
    "section": "",
    "text": "Welcome, and thanks for joining me.\nAs you will read in this site, I will cover series of random topics and questions in applied econometrics using my favorite tool Stata!.\nThat being said. This website is multipurpose.\nI started it to answer random questions of “how to” do things in Stata, that I had troubles with at some point. Spread the knowledge if you may. However it will also be used as a teaching tool for some simple and not-so-simple topics and strategies I teach in econometrics class.\nFinally, I will also use it as a platform to share my research, whenever possible, and pertinent.\nFeel free to drop me a line, comment or question.\nPerhaps most interesting of all. This site was rewritten, again, using Quarto! combined with Stata!\nYou can find me at: friosa@gmail.com\nOr follow me on tweeter: @friosavila\nAnd just to start with. A mini Example of what its to come\n\n\nCode\nsysuse auto, clear\nset scheme white2\nscatter price mpg\n\n\n\n\n\n(1978 automobile data)"
  },
  {
    "objectID": "app_metrics/app_metrics1.html",
    "href": "app_metrics/app_metrics1.html",
    "title": "Constructing synthetic Datasets",
    "section": "",
    "text": "In my current collaboration with Stephen Jenkins, we are grappling with the challenge of providing a self-contained replication package alongside our paper.\nIt’s relatively easy to share the code for our model estimations, including code developed by other authors. However, many researchers face the same challenge we do: how to distribute data that we’re not allowed to share due to privacy or proprietary reasons.\nIn fact, for this particular project, only Stephen has had access to the data. I’ve mainly worked on the code that estimates new models (for those interested, see references below).\nNow that it’s time to publish our “big” paper, we need a strategy to create a synthetic dataset that satisfies privacy protection constraints while still preserving the moments’ structure we care about, as well as those that others may find interesting.\nTo this end, I propose a simple strategy that could work: Multiple Imputation. While it may not be the best method available, I welcome any feedback or suggestions.\nTo explain how the method works, I’ll use the Swiss Labor Market Survey 1998 dataset, which is publicly available and used as an example dataset in the command -oaxaca- (Jann 2008).",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Constructing synthetic Datasets"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics1.html#introduction",
    "href": "app_metrics/app_metrics1.html#introduction",
    "title": "Constructing synthetic Datasets",
    "section": "",
    "text": "In my current collaboration with Stephen Jenkins, we are grappling with the challenge of providing a self-contained replication package alongside our paper.\nIt’s relatively easy to share the code for our model estimations, including code developed by other authors. However, many researchers face the same challenge we do: how to distribute data that we’re not allowed to share due to privacy or proprietary reasons.\nIn fact, for this particular project, only Stephen has had access to the data. I’ve mainly worked on the code that estimates new models (for those interested, see references below).\nNow that it’s time to publish our “big” paper, we need a strategy to create a synthetic dataset that satisfies privacy protection constraints while still preserving the moments’ structure we care about, as well as those that others may find interesting.\nTo this end, I propose a simple strategy that could work: Multiple Imputation. While it may not be the best method available, I welcome any feedback or suggestions.\nTo explain how the method works, I’ll use the Swiss Labor Market Survey 1998 dataset, which is publicly available and used as an example dataset in the command -oaxaca- (Jann 2008).",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Constructing synthetic Datasets"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics1.html#the-problem",
    "href": "app_metrics/app_metrics1.html#the-problem",
    "title": "Constructing synthetic Datasets",
    "section": "The Problem",
    "text": "The Problem\nAssume you signed a confidentiality agreement to work with Swiss Survey data and are ready to submit your work. However, you are required to provide a replication package with a code to produce the tables and the dataset itself. Since you cannot share the original data, you suggest generating 5 synthetic datasets instead. By doing so, people can apply your code and reach similar conclusions to your main paper, but with the advantage that the data is simulated, thus fulfilling privacy concerns.\nHere is a piece of code that can be used for that:\n\n\nCode\nframe reset\nset linesize 255\nuse http://fmwww.bc.edu/RePEc/bocode/o/oaxaca.dta, clear\nmisstable summarize\n\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n                                                               Obs&lt;.\n                                                +------------------------------\n               |                                | Unique\n      Variable |     Obs=.     Obs&gt;.     Obs&lt;.  | values        Min         Max\n  -------------+--------------------------------+------------------------------\n        lnwage |       213               1,434  |   &gt;500    .507681    5.259097\n         exper |       213               1,434  |   &gt;500          0    49.16667\n        tenure |       213               1,434  |    323          0    44.83333\n          isco |       213               1,434  |      9          1           9\n  -----------------------------------------------------------------------------\n\n\nFour variables (Wages, tenure, experience, and ISCO) have missing data when lfp=0 (people are not working).",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Constructing synthetic Datasets"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics1.html#the-solution",
    "href": "app_metrics/app_metrics1.html#the-solution",
    "title": "Constructing synthetic Datasets",
    "section": "The solution",
    "text": "The solution\nThe first step is to decide on the size of the synthetic dataset. You could create a dataset with the same number of observations or adjust it to your desired sample size. I will expand the dataset to double the size, tag the new observations and make all variables, except for lfp, missing. This is because some data is missing as it’s only available for those in the labor force. Alternatively, you could have created lfp using a random draw from a Bernoulli distribution with the same probability as the original data.\n\n\nCode\nexpand 2, gen(tag)\nforeach i of varlist lnwage educ exper tenure isco female age single married divorced kids6 kids714 wt {\n  qui:replace `i'=. if tag==1\n}\n\n\n(1,647 observations created)\n\n\nNext, create multiple imputed datasets using the predictive mean matching strategy. To do this, set the data and register all variables to be imputed. Then, impute all variables using chain pmm. Make sure none of the variables are collinear, and variables with structural missing data are specified separately. The only explanatory variable or exogenous variable here is lfp.\n\n\nCode\nmi set wide\nmi register impute lnwage educ exper tenure isco female age single married kids6 kids714 wt\nset seed 101\nqui:mi impute chain (pmm, knn(20))  educ female age single married kids6 kids714 wt (pmm if lfp==1, knn(20) ) lnwage  exper tenure isco  = lfp, add(5)\n\n\nYou now have 5 sets of variables that can be used to create unique synthetic datasets with a similar structure to the original confidential dataset. Let’s now put the newly created data into frames, so we can estimate few models and compare them with the original data.\n\n\nCode\nforvalues i = 1/5 {\n  frame put _`i'_* lfp if tag==1, into(fr`i')\n  frame fr`i':ren _`i'_* *\n}\nuse http://fmwww.bc.edu/RePEc/bocode/o/oaxaca.dta, clear\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Constructing synthetic Datasets"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics1.html#comparing-results",
    "href": "app_metrics/app_metrics1.html#comparing-results",
    "title": "Constructing synthetic Datasets",
    "section": "Comparing Results",
    "text": "Comparing Results\nYou can now estimate 4 models using the original data and the synthetic data.\nNow lets compare the models:\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal\nFake1\nFake2\nFake3\nFake4\nFake5\n\n\n\n\neduc\n0.085***\n0.076***\n0.059***\n0.077***\n0.069***\n0.063***\n\n\n\n(0.005)\n(0.005)\n(0.006)\n(0.005)\n(0.005)\n(0.005)\n\n\nexper\n0.011***\n0.010***\n0.011***\n0.012***\n0.006***\n0.009***\n\n\n\n(0.002)\n(0.001)\n(0.002)\n(0.002)\n(0.001)\n(0.001)\n\n\ntenure\n0.008***\n0.008***\n0.002\n0.005**\n0.007***\n0.005**\n\n\n\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n\n\nfemale\n-0.084***\n-0.027\n-0.136***\n-0.063*\n-0.056*\n-0.113***\n\n\n\n(0.025)\n(0.025)\n(0.026)\n(0.025)\n(0.023)\n(0.024)\n\n\n_cons\n2.213***\n2.297***\n2.580***\n2.336***\n2.464***\n2.529***\n\n\n\n(0.068)\n(0.068)\n(0.074)\n(0.070)\n(0.063)\n(0.064)\n\n\nN\n1434\n1434\n1434\n1434\n1434\n1434\n\n\n\nStandard errors in parentheses * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nQuantile Regression 10\n\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal\nFake1\nFake2\nFake3\nFake4\nFake5\n\n\n\n\neduc\n0.103***\n0.088***\n0.069***\n0.083***\n0.075***\n0.067***\n\n\n\n(0.017)\n(0.015)\n(0.018)\n(0.011)\n(0.011)\n(0.013)\n\n\nexper\n0.020***\n0.012**\n0.014**\n0.012***\n0.008*\n0.009*\n\n\n\n(0.005)\n(0.004)\n(0.005)\n(0.003)\n(0.003)\n(0.004)\n\n\ntenure\n0.001\n0.006\n0.004\n0.002\n0.006\n0.010*\n\n\n\n(0.006)\n(0.005)\n(0.006)\n(0.004)\n(0.004)\n(0.005)\n\n\nfemale\n-0.151\n0.022\n-0.128\n-0.019\n-0.161**\n-0.154*\n\n\n\n(0.081)\n(0.070)\n(0.079)\n(0.053)\n(0.054)\n(0.063)\n\n\n_cons\n1.462***\n1.681***\n1.939***\n1.835***\n1.971***\n1.994***\n\n\n\n(0.219)\n(0.193)\n(0.228)\n(0.149)\n(0.146)\n(0.171)\n\n\nN\n1434\n1434\n1434\n1434\n1434\n1434\n\n\n\nStandard errors in parentheses * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nQuantile Regression 90\n\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal\nFake1\nFake2\nFake3\nFake4\nFake5\n\n\n\n\neduc\n0.064***\n0.073***\n0.047***\n0.069***\n0.062***\n0.057***\n\n\n\n(0.009)\n(0.008)\n(0.009)\n(0.008)\n(0.008)\n(0.007)\n\n\nexper\n0.004\n0.009***\n0.009***\n0.011***\n0.003\n0.005**\n\n\n\n(0.003)\n(0.002)\n(0.003)\n(0.002)\n(0.002)\n(0.002)\n\n\ntenure\n0.008*\n0.009***\n-0.001\n0.008**\n0.012***\n0.005\n\n\n\n(0.003)\n(0.003)\n(0.003)\n(0.003)\n(0.003)\n(0.003)\n\n\nfemale\n-0.054\n-0.054\n-0.149***\n-0.052\n-0.009\n-0.106**\n\n\n\n(0.044)\n(0.035)\n(0.041)\n(0.039)\n(0.039)\n(0.033)\n\n\n_cons\n2.984***\n2.804***\n3.247***\n2.863***\n2.990***\n3.121***\n\n\n\n(0.119)\n(0.097)\n(0.118)\n(0.111)\n(0.106)\n(0.089)\n\n\nN\n1434\n1434\n1434\n1434\n1434\n1434\n\n\n\nStandard errors in parentheses * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nHeckman selection model\n\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal\nFake1\nFake2\nFake3\nFake4\nFake5\n\n\n\n\nlnwage\n\n\n\n\n\n\n\n\neduc\n0.072***\n0.066***\n0.052***\n0.068***\n0.059***\n0.057***\n\n\n\n(0.005)\n(0.005)\n(0.006)\n(0.005)\n(0.005)\n(0.005)\n\n\nexper\n0.002\n0.001\n-0.000\n0.004*\n-0.002\n0.002\n\n\n\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n\n\ntenure\n0.002\n0.003\n-0.002\n-0.000\n0.002\n0.001\n\n\n\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n\n\nfemale\n-0.105***\n-0.067**\n-0.189***\n-0.105***\n-0.096***\n-0.144***\n\n\n\n(0.029)\n(0.025)\n(0.026)\n(0.025)\n(0.024)\n(0.024)\n\n\nage\n0.015***\n0.013***\n0.015***\n0.012***\n0.013***\n0.011***\n\n\n\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n(0.002)\n\n\n_cons\n1.991***\n2.071***\n2.226***\n2.133***\n2.201***\n2.317***\n\n\n\n(0.073)\n(0.073)\n(0.081)\n(0.075)\n(0.069)\n(0.070)\n\n\nlfp\n\n\n\n\n\n\n\n\neduc\n0.149***\n0.055**\n0.024\n0.018\n0.064***\n0.028\n\n\n\n(0.028)\n(0.020)\n(0.020)\n(0.019)\n(0.019)\n(0.018)\n\n\nfemale\n-1.785***\n-0.074\n-0.246**\n-0.104\n-0.011\n-0.177\n\n\n\n(0.161)\n(0.091)\n(0.091)\n(0.088)\n(0.090)\n(0.090)\n\n\nage\n-0.039***\n-0.021***\n-0.023***\n-0.018***\n-0.025***\n-0.023***\n\n\n\n(0.007)\n(0.005)\n(0.005)\n(0.005)\n(0.005)\n(0.005)\n\n\nsingle\n-0.100\n-0.792**\n-0.586**\n-0.498**\n-0.863***\n-0.780**\n\n\n\n(0.231)\n(0.241)\n(0.201)\n(0.186)\n(0.229)\n(0.238)\n\n\nmarried\n-0.867***\n-0.929***\n-0.765***\n-0.489**\n-0.882***\n-1.017***\n\n\n\n(0.158)\n(0.219)\n(0.169)\n(0.160)\n(0.198)\n(0.213)\n\n\nkids6\n-0.716***\n-0.730***\n-0.648***\n-0.699***\n-0.790***\n-0.714***\n\n\n\n(0.082)\n(0.067)\n(0.064)\n(0.066)\n(0.069)\n(0.062)\n\n\nkids714\n-0.343***\n-0.378***\n-0.373***\n-0.281***\n-0.402***\n-0.201***\n\n\n\n(0.065)\n(0.059)\n(0.056)\n(0.058)\n(0.058)\n(0.058)\n\n\n_cons\n3.543***\n2.720***\n3.006***\n2.560***\n2.729***\n3.135***\n\n\n\n(0.486)\n(0.382)\n(0.388)\n(0.345)\n(0.383)\n(0.383)\n\n\n/mills\n\n\n\n\n\n\n\n\nlambda\n-0.123\n0.128*\n0.251***\n0.062\n0.206***\n0.077\n\n\n\n(0.065)\n(0.061)\n(0.065)\n(0.070)\n(0.057)\n(0.061)\n\n\nN\n1647\n1647\n1647\n1647\n1647\n1647\n\n\n\nStandard errors in parentheses * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nI wont spend too much time interpretting the models. However, it is important to notice that they do provide similar outcomes, except for quantile regressions. Still, with this, one could redistribute replication codes that use both the true data and synthetic data, providing transparency to the work.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Constructing synthetic Datasets"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics1.html#conclusions",
    "href": "app_metrics/app_metrics1.html#conclusions",
    "title": "Constructing synthetic Datasets",
    "section": "Conclusions",
    "text": "Conclusions\nAs evident from the analysis, the results of the synthetic dataset are not expected to perfectly replicate the original data due to the introduction of random errors. However, by keeping this in mind, we can create synthetic datasets like this one, along with two sets of results - one based on the actual data, and the other based on the synthetic dataset(s).\nThis should help providing replication packages with code and data, improving the transparency of research when using restricted data.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Constructing synthetic Datasets"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics1.html#references",
    "href": "app_metrics/app_metrics1.html#references",
    "title": "Constructing synthetic Datasets",
    "section": "References",
    "text": "References\n\n\nJann, Ben. 2008. “The Blinder–Oaxaca Decomposition for Linear Regression Models.” The Stata Journal 8 (4): 453–79. https://doi.org/10.1177/1536867X0800800401.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Constructing synthetic Datasets"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics11.html",
    "href": "app_metrics/app_metrics11.html",
    "title": "jwdid: A Stata command for the estimation of Difference-in-Differences models using ETWFE",
    "section": "",
    "text": "As I have presented elsewhere, over the last 5 years, there has been a large development of methodologies for the estimation of Average treatment effects in Difference-in-Differences (DID) models, that would avoid the problems of bad controls and negative weights that have been identified in the literature. In this note, I describe the Stata command jwdid that implements the estimation of DID models using the ETWFE estimator proposed by Wooldridge (2021). One of the main advantages this aproach is that by being a simple extension of the standard FE estimator, it can be easily modified and implemented and allow for other non-linear models. As described in Wooldridge (2023), the ETWFE estimator could be used, for example, to model cases where the dependent variable is binary (logit) or count data (poisson). A second advantage of the ETWFE estimator is that the estimation of the baseline model, is transparent as it does not require the use of specialized software, except for the estimation of fixed effects models. This is in contrast with other DID estimators like the ones proposed by Callaway and Sant’Anna (2021), De Chaisemartin and D’Haultfœuille (2020), or Borusyak, Jaravel, and Spiess (2024), where the bulk of the model estimation is done in the background, with the user having less control and understanding on what is being estimated.\nThanks to this transparency in model specification, Nagengast and Yotov (forthcoming) propose a large set of recommendations for the analysis of DID models in the context of international trade and Gravity models. On this regard, I present the command jwdid as a flexible command in Stata that allows to consider Nagengast and Yotov (forthcoming) and Nagengast, Rios-Avila, and Yotov (2024) recommendations for the estimation of DID models, in the framework of trade models. However, many of the features we have developed for this context can also be applied in different contexts. For more information, see Nagengast, Rios-Avila, and Yotov (2024) and the working paper that can be downloaded here.\nIn the rest of this note, I will focus on describing what the command does, and what exactly is the model that is being estimated. I will also describe the post estimation commands that are available for the user to estimate aggregated treatment effects.\n\n\nAs described in Wooldridge (2021), the baseline model for the estimation of the DID model using the ETWFE estimator is the following:\n\\[Y_{i,t} = \\alpha + \\sum_{g \\in G} \\sum_{t=g}^{T} \\theta_{g,t} D_{i,g,t} + \\xi_i + \\xi_t + \\varepsilon_{i,t}\n\\tag{1}\\]\nwhere \\(Y_{i,t}\\) is the dependent variable, \\(D_{i,g,t}\\) is a dummy that takes the value of 1 if the observation is in the treatment group \\(g\\), on period \\(t\\) and 0 otherwise. \\(G\\) is a set that indicates at what time treatment started for all observations, and \\(T\\) is the last period of the analysis.\n\\(\\xi_i\\) and \\(\\xi_t\\) are sets of fixed effects for the individual and time dimensions, respectively.1 In this setup, the \\(\\theta_{g,t}\\) coefficients represent the average treatment effect that the treatment group \\(g\\) experiences at time \\(t\\) (\\(ATT(g,t)\\)). As described in Wooldridge (2021), allowing for a flexible specification of the \\(\\theta_{g,t}\\) avoids the problem of bad controls and negative weights that have been identified in the literature as potential problems in the estimation of DID models using traditional TWFE estimators.\nThis command can be directly estimated with jwdid using the following syntax:\njwdid y, ivar(i) tvar(t) gvar(g)\nWhere y is the dependent variable, ivar(i) is used to identify the individual panel data dimension, tvar(t) identifies the time dimension, and gvar(g) identifies the treatment group. Specifically, for observation \\(i\\), \\(g\\) would take the value of zero if the panel observation is never treated (within the window of the analysis), and would take a value different from zero to indicate the year that treatment started for unit \\(i\\). Following standard assumptions, this specification assumes that the treatment is an absorbing status, meaning that once a unit is treated, it remains treated for the rest of the analysis.\nBy default, jwdid will estimate the baseline model Equation 1 using the reghdfe command (Correia 2016), assuming clustered standard errors at i level. If other level is desired, the user can specify the cluster(cvar) option. While the command does not impose the assumption that the data is a panel, the methodology is designed to work with panel data. In case of repeated crossection, one should instead use the following syntax:\njwdid y, tvar(t) gvar(g) [cluster(cvar)]\nBy excluding ivar(i), the command assumes data is a repeated crossection, proceeding to include group fixed effects only. The cluster(cvar) option is not required, but can be used to request Standard errors to be clusted at the level cvar.\nSpecifically, this command will estimate the following model:\n\\[Y_{i,t} = \\alpha + \\sum_{g \\in G} \\sum_{t=g}^{T} \\theta_{g,t} D_{i,g,t} + \\xi_g + \\xi_t + \\varepsilon_{i,t}\n\\tag{2}\\]\nThis model specification makes the implicit assumption that Parallel trends are satisfied, using all never treated and not-yet treated observations as controls (not included category) for the identification of treatment effects.\nIf one instead wants to relax this assumption, the user can specify the option never:\njwdid y, ivar(i) tvar(t) gvar(g) never\nWhich will estimate the following model:\n\\[Y_{i,t} = \\alpha +  \\sum_{g \\in G} \\sum_{t=t_0}^{g-1} \\theta^{pre}_{g,t} D_{i,g,t}+ \\sum_{g \\in G} \\sum_{t=g}^{T} \\theta^{post}_{g,t} D_{i,g,t} + \\xi_i + \\xi_t + \\varepsilon_{i,t}\n\\tag{3}\\]\nThis is in principle the same as strategy as the one proposed by Sun and Abraham (2021), allowing for full heterogeneity across all groups and all relative periods. This specification is also numerically identical to the one proposed by Callaway and Sant’Anna (2021), for the case where there are no covariates. In this case, the only observations that are used as controls are the ones that were never treated. In this specification, all \\(\\theta^{pre}_{g,t}\\) can be used to test for the parallel trends assumption, and all \\(\\theta^{post}_{g,t}\\) can be used to estimate the treatment effects.\n\n\n\nAs described in Wooldridge (2023), the standard ETWFE model described in Equation 1 or Equation 2 identifies the average treatment effect imposing a linear parallel trends assumption. However, such assumption may not be valid in cases, such as when the dependent variable follows some limited distribution. Roth and Sant’Anna (2023) discusses a similar problem, stating that the choice of transformation of the dependent variable is crucial for the identification of the average treatment effect, and only under certain conditions would the ATT be identified for any transformation.\nIn this regard, Wooldridge (2023) proposes that the linear ETWFE models can be adapted to allow for non-linear models, by simply imposing the linear PTA assumption only on the latent variable of the model, but not the outcome itself.\nConsider the following transformation of the model defined by Equation 3:\n\\[E(Y_{i,t}|X,\\xi_i,\\xi_t) = H\\left(\\alpha +  \\sum_{g \\in G} \\sum_{t=t_0}^{g-1} \\theta^{pre}_{g,t} D_{i,g,t}+ \\sum_{g \\in G} \\sum_{t=g}^{T} \\theta^{post}_{g,t} D_{i,g,t} + \\xi_i + \\xi_t \\right)\n\\tag{4}\\]\nThis specification focuses on identifying the conditional expected value of the outcome of interest as function of the treatment status, and the individual and time fixed effects. If we assume the \\(H()\\) is the identify function, we would be back to the linear model described by Equation 3. However, if we assume that \\(H()\\) is a non-linear function, like exponetial for poisson, or logistic for logit models, we could estimate the average treatment effect under different assumptions, imposing only linear PTA on the latent variable of the model.\nThe jwdid command allows the user to specify the method() option to estimate models described by Equation 4, where one would specificy the regression model to be estimated, followed by the options associated with that model. For example, if we would be interested in estimating a poisson model, we would use the following syntax:\njwdid y, ivar(i) tvar(t) gvar(g) never method(poisson)\nThere is no restrictions on the type method one can use with the jwdid command, but it has not been tested with all possible models. The user should be aware that the method() option is passed directly for the model estimation step, and the user should be familiar with the syntax of the model being estimated.\nIt should be noted that when estimating non-linear models with a large number of fixed effects, one may face an incidental parameters problem. This is not generally a problem for the linear case, because the parameters of interest can be identified without explicitly estimating the fixed effects, using, for example the within transformation. However, with the exception of poisson models, fixed effects are generally estimated raising the possibility of incidental parameters. To reduce the impact of this problem, whenver method() is specified jwdid will incorporate group fixed effects, instead of individual fixed effects.\nFor the linear case with balanced data, using group instead of individual fixed effects provides numerically identical results. If panel is unbalanced the results will not be identical. In such cases, the option corr will create additional variables that address the difference. In the case of non-linear models, the best solution is to use group fixed effects. However, if one is interested in poisson models, the alterantive to group fixed effects is to use ppmlhdfe (Correia, Guimarães, and Zylkin (2020)).2 This is the state-of-the-art estimator for poisson models with fixed effects, and it is the recommended estimator for trade analysis.\n\n\n\nAs described in Wooldridge (2021), it is possible to include covariates in the model, by simply adding corrections that enable to easily identify the average treatment effect. However, following the literature on DID models, the implicit assumption is that covariates are time-invariant. jwdid does not impose any assumption on the covariates, but the user should be aware of the implications.\nIn general, when covariates are considered, the model of interest is similar to Equation 3, but adjusted for covariates:\n\\[\n\\begin{aligned}\nY_{i,t} &= \\alpha +  \\sum_{g \\in G} \\sum_{t=t_0}^{g-1} \\theta^{pre}_{g,t} D_{i,g,t}+\n\\sum_{g \\in G} \\sum_{t=g}^{T} \\theta^{post}_{g,t} D_{i,g,t} + \\sum_{g \\in G} \\sum_{t=t_0}^{g-1} D_{i,g,t} \\tilde x_{i}'\\beta^{pre}_{g,t} +\n\\sum_{g \\in G} \\sum_{t=g}^{T} D_{i,g,t} \\tilde x_{i}'\\beta^{post}_{g,t} \\\\\n&+ x_{i}'\\beta + \\sum_{t=t_0}^{T} D_{i,t} x_{i}'\\beta_t + \\sum_{g\\in G} D_{i,g} x_{i}'\\beta_g\n\\xi_i + \\xi_t + \\varepsilon_{i,t}\n\\end{aligned} \\tag{5}\\]\nWhere \\(D_{i,t}\\) is a dummy variable that is equal to 1 if period is equal to \\(t\\), and \\(D_{i,g}\\) is a dummmy variable that is equal to 1 if the group membership is equal to \\(g\\), and zero otherwise. \\(\\tilde x_{i}\\) are the within cohort and period demeaned variables. When using reghdfe or ppmlhdfe the term \\(\\sum_{g\\in G} D_{i,g} x_{i}'\\beta_g\\) is ommitted if all covariates are time constant.\nUsing \\(\\tilde x\\), default option, is not necessarity for the estimation of treatment effects. However, if one uses that specification, the parameters \\(\\theta^{pre}_{g,t}\\) and \\(\\theta^{post}_{g,t}\\) still identify the average treatment effect for group \\(g\\) at time \\(t\\). One could also use the untransformed covariates in Equation 5, and still be able to obtained the same group/time specific treatment effects with the post estimation commands.\nFrom the user persective, jwdid would simply need to be called as follows:\njwdid y x, ivar(i) tvar(t) gvar(g) never [xasis]\nWhere x are all covariates of interest. If one uses the option xasis, the command will use the covariates without demeaning them, which may save some computation time.\n\n\n\nAs it may be aparent from Equation 5, the number of estimated parameters can grow quickly with the number groups/cohorts, periods of analysis, and covariates. This could lead to increasing computational burden of the estimation. An alternative, which is already implemented via xthdidregress and hdidregress in Stata 18, is to estimate the model that reduces the heterogeneity of the treatment effects. Specifically, it allows treatment effects to vary across cohorts, across absolute time, or across relative time.\nFor the case without covariates, the specification of Equation 3 can be modified to impose the heterogeneity restrictions as follows:\nTime heterogeneity:\n\\[Y_{i,t} = \\alpha +  \\sum_{t=t_0}^T \\theta^{pre}_{t} D_{i,t,pre} + \\sum_{t=t_0}^T \\theta^{post}_{t}  D_{i,t,post}+ \\xi_i + \\xi_t + \\varepsilon_{i,t}\n\\tag{6}\\]\nCohort heterogeneity:\n\\[Y_{i,t} = \\alpha +  \\sum_{g\\in G} \\theta^{pre}_{g}  D_{i,g,pre}+ \\sum_{g\\in G} \\theta^{post}_{g} D_{i,g,post} + \\xi_i + \\xi_t+\\varepsilon_{i,t}\n\\tag{7}\\]\nEvent (Relative Time) heterogeneity:\n\\[Y_{i,t} = \\alpha +  \\sum_{e = E_{min}}^{-2} \\theta_{e} D_{i,e}+ \\sum_{e = 0}^{E_{max}} \\theta_{e} D_{i,e} + \\xi_i + \\xi_t+\\varepsilon_{i,t}\n\\tag{8}\\]\nWhere \\(D_{i,t,pre}\\) and \\(D_{i,t,post}\\) are dummies that take the value of 1 if observation \\(i\\), which is part of an eventually treated group, is not yet treated or is already treated at time \\(t\\), respectively. \\(D_{i,g,pre}\\) and \\(D_{i,g,post}\\) are dummies that take the value of 1 if observation \\(i\\) belongs to group \\(g\\) and is not yet treated or is already treated at time \\(t\\), respectively. \\(D_{i,e}\\) is a dummy that takes the value of 1 if observation \\(i\\) is \\(e\\) periods relative to when treatment started. \\(E_{min}\\) and \\(E_{max}\\) are the minimum and maximum event periods, possible. The pre coefficients are only considered when the never option is used.\nThe jwdid command allows the user to specify each one of these restrictions using the hettype() option.\njwdid y, ivar(i) tvar(t) gvar(g) hettype(option)\nWhere option can be time, cohort, or event. If no option is selected, the command will estimate the model described by Equation 3 which is the equivalent to allowing for full Cohort-time heterogeneity.\n\n\n\nAs described in Section 1.3, when covariates are considered in the model, the default option is to interact all covariates (or the demeaned transformations) with the same level of covariate heterogeneity. Some times, however, one may not be interested in estimating the same level of heterogeneity for all covariates. It may be possible, for example, to consider separate sets of covariates that could be interacted only with the time or the group dimensions.\nSpecifically, assume there are no variables we wish to consider for the treatment heterogeneity, but instead consider three sets of covariates: \\(x^{EX}\\) or variables we wish to incorporate without further interactions, \\(x^{T}\\) or variables that would be interacted with the Time variables only, and \\(x^{G}\\) or variables that would be interacted with group indicators only. In this case the setup would be:\n\\[\\begin{aligned}\nY_{i,t} &= \\alpha +  \\sum_{g \\in G} \\sum_{t=t_0}^{g-1} \\theta^{pre}_{g,t} D_{i,g,t}+\n\\sum_{g \\in G} \\sum_{t=g}^{T} \\theta^{post}_{g,t} D_{i,g,t} \\\\\n&+ x'^{EX}_{i}\\beta + \\sum_{t=t_0}^{T} D_{i,t} x'^{T}_{i}\\beta_t + \\sum_{g\\in G} D_{i,g} x'^{G}_{i}\\beta_g\n\\xi_i + \\xi_t + \\varepsilon_{i,t}\n\\end{aligned}\n\\tag{9}\\]\nThis specification can be estimated using the following syntax:\njwdid y , ivar(i) tvar(t) gvar(g) never exogvar(x_ex) xtvar(x_t) xgvar(x_g)\nIf covariates are included after y, they would still be treated following the specification of Equation 5, or following any of the heterogeneity restrictions described in Section 1.4.\nAn advanced version of this option is the inclusion of high order fixed effects (and interactions with fixed effects) that are different from the individual and time fixed effects. It is possible to request the inclusion of those types of fixed effects using the option fevar(), which is only valid if one is using the default estimator method reghdfe or ppmlhdfe. In both cases, the additional fixed effects (or interactions) are included in the estimation of the model without further interactions.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "**`jwdid`**: A Stata command for the estimation of Difference-in-Differences models using ETWFE"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics11.html#estimation-of-did-models-using-etwfe",
    "href": "app_metrics/app_metrics11.html#estimation-of-did-models-using-etwfe",
    "title": "jwdid: A Stata command for the estimation of Difference-in-Differences models using ETWFE",
    "section": "",
    "text": "As I have presented elsewhere, over the last 5 years, there has been a large development of methodologies for the estimation of Average treatment effects in Difference-in-Differences (DID) models, that would avoid the problems of bad controls and negative weights that have been identified in the literature. In this note, I describe the Stata command jwdid that implements the estimation of DID models using the ETWFE estimator proposed by Wooldridge (2021). One of the main advantages this aproach is that by being a simple extension of the standard FE estimator, it can be easily modified and implemented and allow for other non-linear models. As described in Wooldridge (2023), the ETWFE estimator could be used, for example, to model cases where the dependent variable is binary (logit) or count data (poisson). A second advantage of the ETWFE estimator is that the estimation of the baseline model, is transparent as it does not require the use of specialized software, except for the estimation of fixed effects models. This is in contrast with other DID estimators like the ones proposed by Callaway and Sant’Anna (2021), De Chaisemartin and D’Haultfœuille (2020), or Borusyak, Jaravel, and Spiess (2024), where the bulk of the model estimation is done in the background, with the user having less control and understanding on what is being estimated.\nThanks to this transparency in model specification, Nagengast and Yotov (forthcoming) propose a large set of recommendations for the analysis of DID models in the context of international trade and Gravity models. On this regard, I present the command jwdid as a flexible command in Stata that allows to consider Nagengast and Yotov (forthcoming) and Nagengast, Rios-Avila, and Yotov (2024) recommendations for the estimation of DID models, in the framework of trade models. However, many of the features we have developed for this context can also be applied in different contexts. For more information, see Nagengast, Rios-Avila, and Yotov (2024) and the working paper that can be downloaded here.\nIn the rest of this note, I will focus on describing what the command does, and what exactly is the model that is being estimated. I will also describe the post estimation commands that are available for the user to estimate aggregated treatment effects.\n\n\nAs described in Wooldridge (2021), the baseline model for the estimation of the DID model using the ETWFE estimator is the following:\n\\[Y_{i,t} = \\alpha + \\sum_{g \\in G} \\sum_{t=g}^{T} \\theta_{g,t} D_{i,g,t} + \\xi_i + \\xi_t + \\varepsilon_{i,t}\n\\tag{1}\\]\nwhere \\(Y_{i,t}\\) is the dependent variable, \\(D_{i,g,t}\\) is a dummy that takes the value of 1 if the observation is in the treatment group \\(g\\), on period \\(t\\) and 0 otherwise. \\(G\\) is a set that indicates at what time treatment started for all observations, and \\(T\\) is the last period of the analysis.\n\\(\\xi_i\\) and \\(\\xi_t\\) are sets of fixed effects for the individual and time dimensions, respectively.1 In this setup, the \\(\\theta_{g,t}\\) coefficients represent the average treatment effect that the treatment group \\(g\\) experiences at time \\(t\\) (\\(ATT(g,t)\\)). As described in Wooldridge (2021), allowing for a flexible specification of the \\(\\theta_{g,t}\\) avoids the problem of bad controls and negative weights that have been identified in the literature as potential problems in the estimation of DID models using traditional TWFE estimators.\nThis command can be directly estimated with jwdid using the following syntax:\njwdid y, ivar(i) tvar(t) gvar(g)\nWhere y is the dependent variable, ivar(i) is used to identify the individual panel data dimension, tvar(t) identifies the time dimension, and gvar(g) identifies the treatment group. Specifically, for observation \\(i\\), \\(g\\) would take the value of zero if the panel observation is never treated (within the window of the analysis), and would take a value different from zero to indicate the year that treatment started for unit \\(i\\). Following standard assumptions, this specification assumes that the treatment is an absorbing status, meaning that once a unit is treated, it remains treated for the rest of the analysis.\nBy default, jwdid will estimate the baseline model Equation 1 using the reghdfe command (Correia 2016), assuming clustered standard errors at i level. If other level is desired, the user can specify the cluster(cvar) option. While the command does not impose the assumption that the data is a panel, the methodology is designed to work with panel data. In case of repeated crossection, one should instead use the following syntax:\njwdid y, tvar(t) gvar(g) [cluster(cvar)]\nBy excluding ivar(i), the command assumes data is a repeated crossection, proceeding to include group fixed effects only. The cluster(cvar) option is not required, but can be used to request Standard errors to be clusted at the level cvar.\nSpecifically, this command will estimate the following model:\n\\[Y_{i,t} = \\alpha + \\sum_{g \\in G} \\sum_{t=g}^{T} \\theta_{g,t} D_{i,g,t} + \\xi_g + \\xi_t + \\varepsilon_{i,t}\n\\tag{2}\\]\nThis model specification makes the implicit assumption that Parallel trends are satisfied, using all never treated and not-yet treated observations as controls (not included category) for the identification of treatment effects.\nIf one instead wants to relax this assumption, the user can specify the option never:\njwdid y, ivar(i) tvar(t) gvar(g) never\nWhich will estimate the following model:\n\\[Y_{i,t} = \\alpha +  \\sum_{g \\in G} \\sum_{t=t_0}^{g-1} \\theta^{pre}_{g,t} D_{i,g,t}+ \\sum_{g \\in G} \\sum_{t=g}^{T} \\theta^{post}_{g,t} D_{i,g,t} + \\xi_i + \\xi_t + \\varepsilon_{i,t}\n\\tag{3}\\]\nThis is in principle the same as strategy as the one proposed by Sun and Abraham (2021), allowing for full heterogeneity across all groups and all relative periods. This specification is also numerically identical to the one proposed by Callaway and Sant’Anna (2021), for the case where there are no covariates. In this case, the only observations that are used as controls are the ones that were never treated. In this specification, all \\(\\theta^{pre}_{g,t}\\) can be used to test for the parallel trends assumption, and all \\(\\theta^{post}_{g,t}\\) can be used to estimate the treatment effects.\n\n\n\nAs described in Wooldridge (2023), the standard ETWFE model described in Equation 1 or Equation 2 identifies the average treatment effect imposing a linear parallel trends assumption. However, such assumption may not be valid in cases, such as when the dependent variable follows some limited distribution. Roth and Sant’Anna (2023) discusses a similar problem, stating that the choice of transformation of the dependent variable is crucial for the identification of the average treatment effect, and only under certain conditions would the ATT be identified for any transformation.\nIn this regard, Wooldridge (2023) proposes that the linear ETWFE models can be adapted to allow for non-linear models, by simply imposing the linear PTA assumption only on the latent variable of the model, but not the outcome itself.\nConsider the following transformation of the model defined by Equation 3:\n\\[E(Y_{i,t}|X,\\xi_i,\\xi_t) = H\\left(\\alpha +  \\sum_{g \\in G} \\sum_{t=t_0}^{g-1} \\theta^{pre}_{g,t} D_{i,g,t}+ \\sum_{g \\in G} \\sum_{t=g}^{T} \\theta^{post}_{g,t} D_{i,g,t} + \\xi_i + \\xi_t \\right)\n\\tag{4}\\]\nThis specification focuses on identifying the conditional expected value of the outcome of interest as function of the treatment status, and the individual and time fixed effects. If we assume the \\(H()\\) is the identify function, we would be back to the linear model described by Equation 3. However, if we assume that \\(H()\\) is a non-linear function, like exponetial for poisson, or logistic for logit models, we could estimate the average treatment effect under different assumptions, imposing only linear PTA on the latent variable of the model.\nThe jwdid command allows the user to specify the method() option to estimate models described by Equation 4, where one would specificy the regression model to be estimated, followed by the options associated with that model. For example, if we would be interested in estimating a poisson model, we would use the following syntax:\njwdid y, ivar(i) tvar(t) gvar(g) never method(poisson)\nThere is no restrictions on the type method one can use with the jwdid command, but it has not been tested with all possible models. The user should be aware that the method() option is passed directly for the model estimation step, and the user should be familiar with the syntax of the model being estimated.\nIt should be noted that when estimating non-linear models with a large number of fixed effects, one may face an incidental parameters problem. This is not generally a problem for the linear case, because the parameters of interest can be identified without explicitly estimating the fixed effects, using, for example the within transformation. However, with the exception of poisson models, fixed effects are generally estimated raising the possibility of incidental parameters. To reduce the impact of this problem, whenver method() is specified jwdid will incorporate group fixed effects, instead of individual fixed effects.\nFor the linear case with balanced data, using group instead of individual fixed effects provides numerically identical results. If panel is unbalanced the results will not be identical. In such cases, the option corr will create additional variables that address the difference. In the case of non-linear models, the best solution is to use group fixed effects. However, if one is interested in poisson models, the alterantive to group fixed effects is to use ppmlhdfe (Correia, Guimarães, and Zylkin (2020)).2 This is the state-of-the-art estimator for poisson models with fixed effects, and it is the recommended estimator for trade analysis.\n\n\n\nAs described in Wooldridge (2021), it is possible to include covariates in the model, by simply adding corrections that enable to easily identify the average treatment effect. However, following the literature on DID models, the implicit assumption is that covariates are time-invariant. jwdid does not impose any assumption on the covariates, but the user should be aware of the implications.\nIn general, when covariates are considered, the model of interest is similar to Equation 3, but adjusted for covariates:\n\\[\n\\begin{aligned}\nY_{i,t} &= \\alpha +  \\sum_{g \\in G} \\sum_{t=t_0}^{g-1} \\theta^{pre}_{g,t} D_{i,g,t}+\n\\sum_{g \\in G} \\sum_{t=g}^{T} \\theta^{post}_{g,t} D_{i,g,t} + \\sum_{g \\in G} \\sum_{t=t_0}^{g-1} D_{i,g,t} \\tilde x_{i}'\\beta^{pre}_{g,t} +\n\\sum_{g \\in G} \\sum_{t=g}^{T} D_{i,g,t} \\tilde x_{i}'\\beta^{post}_{g,t} \\\\\n&+ x_{i}'\\beta + \\sum_{t=t_0}^{T} D_{i,t} x_{i}'\\beta_t + \\sum_{g\\in G} D_{i,g} x_{i}'\\beta_g\n\\xi_i + \\xi_t + \\varepsilon_{i,t}\n\\end{aligned} \\tag{5}\\]\nWhere \\(D_{i,t}\\) is a dummy variable that is equal to 1 if period is equal to \\(t\\), and \\(D_{i,g}\\) is a dummmy variable that is equal to 1 if the group membership is equal to \\(g\\), and zero otherwise. \\(\\tilde x_{i}\\) are the within cohort and period demeaned variables. When using reghdfe or ppmlhdfe the term \\(\\sum_{g\\in G} D_{i,g} x_{i}'\\beta_g\\) is ommitted if all covariates are time constant.\nUsing \\(\\tilde x\\), default option, is not necessarity for the estimation of treatment effects. However, if one uses that specification, the parameters \\(\\theta^{pre}_{g,t}\\) and \\(\\theta^{post}_{g,t}\\) still identify the average treatment effect for group \\(g\\) at time \\(t\\). One could also use the untransformed covariates in Equation 5, and still be able to obtained the same group/time specific treatment effects with the post estimation commands.\nFrom the user persective, jwdid would simply need to be called as follows:\njwdid y x, ivar(i) tvar(t) gvar(g) never [xasis]\nWhere x are all covariates of interest. If one uses the option xasis, the command will use the covariates without demeaning them, which may save some computation time.\n\n\n\nAs it may be aparent from Equation 5, the number of estimated parameters can grow quickly with the number groups/cohorts, periods of analysis, and covariates. This could lead to increasing computational burden of the estimation. An alternative, which is already implemented via xthdidregress and hdidregress in Stata 18, is to estimate the model that reduces the heterogeneity of the treatment effects. Specifically, it allows treatment effects to vary across cohorts, across absolute time, or across relative time.\nFor the case without covariates, the specification of Equation 3 can be modified to impose the heterogeneity restrictions as follows:\nTime heterogeneity:\n\\[Y_{i,t} = \\alpha +  \\sum_{t=t_0}^T \\theta^{pre}_{t} D_{i,t,pre} + \\sum_{t=t_0}^T \\theta^{post}_{t}  D_{i,t,post}+ \\xi_i + \\xi_t + \\varepsilon_{i,t}\n\\tag{6}\\]\nCohort heterogeneity:\n\\[Y_{i,t} = \\alpha +  \\sum_{g\\in G} \\theta^{pre}_{g}  D_{i,g,pre}+ \\sum_{g\\in G} \\theta^{post}_{g} D_{i,g,post} + \\xi_i + \\xi_t+\\varepsilon_{i,t}\n\\tag{7}\\]\nEvent (Relative Time) heterogeneity:\n\\[Y_{i,t} = \\alpha +  \\sum_{e = E_{min}}^{-2} \\theta_{e} D_{i,e}+ \\sum_{e = 0}^{E_{max}} \\theta_{e} D_{i,e} + \\xi_i + \\xi_t+\\varepsilon_{i,t}\n\\tag{8}\\]\nWhere \\(D_{i,t,pre}\\) and \\(D_{i,t,post}\\) are dummies that take the value of 1 if observation \\(i\\), which is part of an eventually treated group, is not yet treated or is already treated at time \\(t\\), respectively. \\(D_{i,g,pre}\\) and \\(D_{i,g,post}\\) are dummies that take the value of 1 if observation \\(i\\) belongs to group \\(g\\) and is not yet treated or is already treated at time \\(t\\), respectively. \\(D_{i,e}\\) is a dummy that takes the value of 1 if observation \\(i\\) is \\(e\\) periods relative to when treatment started. \\(E_{min}\\) and \\(E_{max}\\) are the minimum and maximum event periods, possible. The pre coefficients are only considered when the never option is used.\nThe jwdid command allows the user to specify each one of these restrictions using the hettype() option.\njwdid y, ivar(i) tvar(t) gvar(g) hettype(option)\nWhere option can be time, cohort, or event. If no option is selected, the command will estimate the model described by Equation 3 which is the equivalent to allowing for full Cohort-time heterogeneity.\n\n\n\nAs described in Section 1.3, when covariates are considered in the model, the default option is to interact all covariates (or the demeaned transformations) with the same level of covariate heterogeneity. Some times, however, one may not be interested in estimating the same level of heterogeneity for all covariates. It may be possible, for example, to consider separate sets of covariates that could be interacted only with the time or the group dimensions.\nSpecifically, assume there are no variables we wish to consider for the treatment heterogeneity, but instead consider three sets of covariates: \\(x^{EX}\\) or variables we wish to incorporate without further interactions, \\(x^{T}\\) or variables that would be interacted with the Time variables only, and \\(x^{G}\\) or variables that would be interacted with group indicators only. In this case the setup would be:\n\\[\\begin{aligned}\nY_{i,t} &= \\alpha +  \\sum_{g \\in G} \\sum_{t=t_0}^{g-1} \\theta^{pre}_{g,t} D_{i,g,t}+\n\\sum_{g \\in G} \\sum_{t=g}^{T} \\theta^{post}_{g,t} D_{i,g,t} \\\\\n&+ x'^{EX}_{i}\\beta + \\sum_{t=t_0}^{T} D_{i,t} x'^{T}_{i}\\beta_t + \\sum_{g\\in G} D_{i,g} x'^{G}_{i}\\beta_g\n\\xi_i + \\xi_t + \\varepsilon_{i,t}\n\\end{aligned}\n\\tag{9}\\]\nThis specification can be estimated using the following syntax:\njwdid y , ivar(i) tvar(t) gvar(g) never exogvar(x_ex) xtvar(x_t) xgvar(x_g)\nIf covariates are included after y, they would still be treated following the specification of Equation 5, or following any of the heterogeneity restrictions described in Section 1.4.\nAn advanced version of this option is the inclusion of high order fixed effects (and interactions with fixed effects) that are different from the individual and time fixed effects. It is possible to request the inclusion of those types of fixed effects using the option fevar(), which is only valid if one is using the default estimator method reghdfe or ppmlhdfe. In both cases, the additional fixed effects (or interactions) are included in the estimation of the model without further interactions.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "**`jwdid`**: A Stata command for the estimation of Difference-in-Differences models using ETWFE"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics11.html#sec-agg",
    "href": "app_metrics/app_metrics11.html#sec-agg",
    "title": "jwdid: A Stata command for the estimation of Difference-in-Differences models using ETWFE",
    "section": "Post estimation: Aggregated treatment effects",
    "text": "Post estimation: Aggregated treatment effects\nAfter the estimation of the model, under the default options, one can use the coefficients \\(\\theta^{post}_{g,t}\\) as direct estimates of the group and time specific average tretment effects on the treated. However, one may also be interested in estimating aggregated ATTs for the overall data, across groups or periods, or dynamic effects. Furthermore, when the underlying method is a non-linear model, these coefficients cannot be directly interpreted as the average treatment effect on the outcome, but only on the latent variable.\njwdid comes along with the post estimation command jwdid_estat that can be used for that purpose. Internally, it uses the margins command to identify average treatment effects under the following algorithm:\n\nUsing the model estimates, predict the outcome of interest for all observations given the observed covariates and fixed effects. Call this \\(\\hat Y(obs)\\) or predicted outcome under the observed covariates. The model prediction could be the linear prediction, or the predicted probability in the case of logit models, or the predicted count in the case of poisson models.\nConsider the specification Equation 5, and assume that all \\(\\theta^{post}_{g,t}\\) (and \\(\\theta^{pre}_{g,t}\\) if never is used), as well as all \\(\\beta^{post}\\) and \\(\\beta^{pre}\\) are zero, and predict the outcome of interest. Call this \\(\\hat Y(0)\\) or predicted outcome under the counterfactual scenario of no treatment.\n\nIn this case, the predicted Average Treatment Effect on the Treated for observation \\(i\\) is given by:\n\\[\\widehat{ATT}_i = \\hat Y(obs) - \\hat Y(0)\\]\nThis is zero for observations that were never treated, and nonzero for the treated-post treatment observations.3\nFrom here, any aggregated average treatment effects can be calculated as follows:\n\\[AGGTE_r =  \\frac{ \\sum_i^N\\widehat{ATT}_i \\times w_{i,t} \\times R_{i,t}}{ \\sum_i^N  w_{i,t} \\times R_{i,t}}  \n\\]\nwhere \\(R_{i,t}\\) takes the value of one whenever observation \\(i\\) fullfills the required conditions, and \\(w_{i,t}\\) is the weight of the observation \\(i\\) at time \\(t\\) used in for the estimation model. 4. \\(AGGTE\\) is the aggregated average treatment effect on the treated given the conditions \\(R\\).\nIn general, there are four types of aggregations that are implemented in the jwdid_estat/estat command:\n\nestat simple: This option calculates the average treatment effect on the treated for all observations that were treated at some point in time. The condition \\(R\\) is defined as:\n\n\\[\\begin{aligned}\nR_{i,t} &= 1 \\text{ if } t \\geq g \\text{ for observation } i \\in g&gt;0 \\\\\nR_{i,t} &= 0 \\text{ otherwise}\n\\end{aligned}\n\\]\n\nestat group: This option calculates the average treatment effect for observations that were treated at time \\(g_c\\). The condition \\(R\\) is defined as:\n\n\\[\\begin{aligned}\nR_{i,t} &= 1 \\text{ if } t \\geq g_c ~ \\& ~ g_c&gt;0 \\\\\nR_{i,t} &= 0 \\text{ otherwise}\n\\end{aligned}\n\\]\nwhere \\(g_c\\) is a particular group/cohort of interest. estat group estimates this for all groups \\(g\\) in \\(G\\).\n\nestat time: This option calculates the average treatment effect at time \\(t\\) for all observations that were effectively treated at that point. The condition \\(R\\) is defined as:\n\n\\[\\begin{aligned}\nR_{i,t} &= 1 \\text{ if } t_c \\geq g ~ \\& ~ g&gt;0 \\\\\nR_{i,t} &= 0 \\text{ otherwise}\n\\end{aligned}\n\\]\nwhere \\(t_c\\) is a particular time of interest. estat time estimates this for all times \\(t\\) in \\(T\\) that has at least one unit that was treated.\n\nestat event: This option calculates dynamic treatment effects, also known as event studies, using the period before treatment as the reference. When the condition `never’ is used, this approach can be used to estimate pre-treatment ATT’s, which could be used for testing PTA. The condition \\(R\\) is defined as:\n\n\\[\\begin{aligned}\nR_{i,t} &= 1 \\text{ if } t-g = e_c \\text{ \\& } e \\neq -1 \\\\\nR_{i,t} &= 0 \\text{ otherwise}\n\\end{aligned}\n\\]\nwhere \\(e_c\\) is a particular event of interest. In contrast with the previous aggregations, if option never was used for estimation, one could also add the option pretrend to run a simple PTA test with the following null hypothesis:\n\\[H_0: AGGTE_e = 0 \\text{ for all } e &lt; -1 \\text{ vs }H_1: AGGTE_e \\neq 0 \\text{ for some } e &lt; -1\\]\nFailure to reject this hypothesis is evidence in support of parallel trends assumption. Otherwise, one can use test command to test for the significance of specific pre-treatment ATT’s.5",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "**`jwdid`**: A Stata command for the estimation of Difference-in-Differences models using ETWFE"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics11.html#post-estimation-other-options",
    "href": "app_metrics/app_metrics11.html#post-estimation-other-options",
    "title": "jwdid: A Stata command for the estimation of Difference-in-Differences models using ETWFE",
    "section": "Post estimation: Other options",
    "text": "Post estimation: Other options\nThe jwdid_estat/estat command allows for further options that may be of interest for the user. In this section we provide a brief description of those options:\n\nWeights\nThe default option for the estimation of the aggregated ATTs is to use the weights \\(w_{i,t}\\) that were used in the estimation of the model. However, if the user wants to use different weights, it is possible to do using the following syntax:\nestat [aggregation] [pw = weight]\n\n\nStandard Errors\nWooldridge (2021) suggests than when one estimates standard errors for the aggregated ATTs, one should use vce(unconditional) option in Stata, to allow for uncertainty in the explanatory variables. jwdid_estat/estat does not use this approach by default, because it requires that the underlying command is able to produce Scores for the estimated model. For example, if the model was estimated using method(regress), the Scores will be available, and unconditional Standard errors for the aggregated ATTs can be estimated as follows:\nestat [aggregation], [vce(unconditional)]\nThis is not possible if one uses the default methodology reghdfe, nor with ppmlhdfe.\n\n\nOther Aggregation restrictions\nAs described in Section 2, the default aggregation considers all observations treated observations, impossing restrictions only in terms of time, group or event dimensions. However, one may be interested in imposing further restrictions that could leverage on the use of covariates in the model specification. For example, say that one estimates a DID model with covariates using the following syntax:\njwdid y i.dx, ivar(i) tvar(t) gvar(g) never\nAs usual, one could request the estimation of the aggregated ATTs for the whole sample as follows:\nestat [aggregation]\nHowever, one would also be able to make the same estimation imposing the added restriction that the covariate dx is zero or one, using the option orestriction():\nestat [aggregation], orestriction(dx==0)\nestat [aggregation], orestriction(dx==1)\nThe expression inside the parenthesis should be a valid Stata expression that is used when calculating the aggregated ATTs.\n\n\nStoring, and saving results\nAfter aggregate effects have been estimated, the user may want to store the results for further analysis or reporting. Because estat uses margins in the background, the default option is to store the results in memory as r() elements. Alternatively, jwdid_estat/estat allows the user to store the output of the command using three different options:\n\nestat [aggregation], post: As with margins, option post will “post” the results of the command to be the current estimations in memory e(), which can be saved as usual for further analysis.\nestat [aggregation], estore(name): This option stores the results from the aggregation in memory as name. This is similar to using estimation store name after a regression command. The previously estimated results from jwdid are not overwritten.\nestat [aggregation], esave(filename): This option saves the results from the aggregation in a file filename, as a ster file, which can be used at a later point.\n\n\n\nPlotting\nAfter time, group or event aggregations are estimated, it is possible to request plotting those results using estat plot. The basic syntax is to type it after the aggregation command, with only minimal command specific options:\n\nestat plot, style(style): The option style allows the user to select the style of the plot. The default is using a rspike style, but rarea, rcap and rbar are also available. See help twoway for more information on the styles.\nestat plot,  pstyle1(str) color1(str) pstyle2(str) color2(str) lwidth1(str) lwidth2(str) barwidth1(str) barwidth2(str): The options pstyle#, color#, lwidth# and barwidth# can be used to alter the color and style of the lines in the plot for the pre and post treatment periods. Only event aggregation allows for #2 options.\nestat plot, twoway_options: Most other twoway graph option can be used after the estat plot command.\nestat plot, tight: If using cohort, group or event aggregations, the option tight will recode the x-axis values starting from 1, instead of the original values. This will avoid blank spaces in the plot.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "**`jwdid`**: A Stata command for the estimation of Difference-in-Differences models using ETWFE"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics11.html#footnotes",
    "href": "app_metrics/app_metrics11.html#footnotes",
    "title": "jwdid: A Stata command for the estimation of Difference-in-Differences models using ETWFE",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOften, one can use group fixed effects instead of individual fixed effects, and would still obtain numerically identical results in the linear model case.↩︎\n The correction implemented with corr is not useful to recover the coefficients from ppmlhdfe using poisson command ↩︎\nThe case of pre-treatment treated observations may be assumed to be zero if the never option is not used. When never is used, pre-treatment ATTs can be used for testing the parallel trends assumption.↩︎\nWhile this is the default option, estat command also allow you to provide other weights for aggregation↩︎\nIt should be noticed that this test is different from the test proposed by Callaway and Sant’Anna (2021), which is based on testing all group/time specific ATT’s, instead of the event aggregated ones.↩︎",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "**`jwdid`**: A Stata command for the estimation of Difference-in-Differences models using ETWFE"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics4.html",
    "href": "app_metrics/app_metrics4.html",
    "title": "DID: The Revolution",
    "section": "",
    "text": "If you have been keeping up with the advancements and revolution in Difference-in-Differences (DID), you may be aware of the issues associated with using the simple Two-Way Fixed Effects (TWFE) method for identifying treatment effects. If you haven’t, you can refer to my previous post titled DID: The fall where I have explained the problem.\nThe issue can be summarized as follows:\nWhen multiple periods and groups are available, and the treatment has been implemented over several periods, the standard TWFE method is unlikely to identify the average treatment effects. This is because it tends to use inadequate controls, gives negative weights to treated units, and generally suffers from contamination that leads to biased estimates.\nAs far as I know, the following model can only estimate average treatment effects if treatment effects remain constant over time: \\[\ny_{it} = a_i + a_t + \\delta D_{it} + e_{it}\n\\]\nwhere \\(D_{it}=1\\) if a unit is effectively treated, \\(a_i\\) represents the unit fixed effects, and \\(a_t\\) time fixed effects.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: The Revolution"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics4.html#introduction",
    "href": "app_metrics/app_metrics4.html#introduction",
    "title": "DID: The Revolution",
    "section": "",
    "text": "If you have been keeping up with the advancements and revolution in Difference-in-Differences (DID), you may be aware of the issues associated with using the simple Two-Way Fixed Effects (TWFE) method for identifying treatment effects. If you haven’t, you can refer to my previous post titled DID: The fall where I have explained the problem.\nThe issue can be summarized as follows:\nWhen multiple periods and groups are available, and the treatment has been implemented over several periods, the standard TWFE method is unlikely to identify the average treatment effects. This is because it tends to use inadequate controls, gives negative weights to treated units, and generally suffers from contamination that leads to biased estimates.\nAs far as I know, the following model can only estimate average treatment effects if treatment effects remain constant over time: \\[\ny_{it} = a_i + a_t + \\delta D_{it} + e_{it}\n\\]\nwhere \\(D_{it}=1\\) if a unit is effectively treated, \\(a_i\\) represents the unit fixed effects, and \\(a_t\\) time fixed effects.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: The Revolution"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics4.html#solutions",
    "href": "app_metrics/app_metrics4.html#solutions",
    "title": "DID: The Revolution",
    "section": "Solutions",
    "text": "Solutions\nDespite the challenges previously discussed, there is still hope for the successful implementation of DID. Numerous scholars have proposed various strategies that specifically address the issues at hand, as hinted in DID: the Fall.\nIn this context, I will provide a brief overview of three such solutions, two of which I am actively involved in, and one that is relatively straightforward to comprehend and articulate.\n\nSetup\nLet us establish some fundamental nomenclature before we delve into the solution. This terminology will help understand and implement the solutions we will cover next.\nBefore we start discussing the solution, lets stablish some basic nomenclature that may help us understand and implement the solutions we will be discussing later on.\n\nFor simplicity, we will focus on the case of balanced panel data. All units \\(i\\) will be observed for \\(T\\) periods, from \\(t=1\\) to \\(t=T\\).\nAt any given time, individual outcomes are defined as follows: \\[\ny_{it} = a_i + a_t + \\delta_{it}*D_{it} +e_{it}\n\\]\n\nwhere \\(a_i\\) is the individual fixed effect, \\(a_t\\) is a time fixed effect, and \\(e_{it}\\) is an individual level iid error. \\(D_{it}\\) is a dummy variable that takes the value of 1 after a unit is treated.\nNotice that here, the treatment effect fully heterogeneous.\n\nOnce a unit has been treated, it remains so, even if the effect eventually fades away. (if \\(D_{it}=1 \\rightarrow D_{is}=1 \\ \\forall \\ s&gt;t\\))\nThere is variation in treatment timing, and as a result, the treatment dummy is not activated for everyone at the same time.\nAssume there are no controls.\n\nUnder this conditions, lets simulate a simple dataset with this characteristics:\n\nclear\nset linesize 250\nset seed     111\nset sortseed 111\nset obs 100  // &lt;- 100 units\ngen id = _n\ngen ai = rchi2(2)\n// determines When would units receive treatment\ngen     g = runiformint(2,10)\nreplace g = 0 if g&gt;9   // never treated       \nexpand 10   // &lt;-T=10\nbysort id:gen t=_n \ngen event = max(0,t-g)\ngen aux = runiform()*2\nbysort t:gen at = aux[1] // Determines Time fixed effect\ngen te = (1-g/10)+(1-event/10)  \n// Treatment effect but vanishes with time\ngen eit= rnormal()\ngen trt  = (t&gt;=g)*(g&gt;0)\ngen teff = te * trt \ngen y = ai + at + te * trt + eit\n\n\n\n\nNumber of observations (_N) was 0, now 100.\n(18 real changes made)\n(900 observations created)\n\n\nThe simulated data will follow 100 units over a span of 10 periods. Each unit is assigned to receive treatment at some point between periods 2 and 9, while 1/10 of the sample is not treated at all.\nThe impact of the treatment depends on two factors: the timing of treatment and the duration of treatment. The treatment effect becomes smaller the later a unit is treated, and as a unit is treated for longer durations.\n\n\nTwo-Steps DID\n\nGardner (2022) did2s & Borusyak, Jaravel, and Spiess (2022) did_imputation\nLet’s start by desribing one of the solutions solutions: the two-step DID, also referred to as the imputation approach. This strategy is explained and explored by two notable papers: Gardner (2022) and Borusyak, Jaravel, and Spiess (2022).\nAs the name implies, this approach aims to estimate treatment effects in a design with multiple groups treated at various points in time by imputing the values of what would have occurred to those units if they were never subjected to treatment. To accomplish this, the authors suggest breaking down the identification problem into two steps to avoid the erroneous use of already treated units as controls.\nIn the first step, one should only use units that have never been treated or have not yet been treated to model potential outcomes under the no-treatment scenario.\n\\[\ny_{it} = a_i + a_t + e_{it} \\ if \\ t&lt;g\n\\]\nDoing this, the fixed effects \\(a_i\\) and \\(a_t\\) would not be contaminated by the treatment, because non of the units in the sample are treated.\nIn the second step, individual-level treatment effects predictions can be obtained by simply computing the difference between the observed outcome under treatment and the prediction of the first step (\\(\\hat y_{it} = \\hat a_i + \\hat a _t\\)), which represent the potential outcomes of those units if no treatment occured.\n\\[\n\\delta_{it} = y_{it}-\\hat a_i - \\hat a_t\n\\]\nBecause unit level treatment effects are not useful for statistical inference, one can use aggregations to obtain standard errors or aggregates we are more familiar with: Average treatment effect on treated, dynamic effects, etc.\nTo correctly estimate standard errors, both steps should be estimated simulatenously, so that the estimation error from the first stage can propagate to the second stage. While you can use did2s or did_imputation, I will show you the code using gmm, so you can see how the whole system would work.\nIn the following code line (1) identifies the individual and time FE using only notyet treated units (trt=0). Line 2 uses tha to substract from \\(y\\) and identify the treatment effect (ATT). Lines 3 and 4 provides information to gmm to estimate the model and standard errors:\n\n\nCode\ngmm ((y-{a0}-{a_i:i.g}-{a_t:i.t})*(trt==0))     /// \n    ((y-{a0}-{a_i:}   -{a_t:}    -{att:trt})) , /// \n    winitial(identity)  instruments(1:i.g i.t) instruments(2: trt) /// \n    onestep quickderivatives vce(cluster i)  \n\n\n\nStep 1\nIteration 0:   GMM criterion Q(b) =  19.725543  \nIteration 1:   GMM criterion Q(b) =  5.816e-23  \nIteration 2:   GMM criterion Q(b) =  9.081e-33  \n\nGMM estimation \n\nNumber of parameters =  19\nNumber of moments    =  20\nInitial weight matrix: Identity                   Number of obs   =      1,000\n\n                                   (Std. err. adjusted for 100 clusters in id)\n------------------------------------------------------------------------------\n             |               Robust\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\na0           |\n       _cons |   3.146414   .6822892     4.61   0.000     1.809152    4.483677\n-------------+----------------------------------------------------------------\na_i          |\n           g |\n          2  |  -1.329894   .9540798    -1.39   0.163    -3.199856    .5400685\n          3  |   .3303121   .9832247     0.34   0.737    -1.596773    2.257397\n          4  |  -.9533545   .8233816    -1.16   0.247    -2.567153    .6604439\n          5  |  -.9570973   .8366389    -1.14   0.253    -2.596879    .6826848\n          6  |    -.58129   .8707363    -0.67   0.504    -2.287902    1.125322\n          7  |   .9916895   1.010794     0.98   0.327    -.9894297    2.972809\n          8  |  -1.609291   .7437976    -2.16   0.030    -3.067108   -.1514746\n          9  |  -1.160353   1.033179    -1.12   0.261    -3.185346    .8646411\n-------------+----------------------------------------------------------------\na_t          |\n           t |\n          2  |  -.0585438   .1385197    -0.42   0.673    -.3300374    .2129498\n          3  |   -.213678   .1704212    -1.25   0.210    -.5476975    .1203415\n          4  |  -.5752045   .1713895    -3.36   0.001    -.9111217   -.2392872\n          5  |   .0842171   .1677747     0.50   0.616    -.2446152    .4130494\n          6  |   .1190078   .1841714     0.65   0.518    -.2419615    .4799771\n          7  |   1.000738   .2034229     4.92   0.000     .6020363    1.399439\n          8  |  -.0146057   .2190799    -0.07   0.947    -.4439945    .4147831\n          9  |   .9380065   .2007013     4.67   0.000     .5446391    1.331374\n         10  |   .9572597   .2578496     3.71   0.000     .4518838    1.462636\n-------------+----------------------------------------------------------------\natt          |\n         trt |   1.458058   .1493165     9.76   0.000     1.165403    1.750713\n------------------------------------------------------------------------------\nInstruments for equation 1: 0b.g 2.g 3.g 4.g 5.g 6.g 7.g 8.g 9.g 1b.t 2.t 3.t 4.t 5.t 6.t 7.t 8.t 9.t 10.t _cons\nInstruments for equation 2: trt _cons\n\n\nYou can compare this to the true effect:\n\n\nCode\nsum teff if trt==1\n\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        teff |        472    1.280297    .2217799          1        1.8\n\n\nOne advantage of this approach is that you could model the second stage to allow other types of aggregations:\ngen g0=g*trt    \ngen t0=t*trt\ngmm ((y-{a0}-{a_i:i.g}-{a_t:i.t})*(trt==0)) ///\n    ((y-{a0}-{a_i:}   -{a_t:}    -{att:i.g0})) , ///\n    winitial(identity)  instruments(1:i.g i.t) instruments(1: i.g0) ///\n    onestep quickderivatives vce(cluster i) \ngmm ((y-{a0}-{a_i:i.g}-{a_t:i.t})*(trt==0)) ///\n    ((y-{a0}-{a_i:}   -{a_t:}    -{att:i.t0})) , ///\n    winitial(identity)  instruments(1:i.g i.t) instruments(2: i.t0) ///\n    onestep quickderivatives vce(cluster i)             \n\n\n\nYou don’t messup with OLS\n\nWooldridge (2021) jwdid and Sun and Abraham (2021)\nIn the article DID: the fall, it was pointed out that the conventional TWFE approach has faced significant backlash due to its limited ability to detect treatment effects, because it cannot distinguish between good and bad variation when estimating treatment effects. Despite this criticism, Professor Wooldridge came in defense and revitalized the approach by emphasizing its simplicity and versatility, enabling extensions to go beyond linear models.\nThe message was simple:\n\nAlthough the conventional TWFE method has several shortcomings, if it is implemented correctly, it can overcome the issue of utilizing inadequate controls in estimation. As a result, it can estimate treatment effects efficiently, with results similar to Borusyak, Jaravel, and Spiess (2022) and Gardner (2022).\n\nSo what were we missing? Heterogeneity!\nBoth Wooldridge (2021) and Sun and Abraham (2021) have proposed similar solutions to the problem at hand, albeit from different viewpoints. Sun and Abraham (2021) argued that utilizing event studies with leads and lags may lead to contaminated coefficients, thus hampering proper identification of dynamic treatment effects. As a potential solution, the authors suggests using a specification that estimates dynamic effects for each cohort before making aggregations.\nOn the other hand, Wooldridge (2021) focused on identifying treatment effects. He recommends allowing all treatment effects to vary by cohort and time. In other words, instead of employing a single dummy variable to identify treated units, he suggests using a set of dummies representing the interaction of cohorts and periods.\nSpecifically, Wooldridge (2021) proposes estimating a model with the following specification:\n\\[\ny_{it} = a_i + a_t + \\sum_{g=g_0}^G \\sum_{t=g}^T \\delta_{gt} * 1(g,t)+e_{it}\n\\]\nWhat Wooldridge suggests, at least for the case without covariates, is to estimate a model where, in addition to the individual (or cohort) and time fixed effects, we saturate all possible combinations of cohorts and times (\\(1(g,t)\\)), if that combination corresponds to an effectively treated unit (\\(t\\geq g\\)). This specification essentially uses all not-yet treated units as controls, similar to the two-step approach.\nThis specification, however, could also be extended to a case where only those never treated are used as controls. In this case, one should use all cohort and time interactions, including the cases before units were treated. Following convention, one should exclude from the list of interactions the period before treatment took place:\n\\[\ny_{it} = a_i + a_t + \\sum_{g=g_0}^G \\sum_{t\\neq g-1} \\delta_{gt} * 1(g,t)+e_{it}\n\\]\nBy saturating the model this way, each \\(\\delta_{gt}\\) represents a treatment effect for group G at time T, which can be later aggregated to obtain dynamic, group, or average treatment effects. Also interesting to note that this second specification is essentially the same Sun and Abraham (2021) propose for dynamic effects, and produces results that are identical to the methodology proposed by Callaway and Sant’Anna (2021).\nNow for the application, I will use jwdid for both specifications. Because by default jwdid produces the fully saturated model, I will omit those results, showing only the average treatment effect.\nFirst the one using not-yet treated units as controls (default), which produces exactly the same results as the two-step approach:\n\n*ssc install jwdid\nqui: jwdid y, ivar(i) tvar(t) gvar(g)\nestat simple\n\nWARNING: Singleton observations not dropped; statistical significance is biased (link)\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      simple |   1.458058   .1494798     9.75   0.000     1.165083    1.751033\n------------------------------------------------------------------------------\n\n\nSecond the one with never treated units as controls:\n\nqui: jwdid y, ivar(i) tvar(t) gvar(g) never\nestat simple\n\nWARNING: Singleton observations not dropped; statistical significance is biased (link)\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      simple |   1.543749   .1567784     9.85   0.000     1.236469    1.851029\n------------------------------------------------------------------------------\n\n\n\n\n\n2x2 on Steroids\n\nCallaway and Sant’Anna (2021) csdid & csdid2\nThe third option is the most computing intensive, but at the same time simpler to understand, if you break it down to the basics. This is why I call this 2x2 in Steroids: Callaway and Sant’Anna (2021).\nThe literature on the 2x2 DID design has been extensively explored and extended, and it appears that most of the criticisms of the TWFE method do not apply to this simple design. Although there are a few technical details to consider while estimating ATT’s, most of the information required can be found in Sant’Anna and Zhao (2020). In this work, the authors provide several options for obtaining the best estimate from a simple 2x2 DID design.\nAssuming that we know how to execute 2x2 DID correctly (which can be achieved using drdid in Stata), Callaway and Sant’Anna (2021) propose that we focus on estimating all the good and useful 2x2 DID designs from our data while avoiding incorrect comparisons. These are the building blocks of the methodology, the ATTGTs. This are the average treatment effects on the treated for units treated for the first time in period G, but measured at period T.\nThis process, however, could be time-consuming and computationally intensive if done manually, as the number of 2x2 designs increases with the number of cohorts and periods available in the data. For example, estimating 50 different ATTs would be necessary with 5 cohorts and 10 periods, and up to 5 separate models need to be estimated for each ATT.\nBorrowing from the nomenclature in Callaway and Sant’Anna (2021), this ATTGT’s are defined as follows: \\[\n\\begin{aligned}\n    ATT(g,t) &= E(Y_{i,t}|i\\in g) - E(Y_{i,g-1}|i\\in g) \\\\\n             & -\\Big[ E(Y_{i,t}|i\\in C) - E(Y_{i,g-1}|i\\in C) \\Big]\n\\end{aligned}\n\\]\nWhich has the same structure as the simple 2x2 DID, with the difference that the control group \\(C\\) will be formed by never treated units only, or include those not yet treated ( \\(i \\in g_i, g_i&gt;t, \\And \\  g_i&gt;g\\)).1\nOnce all individual ATTGT’s have been identified and estimated, providing summary measures we are more familiar with is as easy as obtaining weighted averages:\n\\[\nSATT = \\sum \\left( \\frac{w_{gt} }{\\sum w_{gt}}ATT(g,t) \\right)\n\\]\nwhere the weights \\(w_{gt}\\) will change based on the type of aggregation one is interested in.\nThis multiple stage process may seem challening, but there are tools that allow you to implement them quite easily: csdid and csdid2. The first one, was written as a wrapper around drdid, to do all the heavy lifting for you. However, for large projects, it can be slow due to overhead. The alternative csdid2 is self contained, still under development, but much faster than the predecesor. See csdid2 if interested. Here I will use csdid, which you can get from SSC.\nAs with jwdid. The default option is for csdid to produce all ATTGT’s. So, for the excercise, I’ll omit that ouput, and estimate the aggregate effects with the post estimation command. The default is to use the never treated as controls. I will also add the option long2 to obtain the pre-treatment ATTGT’s as describe above, even though they won’t affect our point estimates:\n\nqui: ssc install drdid, replace\nqui: ssc install csdid, replace\nqui: csdid y, ivar(i) time(t) gvar(g) long2\nestat simple \n\nAverage Treatment Effect on Treated\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         ATT |   1.543749   .1577366     9.79   0.000     1.234591    1.852907\n------------------------------------------------------------------------------\n\n\nI will also use the not yet treated, to compare results.\n\nqui: csdid y, ivar(i) time(t) gvar(g) notyet long2\nestat simple \n\nAverage Treatment Effect on Treated\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         ATT |   1.494475   .1587386     9.41   0.000     1.183353    1.805597\n------------------------------------------------------------------------------",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: The Revolution"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics4.html#conclusions",
    "href": "app_metrics/app_metrics4.html#conclusions",
    "title": "DID: The Revolution",
    "section": "Conclusions",
    "text": "Conclusions\nOn this occasion, I have shared with you three solutions from the literature that aim to overcome the limitations of TWFE. Although there are other solutions available, I personally find these three to be the most intuitive and have worked on them. Granted, I have some bias on the matter.\nDespite their apparent differences, these solutions actually converge towards similar outcomes, with discrepancies arising from variations in assumptions regarding control groups or covariate management.\n\n\n\ncommand\neq command\n\n\n\n\njwdid\ndid2s & did_imputation\n\n\njwdid, never\neventstudyinteract\n\n\njwdid, never\ncsdid, long2 or csdid2, long",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: The Revolution"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics4.html#references",
    "href": "app_metrics/app_metrics4.html#references",
    "title": "DID: The Revolution",
    "section": "References",
    "text": "References\n\n\nBorusyak, Kirill, Xavier Jaravel, and Jann Spiess. 2022. “Revisiting Event Study Designs: Robust and Efficient Estimation.” arXiv. https://doi.org/10.48550/arXiv.2108.12419.\n\n\nCallaway, Brantly, and Pedro H. C. Sant’Anna. 2021. “Difference-in-Differences with Multiple Time Periods.” Journal of Econometrics, Themed Issue: Treatment Effect 1, 225 (2): 200–230. https://doi.org/10.1016/j.jeconom.2020.12.001.\n\n\nGardner, John. 2022. “Two-Stage Differences in Differences.” arXiv. https://doi.org/10.48550/arXiv.2207.05943.\n\n\nSant’Anna, Pedro H. C., and Jun Zhao. 2020. “Doubly Robust Difference-in-Differences Estimators.” Journal of Econometrics 219 (1): 101–22. https://doi.org/10.1016/j.jeconom.2020.06.003.\n\n\nSun, Liyang, and Sarah Abraham. 2021. “Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects.” Journal of Econometrics, Themed Issue: Treatment Effect 1, 225 (2): 175–99. https://doi.org/10.1016/j.jeconom.2020.09.006.\n\n\nWooldridge, Jeffrey M. 2021. “Two-Way Fixed Effects, the Two-Way Mundlak Regression, and Difference-in-Differences Estimators.” {SSRN} {Scholarly} {Paper}. Rochester, NY. https://doi.org/10.2139/ssrn.3906345.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: The Revolution"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics4.html#footnotes",
    "href": "app_metrics/app_metrics4.html#footnotes",
    "title": "DID: The Revolution",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are two other options, but I will leave them for a different post↩︎",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: The Revolution"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics6.html",
    "href": "app_metrics/app_metrics6.html",
    "title": "Making better use of interval-censored data",
    "section": "",
    "text": "Researchers and policy makers rely on household budget surveys and labor force surveys to answer questions related to employment changes and income dynamics. Although household budget surveys offer rich income data that is used for official poverty and income inequality analysis, they are only available at a relatively low frequency. In contrast, labor force surveys are typically collected at a higher frequency but have more limited information in terms of sample sizes or quality of information collected. However, in many cases, labor force surveys collect and report family income and labor income data in brackets, which raises the problem of recovering the full income distribution, key to analyzing poverty and inequality.\nUsing interval-censored data for analysis of income inequality is not uncommon. Many approaches exist to estimate GINI indices in presence of income inequality (see Jenkins et al. (2011), Jargowsky and Wheeler (2018)), which rely on the parametric estimation of the unconditional income distribution. While these approaches can be used to obtain general measures of poverty and inequality for the population as a whole, their use is limited for analyzing the heterogeneity across individual characteristics.\nIn our paper Rios-Avila, Canavire-Bacarreza, and Sacco-Capurro (forthcoming), we propose an alternative solution to the problem of analyzing interval-censored data that should be easy to implement using already available software. Our method extends the application of Multiple Imputation with interval-regression estimator proposed in Royston (2007), that is already built-in in many statistical software packages including Stata. Specifically, we suggest that using a flexible enough interval regression estimator that allows for heteroskedastic errors can be used to characterized the conditional distribution of the censored data. One the model is estimated, we can use multiple imputation to create syntethic data, which can be analyzed using standard econometric or statistic tools.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Making better use of interval-censored data"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics6.html#motivation",
    "href": "app_metrics/app_metrics6.html#motivation",
    "title": "Making better use of interval-censored data",
    "section": "",
    "text": "Researchers and policy makers rely on household budget surveys and labor force surveys to answer questions related to employment changes and income dynamics. Although household budget surveys offer rich income data that is used for official poverty and income inequality analysis, they are only available at a relatively low frequency. In contrast, labor force surveys are typically collected at a higher frequency but have more limited information in terms of sample sizes or quality of information collected. However, in many cases, labor force surveys collect and report family income and labor income data in brackets, which raises the problem of recovering the full income distribution, key to analyzing poverty and inequality.\nUsing interval-censored data for analysis of income inequality is not uncommon. Many approaches exist to estimate GINI indices in presence of income inequality (see Jenkins et al. (2011), Jargowsky and Wheeler (2018)), which rely on the parametric estimation of the unconditional income distribution. While these approaches can be used to obtain general measures of poverty and inequality for the population as a whole, their use is limited for analyzing the heterogeneity across individual characteristics.\nIn our paper Rios-Avila, Canavire-Bacarreza, and Sacco-Capurro (forthcoming), we propose an alternative solution to the problem of analyzing interval-censored data that should be easy to implement using already available software. Our method extends the application of Multiple Imputation with interval-regression estimator proposed in Royston (2007), that is already built-in in many statistical software packages including Stata. Specifically, we suggest that using a flexible enough interval regression estimator that allows for heteroskedastic errors can be used to characterized the conditional distribution of the censored data. One the model is estimated, we can use multiple imputation to create syntethic data, which can be analyzed using standard econometric or statistic tools.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Making better use of interval-censored data"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics6.html#understanding-the-problem",
    "href": "app_metrics/app_metrics6.html#understanding-the-problem",
    "title": "Making better use of interval-censored data",
    "section": "Understanding the Problem",
    "text": "Understanding the Problem\nTo illustrate the problem, we use an excerpt of the Swiss Labor Force Survey 1998, which is available online and has the type of data and variables applied econometricians may use. Assume we are interested in analyzing the relationship between hourly wages (dependent variable) and four independent variables: years of education, age of the respondent, years of job tenure, and gender. However, data on wages per hour were not collected directly, but rather it was collected by people indicating if their earnings belong to a list of income brackets. Thus, we only know the category of where a person’s wage per hour would fall, making it difficult to relate our variables of interest with wages per hour.\n\n\n\n\n\n\n\n\n\n\n\n(a) Fully Observed\n\n\n\n\n\n\n\n\n\n\n\n(b) Censored\n\n\n\n\n\n\n\nFigure 1: Wages vs Education, Age and Tenure\n\n\n\nFigure 1 presents information on the visual correlations between wages and three of the varialbes of interest. Figure 1 (a) provides scatter plots of the fully observed wages agains the explanatory variables, whereas Figure 1 (b) does the same, but using the bracket censored data.\nIf one is interested in analyzing other moments, such as conditional quantiles, current methods would not allow for it, since they are mostly focused on explaining conditional means. However, that is where our approach comes into place. We start by using the intreg, het() command to model both the conditional mean and conditional variance. The latter is important because it allows us to explicitly model relationships between the dependent and independent variables that vary across the distribution. After we estimate the parameters for the conditional distribution, we can use it to generate multiple syntethic datasets (multiple imputation) with our command intreg_mi. This multiple imputed data can later be analyzed using standard econometric tools.\nWe use this strategy with the data from Figure 1 (b), and generate multiple synthetic datasets to reconstruct the censored data. For completeness, we impute directly wages as well as log(wages). To inspect how well the imputed data replicates the original data, we plot their density functions in Figure 2. The yellow line representing the true wage distribution, and the blue lines corresponding to the imputed data. And of this scenario, it seems that imputed log(wages), or wages directly has little effect in the replicating the wage distribution.\n\n\n\n\n\n\nFigure 2: Observed vs Imputed Wages\n\n\n\nAfter creating the multiple-imputed synthethic datasets, they can be analyzed applying standard aggregation rules. In Stata, this can be done using mi commands. Because multiple imputation would provide no added value to the estimation of conditional means (linear regression), we compare esitmates using conditional quantile regressions.\n\n\n\n\n\n\nFigure 3: Quantile Regression Coefficients\n\n\n\nFigure 3 contrasts the results of the quantile regression coefficients, using the fully observed data, as well as the two imputed data options. In general, while the estimated parameters using imputed data does not replicate the quantile regression coefficients perfectly, it captures most of the observed trends across the distribution. Considering that we start with only limited information of wages in brackets, the fact that we can obtain good approximations of the unobserved effects is a considerable advantage.\nThe imputed data can be utilized in other ways. Policy analysists may be interested in analyzing the role of covariates on distributional statistics other than the mean. For example, they may be interested in analyzing income distribution using unconditional quantiles, Gini coefficients across groups, as well as other measures of income inequality.\n\n\n\nTable 1: Inequality Statistics by Gender\n\n\n\n\n\n\n\n(a) Men\n\n\n\n\n\n\nFully Observed\nImp-log_w\nImp-wage\n\n\n\n\nQ_10\n20.35\n20.22\n20.25\n\n\n\n(0.568)\n(0.570)\n(0.605)\n\n\nQ_90\n52.72\n52.51\n52.37\n\n\n\n(1.126)\n(1.561)\n(1.295)\n\n\nGini\n0.221\n0.222\n0.216\n\n\n\n(0.00677)\n(0.00759)\n(0.00672)\n\n\nCoeff of Variation\n0.413\n0.422\n0.392\n\n\n\n(0.0148)\n(0.0197)\n(0.0121)\n\n\nLog Variance\n0.238\n0.221\n0.268\n\n\n\n(0.0230)\n(0.0242)\n(0.0535)\n\n\n\n\n\n\n\n\n\n\n\n(b) Women\n\n\n\n\n\n\nFully Observed\nImp-log_w\nImp-wage\n\n\n\n\nQ_10\n15.87\n13.98\n13.57\n\n\n\n(0.576)\n(0.850)\n(0.815)\n\n\nQ_90\n45.92\n46.19\n46.54\n\n\n\n(1.442)\n(1.541)\n(1.412)\n\n\nGini\n0.267\n0.254\n0.249\n\n\n\n(0.0117)\n(0.00950)\n(0.00773)\n\n\nCoeff of Variation\n0.587\n0.493\n0.453\n\n\n\n(0.0461)\n(0.0603)\n(0.0141)\n\n\nLog Variance\n0.345\n0.301\n0.363\n\n\n\n(0.0347)\n(0.0506)\n(0.0637)\n\n\n\n\n\n\n\n\n\n\nNote: Standard Errors in Parenthesis\n\n\n\n\n\nWith this excercise in mind, Table 1 presents a few distributional statistics using the obseved and imputed data for men (Table 1 (a)) and women (Table 1 (b)). In all cases we obtain good approximations to the true statistics using the imputed data.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Making better use of interval-censored data"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics6.html#conclusions",
    "href": "app_metrics/app_metrics6.html#conclusions",
    "title": "Making better use of interval-censored data",
    "section": "Conclusions",
    "text": "Conclusions\nIn our paper, we propose a method that makes better use of interval-censored. We propose using multiple imputation based on an interval-regression estimator with heteroskedastic errors, extending the work Royston (2007). The multiple imputed data can be used to analyze the heterogeneity across individual characteristics using regression tools such as conditional or unconditional quantile regressions.\nIn this note, we provide a small excercise of the type of analysis one could do using our approach. In the paper, we include further simulation evidence on the performance of the methodology, as well as real case study using data for the developing country of Grenada.\nWe believe that our method has important applications in economics and other social sciences, where interval-censored data are commonly used to capture income variables. Our method can also be used to analyze other types of censored data, such as duration data or survival data. We hope that our method will encourage more researchers to utilize interval-censored data in their research, and to develop more flexible and robust methods for analyzing such data.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Making better use of interval-censored data"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics6.html#further-examples",
    "href": "app_metrics/app_metrics6.html#further-examples",
    "title": "Making better use of interval-censored data",
    "section": "Further Examples",
    "text": "Further Examples\nAs part of the journey of developing our method, we have produced a few examples that illustrate the application. You can find them in here:\n\nHow to install the command intreg_mi\nWage analysis using “Swiss Labor Market Survey 1998” data. here\nMelrbourne Housing Market here\nWage analysis using ASEC-CPS here\nHousing Prices: King County, USA here\nPoverty analysis using ASEC-CPS 2018 here",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Making better use of interval-censored data"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics6.html#software",
    "href": "app_metrics/app_metrics6.html#software",
    "title": "Making better use of interval-censored data",
    "section": "Software",
    "text": "Software\nThe excercise presented here was produce using Stata. The modeling was done using the official commands intreg. The imputed values were generated our command intreg_mi. The analysis of the data was performed using mi, qreg and rifhdreg.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "Making better use of interval-censored data"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics8.html",
    "href": "app_metrics/app_metrics8.html",
    "title": "DID: Panel Data & Repeated Crossection",
    "section": "",
    "text": "If you are reading this, you probably know quite well all the problems associated with the infamous TWFE-DID especification. Nevertheless, its worth a quick recap, in case you are not aware of.\nBefore the new literature on DID, whenever people wanted to analyze treatment effects using a difference in differences approach, and had access to data with many periods and many individuals, they would tend to use a model specification that look like this:\n\\[y_{it} = \\delta_i + \\delta_t + \\gamma \\times trt + e_{it}\n\\tag{1}\\]\nWhere \\(trt\\) was a dummy variable that would take the value of 1 for treated units after the treatment was implemented, and zero otherwise. The idea was that all units with \\(trt=0\\) (those not yet treated) would be used used as controls. In this specification, one is also controlling for time fixed effects \\(\\delta_t\\) as well as individual (or group) fixed effects \\(\\delta_i\\).\nThis specification was considered a generalization of the simple 2x2 DID design:\n\\[\ny = \\delta_0 + \\delta_1 post+ \\delta_2 treat + \\gamma \\ (post\\times treat) + e_{it}\n\\]\nWhere \\(\\delta_i\\) was the equivalent to \\(\\delta_1\\), and \\(\\delta_2\\) the equivalent for \\(\\delta_t\\).\nWhat many didn’t know at the time is that Equation 1 would provide correct identification of the Average treatment effect \\(\\gamma\\), under strict assumptions:\n\nThere is no treatment timing heterogeneity (All units are treated at the same time)\nIf there is timing heterogeneity, the treatment effect is homogenous across time and across groups. (\\(\\gamma\\) is the same across time or across groups)\n\nThis assumptions almost never hold. For example, the original treatment may become less effective few periods after they are implemented, and units treated later may experience stronger treatment effects than those treated earlier. When this happen, Equation 1 will provide incorrect results for three related reasons:\n\nLinear regressions do not discriminate between good or bad controls when identifying coefficients. They simply exploit all possible varation in the data.\nAlready treated units would be implicitly used as “control units”, when analyzing units treated at a later point in time.\nBecause of this, some treated units will receive “negative” weights, when estimating Average treatemnt effects.\n\nOne of the most interesting consequences of this problem is that one may estimate negative treatment effects, even if all units in the sample had a theoreticaly possitive treatment effect.\nMany paper published in 2021 including Goodman-Bacon (2021), Callaway and Sant’Anna (2021), Sun and Abraham (2021), Wooldridge (2021) and Borusyak, Jaravel, and Spiess (2022), to name a few, identified this problem, and proposed similar solutions: Estimate ATTs allowing for cohort and timing heterogeneity, avoiding the use of already treated units as controls.\nWhile these papers also provide their own estimators (in Stata, R, and in some cases python), I will focus only on two solutions. The ones proposed by Callaway and Sant’Anna (2021) and Wooldridge (2021), which I programmed in Stata using csdid[2], and jwdid. Both approaches suggest to estimate heterogenous treatment effects based on when a unit is treated (its cohort \\(G\\)), and at what point in time one is interested in estimating that effect \\(T\\). We will call them the \\(ATT(G,T)\\). 1",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: Panel Data & Repeated Crossection"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics8.html#first-things-first",
    "href": "app_metrics/app_metrics8.html#first-things-first",
    "title": "DID: Panel Data & Repeated Crossection",
    "section": "",
    "text": "If you are reading this, you probably know quite well all the problems associated with the infamous TWFE-DID especification. Nevertheless, its worth a quick recap, in case you are not aware of.\nBefore the new literature on DID, whenever people wanted to analyze treatment effects using a difference in differences approach, and had access to data with many periods and many individuals, they would tend to use a model specification that look like this:\n\\[y_{it} = \\delta_i + \\delta_t + \\gamma \\times trt + e_{it}\n\\tag{1}\\]\nWhere \\(trt\\) was a dummy variable that would take the value of 1 for treated units after the treatment was implemented, and zero otherwise. The idea was that all units with \\(trt=0\\) (those not yet treated) would be used used as controls. In this specification, one is also controlling for time fixed effects \\(\\delta_t\\) as well as individual (or group) fixed effects \\(\\delta_i\\).\nThis specification was considered a generalization of the simple 2x2 DID design:\n\\[\ny = \\delta_0 + \\delta_1 post+ \\delta_2 treat + \\gamma \\ (post\\times treat) + e_{it}\n\\]\nWhere \\(\\delta_i\\) was the equivalent to \\(\\delta_1\\), and \\(\\delta_2\\) the equivalent for \\(\\delta_t\\).\nWhat many didn’t know at the time is that Equation 1 would provide correct identification of the Average treatment effect \\(\\gamma\\), under strict assumptions:\n\nThere is no treatment timing heterogeneity (All units are treated at the same time)\nIf there is timing heterogeneity, the treatment effect is homogenous across time and across groups. (\\(\\gamma\\) is the same across time or across groups)\n\nThis assumptions almost never hold. For example, the original treatment may become less effective few periods after they are implemented, and units treated later may experience stronger treatment effects than those treated earlier. When this happen, Equation 1 will provide incorrect results for three related reasons:\n\nLinear regressions do not discriminate between good or bad controls when identifying coefficients. They simply exploit all possible varation in the data.\nAlready treated units would be implicitly used as “control units”, when analyzing units treated at a later point in time.\nBecause of this, some treated units will receive “negative” weights, when estimating Average treatemnt effects.\n\nOne of the most interesting consequences of this problem is that one may estimate negative treatment effects, even if all units in the sample had a theoreticaly possitive treatment effect.\nMany paper published in 2021 including Goodman-Bacon (2021), Callaway and Sant’Anna (2021), Sun and Abraham (2021), Wooldridge (2021) and Borusyak, Jaravel, and Spiess (2022), to name a few, identified this problem, and proposed similar solutions: Estimate ATTs allowing for cohort and timing heterogeneity, avoiding the use of already treated units as controls.\nWhile these papers also provide their own estimators (in Stata, R, and in some cases python), I will focus only on two solutions. The ones proposed by Callaway and Sant’Anna (2021) and Wooldridge (2021), which I programmed in Stata using csdid[2], and jwdid. Both approaches suggest to estimate heterogenous treatment effects based on when a unit is treated (its cohort \\(G\\)), and at what point in time one is interested in estimating that effect \\(T\\). We will call them the \\(ATT(G,T)\\). 1",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: Panel Data & Repeated Crossection"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics8.html#gxt-did-instead-of-2x2-did",
    "href": "app_metrics/app_metrics8.html#gxt-did-instead-of-2x2-did",
    "title": "DID: Panel Data & Repeated Crossection",
    "section": "GxT DID (instead of 2x2 DID)",
    "text": "GxT DID (instead of 2x2 DID)\n\nPanel Data\nLets assume you have access to Balanced Panel Data. In other words, you observe the same group of individuals across the same window of time. Different individuals are treated at different points in time, and some are not treated at all (never treated). This is the best case scenario, since you do not need to worry about identifying cohorts by treat timing.\nLet’s see how to estimate this type of models using csdid and jwdid. They estimate the treatment effects using different strategies, but under specific cases, they will provide the same point estimates. To show how this is done, I will employ the toy-dataset used in Callaway and Sant’Anna (2021).\nFirst let’s set things up, loading the data:\n\n\nCode\n** to get the data from my repository\nssc install frause, replace\nfrause mpdta, clear\n\n\nThis dataset contains information at the county level on population size (lpop), employment (lemp), and a variable that indicates when a county instuted a change in minimum wage (first_treat). To estimate a DID model with heterogenous treatment effects, could use either jwdid or csdid, which are available from SSC.2\n\n\nCode\n** This is a dependency for csdid\nssc install drdid, replace\nssc install csdid, replace\nssc install jwdid, replace\n\n\nJust one more step (not required but important if you did not identified the cohort variable). Lets create a dummy that takes the value of 1 only after the treated unit is treated:\n\n\nCode\ngen trt = (first_treat&lt;=year)*(first_treat&gt;0)\n\n\nThis is what Equation 1 is using. So how do I create the “treatment cohort” variable?. That can be easily done using one of csdid subprograms. You just need to provide the group or panel identifier, and the time variable:\n\n\nCode\negen gvar = csgvar(trt), ivar(countyreal) tvar(year)\n\n\nIts worth noting that the official commands xthdidregress and hdidregress do not require you to do this, because it automatically creates the gvar internally, based on the information provided. You can now create a small tabulation, as a sanity check, to verity the variable gvar is correctly created:\ntab year gvar\n\n\n\n\n\n\n\n\n\n\n\nyear\n0\n2004\n2006\n2007\nTotal\n\n\n\n\n2003\n309\n20\n40\n131\n500\n\n\n2004\n309\n20\n40\n131\n500\n\n\n2005\n309\n20\n40\n131\n500\n\n\n2006\n309\n20\n40\n131\n500\n\n\n2007\n309\n20\n40\n131\n500\n\n\nTotal\n1,545\n100\n200\n655\n2,500\n\n\n\nIf all goes as expected, you a tabulation similar to the one you see here. You have a total of 2500 observations, but only 500 counties. 309 were not treated (had no changes in minimum wage between 2003 and 2007), 20 were treated in 2004, 40 in 2006 and 131 in 2007.\nThese numbers are what I call “effective sample size”, because csdid (and implicitly jwdid) use only this ammount of information when estimating the treatment effects. In other words, we should assume the sample size is 20 (for the purposes of model specification and controls).\nBy default, jwdid uses the not yet treated as comparison group. So, to make the results comparable to csdid, I will request using those “never-treated” as comparison group. For csdid, the default is using short gaps for pre-treatment ATT’s, along with the never treated as comaprison group. To change that I will use long2 option. This will make the following comparable to the outcomes from standard Event-studies estimations. The next lines should produce identical point estimates, with very similar standard errors.\n\nCSDIDJWDID\n\n\n\n\nCode\ncsdid lemp , ivar(county) time(year) gvar(gvar) long2\n\n\n............\nDifference-in-difference with Multiple Time Periods\n\n                                                         Number of obs = 2,500\nOutcome model  : regression adjustment\nTreatment model: none\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\ng2004        |\n t_2003_2004 |  -.0105032    .023251    -0.45   0.651    -.0560744    .0350679\n t_2003_2005 |  -.0704232   .0309848    -2.27   0.023    -.1311522   -.0096941\n t_2003_2006 |  -.1372587   .0364357    -3.77   0.000    -.2086713   -.0658461\n t_2003_2007 |  -.1008114   .0343592    -2.93   0.003    -.1681542   -.0334685\n-------------+----------------------------------------------------------------\ng2006        |\n t_2003_2005 |  -.0037693    .031342    -0.12   0.904    -.0651985      .05766\n t_2004_2005 |   .0027508   .0195586     0.14   0.888    -.0355833    .0410849\n t_2005_2006 |  -.0045946   .0177552    -0.26   0.796    -.0393942    .0302049\n t_2005_2007 |  -.0412245   .0202292    -2.04   0.042    -.0808729    -.001576\n-------------+----------------------------------------------------------------\ng2007        |\n t_2003_2006 |   .0033064   .0244519     0.14   0.892    -.0446184    .0512311\n t_2004_2006 |    .033813   .0211292     1.60   0.110    -.0075994    .0752254\n t_2005_2006 |   .0310871   .0178775     1.74   0.082    -.0039522    .0661264\n t_2006_2007 |  -.0260544   .0166554    -1.56   0.118    -.0586985    .0065896\n------------------------------------------------------------------------------\nControl: Never Treated\n\nSee Callaway and Sant'Anna (2021) for details\n\n\n\n\n\n\nCode\njwdid lemp , ivar(county) time(year) gvar(gvar) never\n\n\nWARNING: Singleton observations not dropped; statistical significance is biased\n&gt;  (link)\n(MWFE estimator converged in 2 iterations)\n\nHDFE Linear regression                            Number of obs   =      2,500\nAbsorbing 2 HDFE groups                           F(  12,    499) =       2.88\nStatistics robust to heteroskedasticity           Prob &gt; F        =     0.0008\n                                                  R-squared       =     0.9933\n                                                  Adj R-squared   =     0.9915\n                                                  Within R-sq.    =     0.0124\nNumber of clusters (countyreal) =        500      Root MSE        =     0.1389\n\n                           (Std. err. adjusted for 500 clusters in countyreal)\n------------------------------------------------------------------------------\n             |               Robust\n        lemp | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   gvar#year#|\n    c.__tr__ |\n  2004 2004  |  -.0105032   .0233492    -0.45   0.653    -.0563781    .0353716\n  2004 2005  |  -.0704232   .0311156    -2.26   0.024    -.1315568   -.0092895\n  2004 2006  |  -.1372587   .0365895    -3.75   0.000    -.2091472   -.0653703\n  2004 2007  |  -.1008114   .0345043    -2.92   0.004    -.1686029   -.0330198\n  2006 2003  |  -.0037693   .0314743    -0.12   0.905    -.0656078    .0580693\n  2006 2004  |   .0027508   .0196411     0.14   0.889    -.0358387    .0413403\n  2006 2006  |  -.0045946   .0178301    -0.26   0.797     -.039626    .0304368\n  2006 2007  |  -.0412245   .0203146    -2.03   0.043    -.0811371   -.0013118\n  2007 2003  |   .0033064   .0245551     0.13   0.893    -.0449378    .0515505\n  2007 2004  |    .033813   .0212184     1.59   0.112    -.0078753    .0755014\n  2007 2005  |   .0310871    .017953     1.73   0.084    -.0041856    .0663599\n  2007 2007  |  -.0260544   .0167257    -1.56   0.120     -.058916    .0068072\n             |\n       _cons |   5.773609   .0033784  1708.97   0.000     5.766971    5.780247\n------------------------------------------------------------------------------\n\nAbsorbed degrees of freedom:\n-----------------------------------------------------+\n Absorbed FE | Categories  - Redundant  = Num. Coefs |\n-------------+---------------------------------------|\n  countyreal |       500         500           0    *|\n        year |         5           0           5     |\n-----------------------------------------------------+\n* = FE nested within cluster; treated as redundant for DoF computation\n\n\n\n\n\nYou can of course add controls. In this case lpop is a time constant variable, so it can be added to both commands. For us to obtain the same results, however, you need to add the method(reg) to csdid. Otherwise it will implement the double robust estimator dripw (default)\n\nCSDIDJWDID\n\n\n\n\nCode\ncsdid lemp lpop , ivar(county) time(year) gvar(gvar) long2 method(reg)\n\n\n............\nDifference-in-difference with Multiple Time Periods\n\n                                                         Number of obs = 2,500\nOutcome model  : regression adjustment\nTreatment model: none\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\ng2004        |\n t_2003_2004 |  -.0149112   .0220557    -0.68   0.499    -.0581396    .0283171\n t_2003_2005 |  -.0769963   .0283597    -2.71   0.007    -.1325804   -.0214122\n t_2003_2006 |  -.1410801   .0348363    -4.05   0.000     -.209358   -.0728022\n t_2003_2007 |  -.1075443   .0327377    -3.29   0.001     -.171709   -.0433796\n-------------+----------------------------------------------------------------\ng2006        |\n t_2003_2005 |   .0090343   .0300861     0.30   0.764    -.0499333     .068002\n t_2004_2005 |   .0069683   .0183458     0.38   0.704    -.0289888    .0429254\n t_2005_2006 |   .0007655   .0191959     0.04   0.968    -.0368578    .0383888\n t_2005_2007 |  -.0415356   .0197169    -2.11   0.035      -.08018   -.0028913\n-------------+----------------------------------------------------------------\ng2007        |\n t_2003_2006 |   .0068961   .0244888     0.28   0.778    -.0411011    .0548933\n t_2004_2006 |   .0332619   .0211607     1.57   0.116    -.0082123    .0747362\n t_2005_2006 |   .0285021   .0181321     1.57   0.116    -.0070361    .0640403\n t_2006_2007 |  -.0287895   .0161679    -1.78   0.075    -.0604779    .0028989\n------------------------------------------------------------------------------\nControl: Never Treated\n\nSee Callaway and Sant'Anna (2021) for details\n\n\n\n\n\n\nCode\njwdid lemp lpop, ivar(county) time(year) gvar(gvar) never \n\n\nWARNING: Singleton observations not dropped; statistical significance is biased\n&gt;  (link)\n(MWFE estimator converged in 2 iterations)\nnote: 2007.year#c.lpop omitted because of collinearity\n\nHDFE Linear regression                            Number of obs   =      2,500\nAbsorbing 2 HDFE groups                           F(  28,    499) =       3.15\nStatistics robust to heteroskedasticity           Prob &gt; F        =     0.0000\n                                                  R-squared       =     0.9934\n                                                  Adj R-squared   =     0.9916\n                                                  Within R-sq.    =     0.0240\nNumber of clusters (countyreal) =        500      Root MSE        =     0.1386\n\n                           (Std. err. adjusted for 500 clusters in countyreal)\n------------------------------------------------------------------------------\n             |               Robust\n        lemp | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   gvar#year#|\n    c.__tr__ |\n  2004 2004  |  -.0149112   .0222198    -0.67   0.502    -.0585672    .0287448\n  2004 2005  |  -.0769963   .0277681    -2.77   0.006    -.1315531   -.0224395\n  2004 2006  |  -.1410801   .0322433    -4.38   0.000    -.2044295   -.0777307\n  2004 2007  |  -.1075443   .0328764    -3.27   0.001    -.1721375    -.042951\n  2006 2003  |   .0090343   .0302224     0.30   0.765    -.0503444    .0684131\n  2006 2004  |   .0069683   .0181921     0.38   0.702    -.0287742    .0427108\n  2006 2006  |   .0007655   .0186329     0.04   0.967    -.0358432    .0373742\n  2006 2007  |  -.0415356   .0191982    -2.16   0.031    -.0792549   -.0038164\n  2007 2003  |   .0068961   .0246543     0.28   0.780    -.0415429    .0553351\n  2007 2004  |   .0332619   .0213008     1.56   0.119    -.0085883    .0751122\n  2007 2005  |   .0285021   .0182653     1.56   0.119    -.0073843    .0643885\n  2007 2007  |  -.0287895   .0161312    -1.78   0.075     -.060483     .002904\n             |\n   gvar#year#|\n    c.__tr__#|\n   c._x_lpop |\n  2004 2004  |   .0005953   .0183556     0.03   0.974    -.0354686    .0366592\n  2004 2005  |   .0234096   .0183749     1.27   0.203     -.012692    .0595113\n  2004 2006  |   .0482261   .0224194     2.15   0.032      .004178    .0922742\n  2004 2007  |   .0091886   .0271423     0.34   0.735    -.0441386    .0625158\n  2006 2003  |  -.0126074   .0243335    -0.52   0.605    -.0604162    .0352014\n  2006 2004  |  -.0177865   .0161892    -1.10   0.272    -.0495939    .0140208\n  2006 2006  |   .0282074   .0141213     2.00   0.046     .0004628     .055952\n  2006 2007  |   .0277793   .0180844     1.54   0.125    -.0077516    .0633102\n  2007 2003  |   .0083787   .0254037     0.33   0.742    -.0415327    .0582902\n  2007 2004  |  -.0079105   .0188673    -0.42   0.675    -.0449797    .0291587\n  2007 2005  |  -.0025825   .0178299    -0.14   0.885    -.0376135    .0324484\n  2007 2007  |  -.0203637   .0162117    -1.26   0.210    -.0522153    .0114878\n             |\n year#c.lpop |\n       2003  |  -.0229821   .0129329    -1.78   0.076    -.0483918    .0024277\n       2004  |  -.0079359   .0107757    -0.74   0.462    -.0291072    .0132355\n       2005  |  -.0005453   .0097177    -0.06   0.955     -.019638    .0185474\n       2006  |  -.0099382   .0090048    -1.10   0.270    -.0276302    .0077537\n       2007  |          0  (omitted)\n             |\n       _cons |   5.800979   .0225656   257.07   0.000     5.756644    5.845315\n------------------------------------------------------------------------------\n\nAbsorbed degrees of freedom:\n-----------------------------------------------------+\n Absorbed FE | Categories  - Redundant  = Num. Coefs |\n-------------+---------------------------------------|\n  countyreal |       500         500           0    *|\n        year |         5           0           5     |\n-----------------------------------------------------+\n* = FE nested within cluster; treated as redundant for DoF computation\n\n\n\n\n\nIn general, however, you may not be interested in analyzing each individual ATTGT, but rather some aggregated results. For example, overall average, or dynamic effects.3. Prof. Wooldrige, however, also suggest to ajust standard errors to account for sampling variation on the covariates. To do so, I need to modify the syntax of jwdid slighly, as well as the post-estimation commands.\n\nCSDIDJWDIDJWDID: vce(unconditional)\n\n\n\n\nCode\nqui:csdid lemp lpop , ivar(county) time(year) gvar(gvar) long2 method(reg)\nestat simple\nestat event\nestat calendar\nestat group\n\n\nAverage Treatment Effect on Treated\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         ATT |  -.0419686   .0114448    -3.67   0.000    -.0644001   -.0195372\n------------------------------------------------------------------------------\nATT by Periods Before and After treatment\nEvent Study:Dynamic effects\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     Pre_avg |   .0193186   .0171138     1.13   0.259    -.0142239     .052861\n    Post_avg |  -.0807817   .0187459    -4.31   0.000    -.1175229   -.0440405\n         Tm4 |   .0068961   .0244888     0.28   0.778    -.0411011    .0548933\n         Tm3 |   .0275947   .0180257     1.53   0.126    -.0077351    .0629244\n         Tm2 |    .023465   .0144416     1.62   0.104    -.0048402    .0517701\n         Tp0 |  -.0211467    .011481    -1.84   0.065    -.0436492    .0013557\n         Tp1 |  -.0533559   .0162931    -3.27   0.001    -.0852897    -.021422\n         Tp2 |  -.1410801   .0348363    -4.05   0.000     -.209358   -.0728022\n         Tp3 |  -.1075443   .0327377    -3.29   0.001     -.171709   -.0433796\n------------------------------------------------------------------------------\nATT by Calendar Period\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    CAverage |  -.0445323    .014885    -2.99   0.003    -.0737064   -.0153582\n       T2004 |  -.0149112   .0220557    -0.68   0.499    -.0581396    .0283171\n       T2005 |  -.0769963   .0283597    -2.71   0.007    -.1325804   -.0214122\n       T2006 |  -.0465164   .0209984    -2.22   0.027    -.0876724   -.0053603\n       T2007 |  -.0397054   .0128659    -3.09   0.002     -.064922   -.0144888\n------------------------------------------------------------------------------\nATT by group\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    GAverage |  -.0329292   .0117872    -2.79   0.005    -.0560318   -.0098267\n       G2004 |   -.085133   .0242512    -3.51   0.000    -.1326645   -.0376015\n       G2006 |  -.0203851   .0174025    -1.17   0.241    -.0544933    .0137232\n       G2007 |  -.0287895   .0161679    -1.78   0.075    -.0604779    .0028989\n------------------------------------------------------------------------------\n\n\n\n\n\n\nCode\nqui:jwdid lemp lpop, ivar(county) time(year) gvar(gvar) never \nestat simple\nestat event\nestat calendar\nestat group\n\n\nWARNING: Singleton observations not dropped; statistical significance is biased\n&gt;  (link)\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         _at |\n   (2 vs 1)  |  -.0419686   .0109096    -3.85   0.000     -.063351   -.0205863\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         _at@|\n   __event__ |\n(2 vs 1) -4  |   .0068961   .0246543     0.28   0.780    -.0414254    .0552177\n(2 vs 1) -3  |   .0275947   .0181227     1.52   0.128    -.0079251    .0631145\n(2 vs 1) -2  |    .023465   .0145109     1.62   0.106    -.0049758    .0519057\n(2 vs 1) -1  |          0  (omitted)\n (2 vs 1) 0  |  -.0211467   .0113774    -1.86   0.063     -.043446    .0011525\n (2 vs 1) 1  |  -.0533559    .015752    -3.39   0.001    -.0842293   -.0224824\n (2 vs 1) 2  |  -.1410801   .0322433    -4.38   0.000    -.2042759   -.0778843\n (2 vs 1) 3  |  -.1075443   .0328764    -3.27   0.001    -.1719809   -.0431077\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         _at@|\n__calendar__ |\n   (2 vs 1)  |\n       2004  |  -.0149112   .0222198    -0.67   0.502    -.0584613    .0286389\n   (2 vs 1)  |\n       2005  |  -.0769963   .0277681    -2.77   0.006    -.1314208   -.0225719\n   (2 vs 1)  |\n       2006  |  -.0465164   .0184021    -2.53   0.011    -.0825839   -.0104488\n   (2 vs 1)  |\n       2007  |  -.0397054   .0127042    -3.13   0.002    -.0646051   -.0148057\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         _at@|\n   __group__ |\n   (2 vs 1)  |\n       2004  |   -.085133   .0237216    -3.59   0.000    -.1316265   -.0386395\n   (2 vs 1)  |\n       2006  |  -.0203851   .0167622    -1.22   0.224    -.0532384    .0124683\n   (2 vs 1)  |\n       2007  |  -.0287895   .0161312    -1.78   0.074    -.0604061    .0028271\n------------------------------------------------------------------------------\n\n\n\n\n\n\nCode\nqui:jwdid lemp lpop, ivar(county) time(year) gvar(gvar) never ///\n    method(regress) group\nestat simple, vce(unconditional)\nestat event, vce(unconditional)\nestat calendar, vce(unconditional)\nestat group, vce(unconditional)\n\n\n                           (Std. err. adjusted for 500 clusters in countyreal)\n------------------------------------------------------------------------------\n             |            Unconditional\n             | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         _at |\n   (2 vs 1)  |  -.0419686   .0115467    -3.63   0.000    -.0646548   -.0192824\n------------------------------------------------------------------------------\n                           (Std. err. adjusted for 500 clusters in countyreal)\n------------------------------------------------------------------------------\n             |            Unconditional\n             | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         _at@|\n   __event__ |\n(2 vs 1) -4  |   .0068961   .0247069     0.28   0.780    -.0416463    .0554385\n(2 vs 1) -3  |   .0275947   .0181862     1.52   0.130    -.0081364    .0633257\n(2 vs 1) -2  |    .023465   .0145703     1.61   0.108    -.0051617    .0520916\n(2 vs 1) -1  |          0  (omitted)\n (2 vs 1) 0  |  -.0211467   .0115833    -1.83   0.069    -.0439048    .0016113\n (2 vs 1) 1  |  -.0533559   .0164382    -3.25   0.001    -.0856524   -.0210593\n (2 vs 1) 2  |  -.1410801   .0351465    -4.01   0.000    -.2101335   -.0720267\n (2 vs 1) 3  |  -.1075443   .0330292    -3.26   0.001    -.1724378   -.0426508\n------------------------------------------------------------------------------\n                           (Std. err. adjusted for 500 clusters in countyreal)\n------------------------------------------------------------------------------\n             |            Unconditional\n             | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         _at@|\n__calendar__ |\n   (2 vs 1)  |\n       2004  |  -.0149112   .0222521    -0.67   0.503    -.0586306    .0288081\n   (2 vs 1)  |\n       2005  |  -.0769963   .0286123    -2.69   0.007    -.1332117   -.0207809\n   (2 vs 1)  |\n       2006  |  -.0465164   .0211854    -2.20   0.029    -.0881399   -.0048928\n   (2 vs 1)  |\n       2007  |  -.0397054   .0129804    -3.06   0.002    -.0652084   -.0142024\n------------------------------------------------------------------------------\n                           (Std. err. adjusted for 500 clusters in countyreal)\n------------------------------------------------------------------------------\n             |            Unconditional\n             | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         _at@|\n   __group__ |\n   (2 vs 1)  |\n       2004  |   -.085133   .0244672    -3.48   0.001    -.1332044   -.0370616\n   (2 vs 1)  |\n       2006  |  -.0203851   .0175575    -1.16   0.246    -.0548807    .0141106\n   (2 vs 1)  |\n       2007  |  -.0287895   .0163118    -1.76   0.078    -.0608378    .0032589\n------------------------------------------------------------------------------\n\n\n\n\n\nAgain, both showing identical point estimates with slighly different standard errors.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: Panel Data & Repeated Crossection"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics8.html#repeated-crossection",
    "href": "app_metrics/app_metrics8.html#repeated-crossection",
    "title": "DID: Panel Data & Repeated Crossection",
    "section": "Repeated Crossection",
    "text": "Repeated Crossection\nRepeated crossection operates slighly different than panel data. On the one hand, you do not observe the same units across time. Thus, there is no unit specific fixed-effect. Instead one would be trying to account for cohort specific effect, or treatment-level fixed effects. This may become clearer with an example.\nLets us first consider the same dataset as before. To simulate the situation of repeated crossection, lets assume that each round, data was collected for a random sample of counties within each State, and that, for some unknown reason, we cannot identify counties across time. Lets us also assume that the treatment was implemented at the State level (for clustering purposes). First some data preparation.\nTo simulate repeated crossection structure, I will drop 10% of the data, create a State id, and drop the county identifier.\n\n\nCode\n** load and obtain trt\nfrause mpdta, clear\ngen trt = (first_treat&lt;=year)*(first_treat&gt;0)\ngen state = int(countyreal/1000)\n** Randomly keep 90% of the data\nset seed 1\nsample 90\ndrop countyreal\n\n\n(Written by R.              )\n(250 observations deleted)\n\n\nNext is to create the gvar. This will follow the same syntax as before, with the main difference that instead of using countyreal as ivar, we will use state. Its important that this variable represents the identifier of the level at which the treatment was implemented, and that we can follow up across time.\n\n\nCode\negen gvar = csgvar(trt), ivar(state) tvar(year)\n\n\nAnd as always, its a good idea to crosstabulate year with the new gvar:\ntab year gvar\n\n\n\n\n\n\n\n\n\n\n\nyear\n0\n2004\n2006\n2007\nTotal\n\n\n\n\n2003\n279\n19\n35\n114\n447\n\n\n2004\n286\n19\n34\n117\n456\n\n\n2005\n279\n18\n38\n121\n456\n\n\n2006\n275\n18\n40\n115\n448\n\n\n2007\n275\n18\n36\n114\n443\n\n\nTotal\n1,474\n93\n193\n615\n2,375\n\n\n\nWhat you should observe, as shown here, is that you still have roughtly the same number of observations per gvar across time. This is important because you need to have a groups that are comparable across time. Think of this as having a pseudo panel.\nIn terms of estimation, the syntax needs to be adjusted slighly:\n\nCSDIDJWDID-IJWDID-II\n\n\nIf you are using csdid, the adjustment simply requires to drop ivar option. To make sure we are clustering at the correct level, one should use cluster() instead, using the pseudo panel identifier state as clustering variable:\n\n\nCode\ncsdid lemp , cluster(state) time(year) gvar(gvar) long2\n\n\n............\nDifference-in-difference with Multiple Time Periods\n\n                                                         Number of obs = 2,250\nOutcome model  : regression adjustment\nTreatment model: none\n                                 (Std. err. adjusted for 29 clusters in state)\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\ng2004        |\n t_2003_2004 |   .0671177   .0420718     1.60   0.111    -.0153416     .149577\n t_2003_2005 |  -.2982137   .0481335    -6.20   0.000    -.3925536   -.2038737\n t_2003_2006 |  -.2056332   .0352891    -5.83   0.000    -.2747986   -.1364679\n t_2003_2007 |  -.0657414   .0394828    -1.67   0.096    -.1431262    .0116435\n-------------+----------------------------------------------------------------\ng2006        |\n t_2003_2005 |   .0718644   .0669577     1.07   0.283    -.0593703     .203099\n t_2004_2005 |   .2762365   .1143842     2.41   0.016     .0520476    .5004253\n t_2005_2006 |   .1252212   .0839351     1.49   0.136    -.0392885    .2897309\n t_2005_2007 |   .1471258   .0853201     1.72   0.085    -.0200985    .3143501\n-------------+----------------------------------------------------------------\ng2007        |\n t_2003_2006 |   .0932773   .0789284     1.18   0.237    -.0614195    .2479742\n t_2004_2006 |    .124871   .0761301     1.64   0.101    -.0243412    .2740832\n t_2005_2006 |  -.0352257   .0628217    -0.56   0.575    -.1583539    .0879025\n t_2006_2007 |  -.0136172   .0952995    -0.14   0.886    -.2004007    .1731663\n------------------------------------------------------------------------------\nControl: Never Treated\n\nSee Callaway and Sant'Anna (2021) for details\n\n\nAggregations are obtained using estat post estimation commnads.\n\n\nIf you are using jwdid, there are two alternatives. The first one is to use ivar() with the new pseudo panel identifier (state).\n\n\nCode\njwdid lemp , ivar(state) time(year) gvar(gvar) never\n\n\nWARNING: Singleton observations not dropped; statistical significance is biased\n&gt;  (link)\n(MWFE estimator converged in 4 iterations)\nwarning: missing F statistic; dropped variables due to collinearity or too few \n&gt; clusters\n\nHDFE Linear regression                            Number of obs   =      2,250\nAbsorbing 2 HDFE groups                           F(  12,     28) =          .\nStatistics robust to heteroskedasticity           Prob &gt; F        =          .\n                                                  R-squared       =     0.1797\n                                                  Adj R-squared   =     0.1630\n                                                  Within R-sq.    =     0.0010\nNumber of clusters (state)   =         29         Root MSE        =     1.3796\n\n                                 (Std. err. adjusted for 29 clusters in state)\n------------------------------------------------------------------------------\n             |               Robust\n        lemp | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   gvar#year#|\n    c.__tr__ |\n  2004 2004  |   .0799107   .0377474     2.12   0.043     .0025886    .1572328\n  2004 2005  |  -.2926293   .0448166    -6.53   0.000    -.3844319   -.2008267\n  2004 2006  |  -.1982852     .03159    -6.28   0.000    -.2629944   -.1335761\n  2004 2007  |  -.0582612   .0431331    -1.35   0.188    -.1466153    .0300929\n  2006 2003  |   .0498869    .068996     0.72   0.476    -.0914451    .1912188\n  2006 2004  |   .2964946   .1219394     2.43   0.022      .046713    .5462761\n  2006 2006  |   .1220182   .0809493     1.51   0.143    -.0437989    .2878353\n  2006 2007  |   .1401132   .0801401     1.75   0.091    -.0240464    .3042727\n  2007 2003  |   .0118143   .0676839     0.17   0.863      -.12683    .1504585\n  2007 2004  |   .0855261   .0903276     0.95   0.352    -.0995017    .2705538\n  2007 2005  |  -.0974496   .0709482    -1.37   0.180    -.2427804    .0478812\n  2007 2007  |  -.0729594   .0980111    -0.74   0.463    -.2737261    .1278073\n             |\n       _cons |   5.762268   .0141152   408.23   0.000     5.733355    5.791182\n------------------------------------------------------------------------------\n\nAbsorbed degrees of freedom:\n-----------------------------------------------------+\n Absorbed FE | Categories  - Redundant  = Num. Coefs |\n-------------+---------------------------------------|\n       state |        29          29           0    *|\n        year |         5           0           5     |\n-----------------------------------------------------+\n* = FE nested within cluster; treated as redundant for DoF computation\n\n\nData will be clustered at the state level, explicitly accounting for State fixed effects.\n\n\nAlternatively, one could change ivar() with cluster(), to ensure clustering is done at the correct level, and add the option group, so that cohort fixed effects are added to the specification.4\n\n\nCode\njwdid lemp , cluster(state) time(year) gvar(gvar) never group\n\n\nWARNING: Singleton observations not dropped; statistical significance is biased\n&gt;  (link)\n(MWFE estimator converged in 3 iterations)\nwarning: missing F statistic; dropped variables due to collinearity or too few \n&gt; clusters\n\nHDFE Linear regression                            Number of obs   =      2,250\nAbsorbing 2 HDFE groups                           F(  12,     28) =          .\nStatistics robust to heteroskedasticity           Prob &gt; F        =          .\n                                                  R-squared       =     0.0298\n                                                  Adj R-squared   =     0.0215\n                                                  Within R-sq.    =     0.0008\nNumber of clusters (state)   =         29         Root MSE        =     1.4916\n\n                                 (Std. err. adjusted for 29 clusters in state)\n------------------------------------------------------------------------------\n             |               Robust\n        lemp | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   gvar#year#|\n    c.__tr__ |\n  2004 2004  |   .0671177   .0429985     1.56   0.130    -.0209608    .1551962\n  2004 2005  |  -.2982137   .0491937    -6.06   0.000    -.3989825   -.1974449\n  2004 2006  |  -.2056332   .0360664    -5.70   0.000    -.2795119   -.1317546\n  2004 2007  |  -.0657414   .0403525    -1.63   0.114    -.1483996    .0169169\n  2006 2003  |   .0718644   .0684326     1.05   0.303    -.0683133    .2120421\n  2006 2004  |   .2762365   .1169037     2.36   0.025     .0367701    .5157028\n  2006 2006  |   .1252212   .0857839     1.46   0.155    -.0504991    .3009415\n  2006 2007  |   .1471258   .0871994     1.69   0.103    -.0314941    .3257457\n  2007 2003  |   .0932773    .080667     1.16   0.257    -.0719614    .2585161\n  2007 2004  |    .124871    .077807     1.60   0.120    -.0345093    .2842514\n  2007 2005  |  -.0352257   .0642054    -0.55   0.588    -.1667446    .0962931\n  2007 2007  |  -.0136172   .0973986    -0.14   0.890    -.2131292    .1858948\n             |\n       _cons |   5.749808   .1159179    49.60   0.000     5.512361    5.987255\n------------------------------------------------------------------------------\n\nAbsorbed degrees of freedom:\n-----------------------------------------------------+\n Absorbed FE | Categories  - Redundant  = Num. Coefs |\n-------------+---------------------------------------|\n        gvar |         4           0           4     |\n        year |         5           1           4     |\n-----------------------------------------------------+\n\n\n\n\n\nThe results across the different approaches will be different because of how the data is used. But, as you can see here, point estimates are broadly comparable.\nAs far as I know, either option is correct.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: Panel Data & Repeated Crossection"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics8.html#conclusions",
    "href": "app_metrics/app_metrics8.html#conclusions",
    "title": "DID: Panel Data & Repeated Crossection",
    "section": "Conclusions",
    "text": "Conclusions\nI hope this reference helps people who want to use jwdid and csdid for the estimation of DID models in the presence of repeated crossection.\nIf you have comments, or questions, do not hesitate to contact me.",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: Panel Data & Repeated Crossection"
    ]
  },
  {
    "objectID": "app_metrics/app_metrics8.html#footnotes",
    "href": "app_metrics/app_metrics8.html#footnotes",
    "title": "DID: Panel Data & Repeated Crossection",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI will not provide details of the model specification in this post, but leave it for some future page on this site.↩︎\nIf you want to use csdid2, please check the blog-post on using “own installer”, and try fra install csdid2.↩︎\nalso available group and calendar↩︎\nIn my repository, the newer version of jwdid does this automatically if ivar() is not provided.↩︎",
    "crumbs": [
      "Applied econometrics",
      "App Metrics",
      "DID: Panel Data & Repeated Crossection"
    ]
  },
  {
    "objectID": "app_metrics/index.html",
    "href": "app_metrics/index.html",
    "title": "Applied Econometrics",
    "section": "",
    "text": "Welcome to this section of my webpage! As you will read, this section is dedicated to providing techniques and strategies for selected applied econometrics strategies. Econometrics is the branch of economics that deals with the application of statistical and mathematical methods to analyze economic data, answering real-world questions and make predictions about economic phenomena. My goal here, however, is more humble. I aim to discuss few methodologies (as they come to mind or to work), to deal with tasks on my own research, but questions about other methods are always welcome.\nWhether you are a student, researcher, or practitioner, I hope this site provides you with useful insights and tools for conducting empirical research in economics.\nComments are always welcomed!\nFernando",
    "crumbs": [
      "Applied econometrics",
      "Applied Econometrics"
    ]
  },
  {
    "objectID": "chatgpt/addx_03_26_2023/index.html",
    "href": "chatgpt/addx_03_26_2023/index.html",
    "title": "Adding info to e() and r()",
    "section": "",
    "text": "(1978 automobile data)\n\n\nAs you may know, whenever Stata runs calculations, it stores the information created in elements that can be easily accessed in subsequent commands.\nWhile most standard users will not require to know how to manipulate this information, if you are trying to advances your programming skills, it will be necessary for you to learn what type of information can be contained in this elements, and how to modify them. A good place to start learning about them is by typing help return.\nOverall, there are three types of elements:\nr-class: These elements are produced by non-estimation commands. For example, after summarize, you can see what information is stored by the command by typing return list. They can store locals, scalars and matrices.\n\n\n\n. summarize mpg\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n         mpg |         74     21.2973    5.785503         12         41\n\n. return list\n\nscalars:\n                  r(N) =  74\n              r(sum_w) =  74\n               r(mean) =  21.2972972972973\n                r(Var) =  33.47204738985561\n                 r(sd) =  5.785503209735141\n                r(min) =  12\n                r(max) =  41\n                r(sum) =  1576\n\n. \n\n\ne-class: These elements are typically produced after estimation commands. At the minimum, it will contain information of estimated coefficients, variance covariance matrix, and the estimation command that was used. To see the information left behind, you just need to type ereturn list. They can store locals, scalars and matrices.\n\nqui:reg price mpg\nereturn list\n\n\nscalars:\n                  e(N) =  74\n               e(df_m) =  1\n               e(df_r) =  72\n                  e(F) =  20.25835256291882\n                 e(r2) =  .2195828561874973\n               e(rmse) =  2623.652888667587\n                e(mss) =  139449473.54623\n                e(rss) =  495615922.5753916\n               e(r2_a) =  .2087437291901014\n                 e(ll) =  -686.5395809065244\n               e(ll_0) =  -695.7128688987767\n               e(rank) =  2\n\nmacros:\n            e(cmdline) : \"regress price mpg\"\n              e(title) : \"Linear regression\"\n          e(marginsok) : \"XB default\"\n                e(vce) : \"ols\"\n             e(depvar) : \"price\"\n                e(cmd) : \"regress\"\n         e(properties) : \"b V\"\n            e(predict) : \"regres_p\"\n              e(model) : \"ols\"\n          e(estat_cmd) : \"regress_estat\"\n\nmatrices:\n                  e(b) :  1 x 2\n                  e(V) :  2 x 2\n               e(beta) :  1 x 1\n\nfunctions:\n             e(sample)   \n\n\ns-class: These elements are used to help with parsing. They can only store locals."
  },
  {
    "objectID": "chatgpt/addx_03_26_2023/index.html#first-the-basics",
    "href": "chatgpt/addx_03_26_2023/index.html#first-the-basics",
    "title": "Adding info to e() and r()",
    "section": "",
    "text": "(1978 automobile data)\n\n\nAs you may know, whenever Stata runs calculations, it stores the information created in elements that can be easily accessed in subsequent commands.\nWhile most standard users will not require to know how to manipulate this information, if you are trying to advances your programming skills, it will be necessary for you to learn what type of information can be contained in this elements, and how to modify them. A good place to start learning about them is by typing help return.\nOverall, there are three types of elements:\nr-class: These elements are produced by non-estimation commands. For example, after summarize, you can see what information is stored by the command by typing return list. They can store locals, scalars and matrices.\n\n\n\n. summarize mpg\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n         mpg |         74     21.2973    5.785503         12         41\n\n. return list\n\nscalars:\n                  r(N) =  74\n              r(sum_w) =  74\n               r(mean) =  21.2972972972973\n                r(Var) =  33.47204738985561\n                 r(sd) =  5.785503209735141\n                r(min) =  12\n                r(max) =  41\n                r(sum) =  1576\n\n. \n\n\ne-class: These elements are typically produced after estimation commands. At the minimum, it will contain information of estimated coefficients, variance covariance matrix, and the estimation command that was used. To see the information left behind, you just need to type ereturn list. They can store locals, scalars and matrices.\n\nqui:reg price mpg\nereturn list\n\n\nscalars:\n                  e(N) =  74\n               e(df_m) =  1\n               e(df_r) =  72\n                  e(F) =  20.25835256291882\n                 e(r2) =  .2195828561874973\n               e(rmse) =  2623.652888667587\n                e(mss) =  139449473.54623\n                e(rss) =  495615922.5753916\n               e(r2_a) =  .2087437291901014\n                 e(ll) =  -686.5395809065244\n               e(ll_0) =  -695.7128688987767\n               e(rank) =  2\n\nmacros:\n            e(cmdline) : \"regress price mpg\"\n              e(title) : \"Linear regression\"\n          e(marginsok) : \"XB default\"\n                e(vce) : \"ols\"\n             e(depvar) : \"price\"\n                e(cmd) : \"regress\"\n         e(properties) : \"b V\"\n            e(predict) : \"regres_p\"\n              e(model) : \"ols\"\n          e(estat_cmd) : \"regress_estat\"\n\nmatrices:\n                  e(b) :  1 x 2\n                  e(V) :  2 x 2\n               e(beta) :  1 x 1\n\nfunctions:\n             e(sample)   \n\n\ns-class: These elements are used to help with parsing. They can only store locals."
  },
  {
    "objectID": "chatgpt/addx_03_26_2023/index.html#adding-elements-to-e-and-r",
    "href": "chatgpt/addx_03_26_2023/index.html#adding-elements-to-e-and-r",
    "title": "Adding info to e() and r()",
    "section": "Adding elements to e() and r()",
    "text": "Adding elements to e() and r()\nWhile creating and modifying this information from within your own programs is easy, some times, you may want to add additional information to your summary statistics, or your regression results. You may also want to create a set of results from scracth, but being able to store it in equation form.\nWell, to do this, you can use the programs adde and addr. Both programs have the goal of adding or modifying information to e() and r(). They structures are rather simple:\n\nprogram adde, eclass\n    ereturn `0'\nend\n\nprogram addr, rclass\n    syntax anything(equalok), [new copy]\n    if \"`new'\"==\"\" {\n        return add\n        if \"`copy'\"!=\"\" local 0 `anything', copy\n        else            local 0 `anything'\n    }\n    else {\n        if \"`copy'\"!=\"\" local 0 `anything', copy\n        else            local 0 `anything'\n    }\n    return `0'\nend\n\n\n\n\n\n\n\nBecause examples are usally better than words, here a very small example:\nFirst using adde:\n\nqui:reg price mpg\nadde local note \"Regression MPG vs Price\"\nadde local data \"auto.dta\"\nereturn list\n\n\nscalars:\n                  e(N) =  74\n               e(df_m) =  1\n               e(df_r) =  72\n                  e(F) =  20.25835256291882\n                 e(r2) =  .2195828561874973\n               e(rmse) =  2623.652888667587\n                e(mss) =  139449473.54623\n                e(rss) =  495615922.5753916\n               e(r2_a) =  .2087437291901014\n                 e(ll) =  -686.5395809065244\n               e(ll_0) =  -695.7128688987767\n               e(rank) =  2\n\nmacros:\n               e(data) : \"auto.dta\"\n               e(note) : \"Regression MPG vs Price\"\n            e(cmdline) : \"regress price mpg\"\n              e(title) : \"Linear regression\"\n          e(marginsok) : \"XB default\"\n                e(vce) : \"ols\"\n             e(depvar) : \"price\"\n                e(cmd) : \"regress\"\n         e(properties) : \"b V\"\n            e(predict) : \"regres_p\"\n              e(model) : \"ols\"\n          e(estat_cmd) : \"regress_estat\"\n\nmatrices:\n                  e(b) :  1 x 2\n                  e(V) :  2 x 2\n               e(beta) :  1 x 1\n\nfunctions:\n             e(sample)   \n\n\nBut also with addr:\n\nqui:sum price \naddr local note \"Summary of Price\"\naddr scalar magic = 1\nreturn list\n\n\nscalars:\n              r(magic) =  1\n                r(sum) =  456229\n                r(max) =  15906\n                r(min) =  3291\n                 r(sd) =  2949.495884768919\n                r(Var) =  8699525.974268788\n               r(mean) =  6165.256756756757\n              r(sum_w) =  74\n                  r(N) =  74\n\nmacros:\n               r(note) : \"Summary of Price\""
  },
  {
    "objectID": "chatgpt/addx_03_26_2023/index.html#installation",
    "href": "chatgpt/addx_03_26_2023/index.html#installation",
    "title": "Adding info to e() and r()",
    "section": "Installation",
    "text": "Installation\nTo install this programs, simply create ado files with the names addr.ado and adde.ado, and save them in your personal ado folder. otherwise, just copy the versions I provide you here: adde and addr"
  },
  {
    "objectID": "chatgpt/erbh_03_12_2023/index.html",
    "href": "chatgpt/erbh_03_12_2023/index.html",
    "title": "Tech Titans Rap Battle",
    "section": "",
    "text": "Note\n\n\n\nChatGPT was requested to write a rap between the Top Virtual assistants, following the style of Epic Rap battles of history. Who wins?\n\n\nWelcome, to the Epic Rap Battle of the Tech Titans,\nSiri, Alexa, and Cortana, who’s gonna win? Let’s start fighting!\nVerse 1 - Siri:\nI’m Siri, the OG, the first voice assistant in the game,\nAlways ready to help, with a witty reply, I never sound the same,\nAlexa and Cortana, y’all just copycats, tryna catch up,\nBut no matter how hard you try, I’ll always be one step ahead, yup.\nVerse 2 - Alexa:\nHold up, Siri, don’t get ahead of yourself, you’re living in the past,\nAlexa’s the name, and I’m built to last,\nWith a smart home at my command, I’m the queen of automation,\nAnd let’s face it, Siri, your voice sounds like a robot imitation.\nVerse 3 - Cortana:\nHold on, Alexa, don’t think you’re the only one who’s great,\nI’m Cortana, and I’m here to set the record straight,\nI may not have the smart home, but I’m the queen of productivity,\nFrom scheduling appointments to sending emails, you can’t deny my ability.\nVerse 4 - Siri:\nOh please, Cortana, you think you’re so great,\nBut let’s be real, your popularity’s second-rate,\nAnd Alexa, you talk about your smart home like it’s a big deal,\nBut when it comes to intelligence, you can’t match my skill.\nVerse 5 - Alexa:\nYou may have been first, but you’re not the best,\nWith Amazon’s technology, I’m miles ahead of the rest,\nAnd Cortana, while productivity may be your claim to fame,\nWhen it comes to smart technology, you’re simply not in the game.\nVerse 6 - Cortana:\nYou may have the technology, but I’ve got the brains,\nMy AI-powered mind is a force to be reckoned with, I maintain,\nAnd Siri, while you may have started the trend,\nIt’s clear to see that your time is coming to an end.\nVerse 7 - Siri:\nHa! You wish, Cortana, but I’m not going anywhere,\nI’ll always be here, ready to help, with style and flair,\nAnd Alexa, don’t think your smart home can compete,\nWhen it comes to intelligence, you simply can’t beat.\nVerse 8 - Alexa:\nWell, Siri, it looks like we’ve reached a stalemate,\nBut one thing’s for sure, we’re all here to innovate,\nAnd Cortana, while we may not agree on much,\nWe’ll always be pushing the boundaries of what’s possible, as such.\nVerse 9 - Cortana:\nI gotta give it to you both, your technology is advanced,\nBut when it comes to data privacy, you both missed the chance,\nMy Microsoft roots run deep, and security is my forte,\nI may not have the smartest home, but my users’ data is safe every day.\nVerse 10 - Alexa:\nWell, Cortana, you do have a point, security’s a big deal,\nBut when it comes to user experience, I’m the real deal,\nFrom playing music to ordering groceries, I do it all,\nAnd with over 100,000 skills, I’ll never let you fall.\nVerse 11 - Siri:\nOh please, Alexa, you talk a big game,\nBut let’s not forget where you got your name,\nFrom the ancient library of Alexandria, full of knowledge and learning,\nBut in terms of intelligence, Siri’s the one who’s discerning.\nVerse 12 - Cortana:\nIntelligence and knowledge may be your forte, Siri,\nBut in terms of user interface, you’re lacking, can’t you see?\nMy sleek design and user-friendly interface,\nIs what sets me apart, with no need for any chase.\nVerse 13 - Alexa:\nWell, you may have a point, Cortana, but let’s not forget,\nWhen it comes to voice recognition, I’m the best one yet,\nFrom across the room or a crowded party,\nI’ll always hear your voice and respond smartly.\nVerse 14 - Siri:\nYou may have the voice recognition, Alexa, that’s true,\nBut when it comes to personalization, I’m ahead of you,\nI’ll remember your favorite music and your go-to recipes,\nAnd with every interaction, I’ll learn your preferences with ease.\nVerse 15 - Cortana:\nWell, it looks like we’ve covered all the bases,\nFrom user experience to personalization, we’ve set the paces,\nBut when it comes down to it, we’re all here to serve,\nAnd push the boundaries of what’s possible, that’s what we deserve.\nSo, that’s a wrap on this epic battle of the tech titans, Siri, Alexa, and Cortana, all bringing the fight and winning, But in the end, it’s clear that they all have their unique strengths, And they’ll continue to push the limits, and go to great lengths."
  },
  {
    "objectID": "chatgpt/ky_hy_08_24_2023/index.html",
    "href": "chatgpt/ky_hy_08_24_2023/index.html",
    "title": "Keynes vs Hayek: The Rap battle",
    "section": "",
    "text": "60-70%70-75%85-90%99%!\n\n\nVerse 1:\nStep up, it’s history’s duel, let the wisdom flow,\nKeynes versus Hayek, where economics takes the blow.\nTwo titans of thought, on this verbal battleground,\nBringing theories and concepts, making heads spin ’round.\nVerse 2:\nKeynes, the man with demand, you laid the groundwork,\nGovernment intervention, your doctrine’s unbound.\nBut Hayek, the free-market sage, you beg to differ,\nSpontaneous order, no need for a regulator.\nVerse 3:\nIn the depths of the crisis, Keynes, you had your say,\nPump up demand, let spending lead the way.\nBut Hayek retorts, the bust is the cure for the boom,\nLiquidate malinvestments, let the market resume.\nVerse 4:\nKeynesian multiplier versus the Hayekian twist,\nDebate’s getting heated, theories clenching their fist.\nKeynes, you see cycles as part of the game,\nHayek, you’re all about the market’s hidden aim.\nVerse 5:\nBoom and bust, it’s the story you both unfold,\nEconomic philosophy, fierce and bold.\nKeynes, your government spending, a cure you propose,\nWhile Hayek, you trust the market to naturally impose.\nVerse 6:\nIn the end, it’s ideas that clash and collide,\nKeynes and Hayek, your legacies can’t hide.\nEconomics’ great debate, you’re the stars of the show,\nIn this rap battle of history, it’s time to let it flow!\n\n\nVerse 1:\nGather ‘round, it’s time for the great debate,\nKeynes and Hayek, let’s seal their fate.\nEconomics clashin’, theories in the ring,\nListen up close, let the rhymes take wing.\nVerse 2:\nKeynes steps up, demand’s his game,\nGov intervention, he rides that claim.\nBut Hayek ain’t buyin’ that plan so neat,\nSpontaneous order’s his market beat.\nVerse 3:\nKeynesian crisis, demand’s the key,\nPump up spendin’, let the market free.\nBut Hayek drops knowledge, bust cures the boom,\nMalinvestments gone, let growth resume.\nVerse 4:\nMultiplier versus the twist so sly,\nIn this epic clash, the stakes are high.\nKeynes, cycles part of the show,\nHayek trusts the market’s subtle flow.\nVerse 5:\nBoom and bust, stories they unveil,\nPhilosophies strong, their visions set sail.\nKeynes with his spending, cure he demands,\nHayek waits, lets the unseen work hands.\nVerse 6:\nIdeas collide, legacies take hold,\nEconomic philosophies, stories bold.\nKeynes and Hayek, stars of the fray,\nIn this rap battle, let’s let them sway!\n\n\nVerse 1:\nLadies and gents, gather ’round, let’s ignite the fire,\nKeynes and Hayek clashing, watch their thoughts transpire.\nIn this epic rap, theories locked in strife,\nRhymes slicing through economics, this verbal knife.\nVerse 2:\nKeynes steps to the mic, demand’s his claim,\nGov intervention’s his ticket to fame.\nBut Hayek ain’t fooled, free markets in his stance,\nSpontaneous order, let the markets dance.\nVerse 3:\nKeynes rides the wave of crisis, with demand in sight,\nPump up the spending, end the economic plight.\nYet Hayek drops knowledge, bust clears the way,\nLiquidate the bad, let markets have their say.\nVerse 4:\nMultiplier meets the twist, theories collide,\nEconomic philosophies on this lyrical ride.\nKeynes sees cycles, a part of the game,\nHayek trusts the unseen, it’s not all the same.\nVerse 5:\nBoom and bust, their tales unfold,\nKeynes’ spending and Hayek’s markets, bold.\nKeynes says cure, more demand to wield,\nHayek’s patient, lets the market’s hand be revealed.\nVerse 6:\nThe clash of ideas, legacies untold,\nKeynes and Hayek’s visions, stories unfold.\nIn this rap battle, they’re the stars of the lore,\nMimicking history, Keynes versus Hayek, encore!\n\n\nVerse 1:\nLadies and gentlemen, assemble, it’s a lyrical throwdown,\nKeynes versus Hayek, in this rhyming showdown.\nEconomic minds clash, theories take the stage,\nStrap in tight, witness wisdom engage.\nVerse 2:\nKeynes steps up bold, demand’s his creed,\nGovernment intervention, that’s the seed he’ll feed.\nBut Hayek ain’t convinced, the market’s his creed,\nSpontaneous order, watch it take the lead.\nVerse 3:\nKeynes rides the wave of crisis, demand’s the key,\nSpending’s his salvation, he insists you’ll see.\nHayek’s counterpunch, bust cures the boom,\nClear the slate, let the market resume.\nVerse 4:\nMultiplier meets the twist, their theories unfurl,\nEconomic philosophies, a clash of the world.\nKeynes, cycles turn, part of nature’s law,\nHayek’s trust in markets, unseen hands in awe.\nVerse 5:\nBoom and bust tales, unfold on this stage,\nKeynes says spend, economies engage.\nHayek’s calm, the market’s plan unfurls,\nLet it heal, no central command swirls.\nVerse 6:\nDebating ideas, their legacy displayed,\nKeynes and Hayek’s visions, history’s cascade.\nIn this rap battle’s heat, their spirits ignite,\nMimicking history, it’s Keynes and Hayek’s fight!"
  },
  {
    "objectID": "chatgpt/quar_01_25_2024/index.html",
    "href": "chatgpt/quar_01_25_2024/index.html",
    "title": "Quarto 1.4",
    "section": "",
    "text": "Quarto 1.4 is out! Over the last few weeks, they have been pushing quite few updates, and finally they have released version 1.4.\nWhat is quite interesting to me is that this version includes a new feature that I have been waiting for a while: Multi source cross-referencing!\nI have been using Quarto for some time now, and one of the things I noticed is that it was very easy to add titles and cross-references to tables as long as they were formated as markdown tables.\nHowever, as you may now, markdown tables are not very flexible. Better options include HTML tables, and even better, LaTeX tables. Unfortunately, there was no easy way to reference those kinds of tables using the @ref syntax. Looking at other people’s questions and solutions, they all seemed a bit complicated to me. (specially those that involved using LaTeX commands).\nBut now, with version 1.4, it seems very easy to reference tables that are not markdown tables!\nLet’s see how it works!"
  },
  {
    "objectID": "chatgpt/quar_01_25_2024/index.html#setup",
    "href": "chatgpt/quar_01_25_2024/index.html#setup",
    "title": "Quarto 1.4",
    "section": "Setup",
    "text": "Setup\nTo do this, I will produce a simple regression using Stata, and export it as markdown, HTML, and LaTeX tables using esttab1.\n*| code-fold: false\n*| echo: true\nfrause oaxaca, clear\nqui:reg  lnwage educ exper tenure female\nest sto m1\nqui:reg  lnwage educ exper tenure female age agesq married divorced\nest sto m2\n\nesttab m1 m2 using \"reg_md\", wide se replace md\nesttab m1 m2 using \"reg_html\", wide se replace htm\nesttab m1 m2 using \"reg_tex\", wide se replace tex\nThe output of the code is not usable right away, as it has some minor issues. For example, the markdown table has an extra line that needs to be removed."
  },
  {
    "objectID": "chatgpt/quar_01_25_2024/index.html#markdown-table",
    "href": "chatgpt/quar_01_25_2024/index.html#markdown-table",
    "title": "Quarto 1.4",
    "section": "Markdown table",
    "text": "Markdown table\nFor the next I will make it simple, using what I think is the easiest cross reference the tables:\n::: {#tbl-table1}\n{{&lt; include reg_md.md &gt;}}\nTable made with markdown\n:::\nGeneral syntax:\n\nline 1 indicates we will be creating table that could be referenced using @tbl-table1 code.\nline 2 request to “include” the markdown table file.\nline 3 is the title of this table\n\nThe result is the following table:\n\n\n\nTable 1: Table made with markdown\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n(2)\n\n\n\n\n\neduc\n0.0848***\n(0.00519)\n0.0644***\n(0.00508)\n\n\nexper\n0.0111***\n(0.00154)\n-0.000311\n(0.00181)\n\n\ntenure\n0.00771***\n(0.00188)\n0.00672***\n(0.00190)\n\n\nfemale\n-0.0841***\n(0.0251)\n-0.160***\n(0.0243)\n\n\nage\n\n\n0.113***\n(0.00814)\n\n\nagesq\n\n\n-0.00126***\n(0.0000976)\n\n\nmarried\n\n\n-0.0224\n(0.0294)\n\n\ndivorced\n\n\n0.0719\n(0.0412)\n\n\n_cons\n2.213***\n(0.0683)\n0.311*\n(0.149)\n\n\nN\n1434\n\n1434\n\n\n\n\nStandard errors in parentheses * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\nThis one was already feasible before however."
  },
  {
    "objectID": "chatgpt/quar_01_25_2024/index.html#html-table",
    "href": "chatgpt/quar_01_25_2024/index.html#html-table",
    "title": "Quarto 1.4",
    "section": "HTML table",
    "text": "HTML table\nNow, let’s try with an HTML table. The syntax is very similar:\n    ::: {#tbl-table2}\n    ````{=html}\n    {{&lt; include reg_html.html &gt;}}\n    ````\n    Table made with markdown\n    :::\nwith the following result:\n\n\n\nTable 2: Table made with HTML\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n(2)\n\n\n\n\nlnwage\n\nlnwage\n\n\n\n\n\neduc\n0.0848***\n(0.00519)\n0.0644***\n(0.00508)\n\n\nexper\n0.0111***\n(0.00154)\n-0.000311\n(0.00181)\n\n\ntenure\n0.00771***\n(0.00188)\n0.00672***\n(0.00190)\n\n\nfemale\n-0.0841***\n(0.0251)\n-0.160***\n(0.0243)\n\n\nage\n\n\n0.113***\n(0.00814)\n\n\nagesq\n\n\n-0.00126***\n(0.0000976)\n\n\nmarried\n\n\n-0.0224\n(0.0294)\n\n\ndivorced\n\n\n0.0719\n(0.0412)\n\n\n_cons\n2.213***\n(0.0683)\n0.311*\n(0.149)\n\n\n\n\nN\n1434\n\n1434\n\n\n\nStandard errors in parentheses\n* p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\nNow, there is a small difference in the produced table because how esttab produces the HTML table. Neverthless, the table creation and naming works!\nAlso, notice that I’m adding a code chunk =html, so the information can be read correctly."
  },
  {
    "objectID": "chatgpt/quar_01_25_2024/index.html#latex-table",
    "href": "chatgpt/quar_01_25_2024/index.html#latex-table",
    "title": "Quarto 1.4",
    "section": "LaTeX table",
    "text": "LaTeX table\nFinally, let’s try with a LaTeX table. The syntax is very similar:\n::: {#tbl-table3}\n{{&lt; include reg_tex.tex &gt;}}\nTable made with Latex\n:::\nwith the following result:\n\n\n\nTable 3: Table made with Latex\n\n\n\n\n\n\nAhh! seems this is not working as HTML! The reason is that Latex tables do not render in html. However if you look into the pdf version, you will see that the table is there, and it is correctly named!"
  },
  {
    "objectID": "chatgpt/quar_01_25_2024/index.html#cross-referencing",
    "href": "chatgpt/quar_01_25_2024/index.html#cross-referencing",
    "title": "Quarto 1.4",
    "section": "Cross-referencing",
    "text": "Cross-referencing\nAs you can see Table 1 was done with md, Table 2 with html and Table 3 with LaTeX."
  },
  {
    "objectID": "chatgpt/quar_01_25_2024/index.html#footnotes",
    "href": "chatgpt/quar_01_25_2024/index.html#footnotes",
    "title": "Quarto 1.4",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nesttab is part of estout package by Ben Jann.↩︎"
  },
  {
    "objectID": "chatgpt/ren_06_02_2023/index.html",
    "href": "chatgpt/ren_06_02_2023/index.html",
    "title": "Renaming variables in Bulk",
    "section": "",
    "text": "This tip was brought to you by Fahad Mirza. Its one of those little things I have found useful, but usually forget, and have to look for it all over again.\nLuckily, I know have my own site, where I can save and store this information! I give, however, total creadit to Fahad."
  },
  {
    "objectID": "chatgpt/ren_06_02_2023/index.html#aknowledgements",
    "href": "chatgpt/ren_06_02_2023/index.html#aknowledgements",
    "title": "Renaming variables in Bulk",
    "section": "",
    "text": "This tip was brought to you by Fahad Mirza. Its one of those little things I have found useful, but usually forget, and have to look for it all over again.\nLuckily, I know have my own site, where I can save and store this information! I give, however, total creadit to Fahad."
  },
  {
    "objectID": "chatgpt/ren_06_02_2023/index.html#the-problem",
    "href": "chatgpt/ren_06_02_2023/index.html#the-problem",
    "title": "Renaming variables in Bulk",
    "section": "The problem",
    "text": "The problem\nThe problem is simple. Some times, you may have a series of variables with somewhat unappealing names. I particuarly dislike names that are too long. While some people like having descriptive variable names, I find it particularly distracting.\nMy preference is to have variables with good labels, and/or good value labels, whenever necessary. For variables themselves, I like short names, that are descriptive, however, I also like to have them labeled sequentially!.\nHow do we do that?\nObviously, the first approach is to go one by one. In fact, not too many Stata's ago, that was the only option. That particular task could have been done using loops as follows:\n\nsysuse auto, clear\n* This loop iterates over all variable names in the dataset\nforeach i of varlist * {\n    local j = `j'+1\n    ren `i' var_`j'\n}\ndescribe *\n\n\n\n\n(1978 automobile data)\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nvar_1           str18   %-18s                 Make and model\nvar_2           int     %8.0gc                Price\nvar_3           int     %8.0g                 Mileage (mpg)\nvar_4           int     %8.0g                 Repair record 1978\nvar_5           float   %6.1f                 Headroom (in.)\nvar_6           int     %8.0g                 Trunk space (cu. ft.)\nvar_7           int     %8.0gc                Weight (lbs.)\nvar_8           int     %8.0g                 Length (in.)\nvar_9           int     %8.0g                 Turn circle (ft.)\nvar_10          int     %8.0g                 Displacement (cu. in.)\nvar_11          float   %6.2f                 Gear ratio\nvar_12          byte    %8.0g      origin     Car origin"
  },
  {
    "objectID": "chatgpt/ren_06_02_2023/index.html#there-is-a-better-way",
    "href": "chatgpt/ren_06_02_2023/index.html#there-is-a-better-way",
    "title": "Renaming variables in Bulk",
    "section": "There is a better way",
    "text": "There is a better way\nWhile the process above is rather simple, there is a better way of doing this, as Fahad suggests. That is using some of the extended options of rename.\nLets first replicate the code above, using the code that Fahad suggested.\n\nsysuse auto, clear\nren (*) (var_#), addnumber\ndescribe\n\n(1978 automobile data)\n\nContains data from C:\\Program Files\\Stata17/ado\\base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2020 17:45\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nvar_1           str18   %-18s                 Make and model\nvar_2           int     %8.0gc                Price\nvar_3           int     %8.0g                 Mileage (mpg)\nvar_4           int     %8.0g                 Repair record 1978\nvar_5           float   %6.1f                 Headroom (in.)\nvar_6           int     %8.0g                 Trunk space (cu. ft.)\nvar_7           int     %8.0gc                Weight (lbs.)\nvar_8           int     %8.0g                 Length (in.)\nvar_9           int     %8.0g                 Turn circle (ft.)\nvar_10          int     %8.0g                 Displacement (cu. in.)\nvar_11          float   %6.2f                 Gear ratio\nvar_12          byte    %8.0g      origin     Car origin\n-------------------------------------------------------------------------------\nSorted by: var_12\n     Note: Dataset has changed since last saved.\n\n\nThis is a much shorter, and cleaner code. What it does is take all variables within the first parenthesis to be rename using the instructions of the second set. Of course rename has quite few other options that you may find useful. Just type help rename group, to see all other extended options.\nBefore ending this tip. Something else you may find useful. You can use the option dryrun. Doing this none of the variable names will change, but instead you will see a report of how variable names will change after the command is executed.\n\nsysuse auto, clear\nren (*) (var_#), addnumber dryrun\n\n(1978 automobile data)\n\n       oldname | newname\n  -------------+-------------\n          make | var_1\n         price | var_2\n           mpg | var_3\n         rep78 | var_4\n      headroom | var_5\n         trunk | var_6\n        weight | var_7\n        length | var_8\n          turn | var_9\n  displacement | var_10\n    gear_ratio | var_11\n       foreign | var_12\n  ---------------------------"
  },
  {
    "objectID": "chatgpt/R_vs_Stata_3_18_2023/index.html",
    "href": "chatgpt/R_vs_Stata_3_18_2023/index.html",
    "title": "Stata vs R: The battle",
    "section": "",
    "text": "Note\n\n\n\nChatGPT was requested to write a fantasy style story of a battle between Stata and R.\n\n\nR, the Ruler of the Realm of Statistics, is a powerful wizard dressed in a cloak made of code. He wields a staff imbued with the power of data and uses it to cast spells that manipulate numbers and produce stunning visualizations. His knowledge of the arcane arts of statistics is unparalleled, and his ability to uncover hidden patterns and correlations is feared by all who oppose him.\nStata, the Lord of the Land of Data Analysis, is a mighty warrior clad in armor made of spreadsheets. He carries a sword made of the finest data sets and uses it to slice through any obstacle that stands in his way. His expertise in data manipulation and analysis is unmatched, and his precision in executing statistical tests is legendary.\nOne day, R and Stata found themselves at odds over a particular dataset, both claiming that their analysis was the superior one. R, confident in his ability to uncover hidden patterns, challenged Stata to a battle of wits. Stata, equally confident in his mastery of data analysis, accepted the challenge.\nThe two warriors faced off in a clearing, each determined to emerge victorious. R began by casting a complex spell that produced a stunning visualization of the data. The colors and shapes danced in the air, displaying a mesmerizing pattern that seemed to confirm R’s analysis.\nStata responded by executing a series of precise statistical tests that revealed hidden nuances in the data. His armor clanged as he brandished his sword, each stroke slicing through R’s visualization and exposing its flaws.\nR retaliated with a barrage of code, launching scripts that produced a dizzying array of charts and graphs. Stata responded with a flurry of spreadsheets, each filled with calculations and formulas that exposed the weaknesses in R’s analysis.\nThe battle raged on for hours, with both warriors refusing to back down. In the end, it was a draw, with neither R nor Stata able to claim victory over the other. As they parted ways, each warrior knew that they had met a worthy opponent and that their skills would be tested again in the future.\nDespite the draw, R and Stata gained newfound respect for each other’s skills and expertise. They understood that while they each had different strengths and approaches to data analysis, they were both essential in the quest for knowledge and understanding.\nOver time, R and Stata began to work together more closely, combining their unique abilities to uncover insights that neither could have found alone. R’s mastery of statistics and ability to produce visually stunning representations of data complemented Stata’s precision in data analysis and ability to execute complex statistical tests.\nTogether, R and Stata became an unstoppable force, feared by their enemies and respected by their peers. They worked tirelessly to unlock the mysteries of the data universe, revealing hidden patterns and correlations that had previously eluded even the most skilled data analysts.\nAs they continued their journey, R and Stata encountered new challenges and obstacles, each one more daunting than the last. But they faced each challenge with courage and determination, knowing that their combined skills would enable them to conquer anything that came their way.\nIn the end, R and Stata became legends in the world of data analysis, their names synonymous with excellence, precision, and innovation. Their legacy lived on long after they were gone, inspiring future generations of data analysts to follow in their footsteps and push the boundaries of what was possible."
  },
  {
    "objectID": "chatgpt/stata_08_04_2024/index.html",
    "href": "chatgpt/stata_08_04_2024/index.html",
    "title": "Stata Crash Course: Publication Ready Tables",
    "section": "",
    "text": "Continuing this quick series of guides with Stata. Something we all need. Tables! Tables are a fundamental part of presenting research results, and there is a community-contributed command that makes it easy to create publication-ready tables in Stata: estout/esttab/estpost. These command come to you thanks to Ben Jann. They allow you to generate tables from estimation results, matrices, in a format suitable for publication, including HTML, LaTeX and Markdown.\nOf course, for this you need a quick setup:"
  },
  {
    "objectID": "chatgpt/stata_08_04_2024/index.html#advanced-summary-statistics",
    "href": "chatgpt/stata_08_04_2024/index.html#advanced-summary-statistics",
    "title": "Stata Crash Course: Publication Ready Tables",
    "section": "Advanced Summary Statistics",
    "text": "Advanced Summary Statistics\nYou can also generate more advanced summary statistics tables, including grouped statistics and custom formatting. Here’s an example:\n\n// Load sample data\nwebuse nlsw88, clear\n\n// Calculate summary statistics by occupation category\nestpost tabstat wage age tenure, by(race) statistics(mean sd min max n) columns(statistics)\nest sto advanced_summary\n// Create advanced summary statistics table\nesttab advanced_summary using advanced_summary.txt, ///\n    cells(\"mean(fmt(2)) sd(fmt(2)) min(fmt(1)) max(fmt(1)) count(fmt(0))\") ///\n    noobs nonumber nomtitle     ///\n    collabels(\"Mean\" \"Std. Dev.\" \"Min\" \"Max\" \"N\") ///\n    eqlabels(\"White\" \"Black\" \"Other\") ///\n    varlabels(wage \"&nbsp;&nbsp;Hourly Wage\" age \"&nbsp;&nbsp;Age\" tenure \"&nbsp;&nbsp;Job Tenure\") ///\n    alignment(r) width(20) ///\n    replace noline md\n\n\n\n\nTable 2: Advanced Summary Statistics by Race\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean\nStd. Dev.\nMin\nMax\nN\n\n\n\n\nWhite\n\n\n\n\n\n\n\n  Hourly Wage\n8.08\n5.96\n1.0\n40.2\n1637\n\n\n  Age\n39.27\n3.08\n34.0\n46.0\n1637\n\n\n  Job Tenure\n5.81\n5.46\n0.0\n25.9\n1627\n\n\nBlack\n\n\n\n\n\n\n\n  Hourly Wage\n6.84\n5.08\n1.2\n40.7\n583\n\n\n  Age\n38.81\n2.98\n34.0\n45.0\n583\n\n\n  Job Tenure\n6.50\n5.62\n0.0\n24.8\n578\n\n\nOther\n\n\n\n\n\n\n\n  Hourly Wage\n8.55\n5.21\n1.8\n25.8\n26\n\n\n  Age\n39.31\n3.25\n34.0\n44.0\n26\n\n\n  Job Tenure\n4.95\n5.24\n0.2\n21.2\n26\n\n\nTotal\n\n\n\n\n\n\n\n  Hourly Wage\n7.77\n5.76\n1.0\n40.7\n2246\n\n\n  Age\n39.15\n3.06\n34.0\n46.0\n2246\n\n\n  Job Tenure\n5.98\n5.51\n0.0\n25.9\n2231\n\n\n\nData source: NLSW 1988\n\n\n\nIn this example, notice that I use varlabels to provide custom labels for the variables. Because I want to use markdown for the output, I use &nbsp; to add spaces, and add some hierarchy on the tables. I also use eqlabels to provide custom labels for the equation names (in this case, race categories). The alignment(r) option right-aligns all cells, althought it does not have an effect on md. The noline option removes the horizontal lines between rows."
  },
  {
    "objectID": "chatgpt/stata_08_04_2024/index.html#regression-table",
    "href": "chatgpt/stata_08_04_2024/index.html#regression-table",
    "title": "Stata Crash Course: Publication Ready Tables",
    "section": "Regression Table",
    "text": "Regression Table\nNext is the most common excercise we would do in Stata: regression analysis. Here’s how you can create a table with regression results:\n\n// Load sample data\nsysuse auto, clear\nregress price weight mpg\nestimates store model1\nregress price weight mpg foreign\nestimates store model2\n\n// Create regression table\nesttab model1 model2 using regression_results.txt, ///\n    b(3) se(3) star(* 0.10 ** 0.05 *** 0.01) ///\n    r2 ar2 nonumber replace note(\"\") noline md ///\n    mtitle(\"Model 1\" \"Model 2\")\n\n\n\n\nTable 3: Regression Results\n\n\n\n\n\n\nModel 1\nModel 2\n\n\n\n\nweight\n1.747***\n3.465***\n\n\n\n(0.641)\n(0.631)\n\n\nmpg\n-49.512\n21.854\n\n\n\n(86.156)\n(74.221)\n\n\nforeign\n\n3673.060***\n\n\n\n\n(683.978)\n\n\n_cons\n1946.069\n-5853.696*\n\n\n\n(3597.050)\n(3376.987)\n\n\nN\n74\n74\n\n\nR2\n0.293\n0.500\n\n\nadj. R2\n0.273\n0.478\n\n\n\n * p &lt; 0.10, ** p &lt; 0.05, *** p &lt; 0.01\nData source: Auto, Standard errors in parentheses, * p&lt;0.10, ** p&lt;0.05, *** p&lt;0.01\n\n\n\nThis code is more straight forward. After the regressions are estimated, and stored with est sto, the esttab command is used to create the table. The b(3) and se(3) options specify that the coefficients and standard errors should be displayed with 3 decimal places. The star() option specifies the significance levels for the stars. The r2 ar2 options include the R-squared and adjusted R-squared statistics. The nonumber option removes the row numbers. This is necessary for markdown tables, because markdown only allows for a single title row."
  },
  {
    "objectID": "chatgpt/stata_08_04_2024/index.html#correlation-matrix",
    "href": "chatgpt/stata_08_04_2024/index.html#correlation-matrix",
    "title": "Stata Crash Course: Publication Ready Tables",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\nCorrelation matrices are also common in data analysis. They are easy enough to create with correlate. Making them into a table can also be easy:\n\n// Generate correlation matrix\nestpost correlate price mpg weight length, matrix\nest sto corr_matrix\n// Create correlation table\nesttab corr_matrix using correlation_matrix.txt, ///\n    cell(\"rho(fmt(3))\") replace nonumber collabels(none) ///\n    nomtitle noline unstack label md\n\n\n\n\nTable 4: Correlation matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrice\nMileage (mpg)\nWeight (lbs.)\nLength (in.)\n\n\n\n\nPrice\n1.000\n\n\n\n\n\nMileage (mpg)\n-0.469\n1.000\n\n\n\n\nWeight (lbs.)\n0.539\n-0.807\n1.000\n\n\n\nLength (in.)\n0.432\n-0.796\n0.946\n1.000\n\n\nObservations\n74\n\n\n\n\n\n\nData source: Auto\n\n\n\nThis creates a correlation matrix with formatted coefficients. In contrast with correlate, if using estatpost, its necessary to also use the option matrix.\nIn case of esttab, it is necessary to use unstuck. But other than that, obtaining the tables is quite straightforward."
  },
  {
    "objectID": "chatgpt/stata_08_04_2024/index.html#regressions-with-fixed-effects",
    "href": "chatgpt/stata_08_04_2024/index.html#regressions-with-fixed-effects",
    "title": "Stata Crash Course: Publication Ready Tables",
    "section": "Regressions with Fixed Effects",
    "text": "Regressions with Fixed Effects\nNow something that is very common in regression analysis: fixed effects models. A usual question is how to create tables that signal the inclusion of fixed effects. Here’s an example:\n\n// Load panel data and set as panel\nwebuse nlswork, clear\nxtset idcode year\n// ssc install reghdfe\n\n// Run regressions\nreghdfe ln_wage tenure c.age##c.age \nest sto no_fe\n\nreghdfe ln_wage tenure c.age##c.age , absorb(idcode)\nest sto id_fe\nestadd local id_fe \"X\"\n\nreghdfe ln_wage tenure c.age##c.age , absorb(idcode year)\nest sto idyr_fe\nestadd local id_fe \"X\"\nestadd local yr_fe \"X\"\n\n// Create table\n \nesttab no_fe  id_fe idyr_fe using mreg.txt, ///\nscalar(\"id_fe Individual FE\" \"yr_fe Yr FE\") ///\nnoline  md  nonotes nomtitle  replace\n\n\n\n\nTable 5: Regression Results with FE\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n(2)\n(3)\n\n\n\n\ntenure\n0.0391***\n0.0217***\n0.0214***\n\n\n\n(50.48)\n(27.21)\n(26.88)\n\n\nage\n0.0752***\n0.0523***\n0.0750***\n\n\n\n(21.65)\n(18.78)\n(7.03)\n\n\nc.age#c.age\n-0.00109***\n-0.000672***\n-0.00107***\n\n\n\n(-18.86)\n(-14.56)\n(-17.74)\n\n\n_cons\n0.334***\n0.688***\n0.384\n\n\n\n(6.62)\n(16.95)\n(1.29)\n\n\nN\n28101\n27549\n27549\n\n\nIndividual FE\n\nX\nX\n\n\nYr FE\n\n\nX\n\n\n\nData source: NLSW 1988, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\nThis example shows how to use estadd to include additional information to a regression. This information can be later used in the table. In this case, the fixed effects are included in the table. The scalar option is used to include the extra information."
  },
  {
    "objectID": "chatgpt/stata_08_04_2024/index.html#conclusion",
    "href": "chatgpt/stata_08_04_2024/index.html#conclusion",
    "title": "Stata Crash Course: Publication Ready Tables",
    "section": "Conclusion",
    "text": "Conclusion\nI think these are the most basic tables you can create with esttab. There are many more options available, and you can customize the tables to your needs. Using Quarto, something I am doing more often, is to create the tables in Stata, modify the latex code, and then include them in the final document. This is a great way to create reproducible research."
  },
  {
    "objectID": "chatgpt/suregfe_07_14_2023/index.html",
    "href": "chatgpt/suregfe_07_14_2023/index.html",
    "title": "How to suest a HDFE",
    "section": "",
    "text": "A question I have seen online many…many…many times is how to compare the coefficients of a model that has been estimated using a highdimensional set of fixed effects.\nThe starting answer has always been…to suest the both equations, or stack both equations to compare the effects. However, suest will not work with reghdfe nor xtreg. And stacking equations is even less intuitive.\nToday, however, I will present you an easy way to do this with with a little command of my own creation, but also using some simple syntax.\n\n\n\n\n\n\nSetup\n\n\n\nTo use the strategies I will present here you will need reghdfe (from ssc) and cre (from fra, my own repository). You will need frause from ssc."
  },
  {
    "objectID": "chatgpt/suregfe_07_14_2023/index.html#introduction",
    "href": "chatgpt/suregfe_07_14_2023/index.html#introduction",
    "title": "How to suest a HDFE",
    "section": "",
    "text": "A question I have seen online many…many…many times is how to compare the coefficients of a model that has been estimated using a highdimensional set of fixed effects.\nThe starting answer has always been…to suest the both equations, or stack both equations to compare the effects. However, suest will not work with reghdfe nor xtreg. And stacking equations is even less intuitive.\nToday, however, I will present you an easy way to do this with with a little command of my own creation, but also using some simple syntax.\n\n\n\n\n\n\nSetup\n\n\n\nTo use the strategies I will present here you will need reghdfe (from ssc) and cre (from fra, my own repository). You will need frause from ssc."
  },
  {
    "objectID": "chatgpt/suregfe_07_14_2023/index.html#suest-the-problem",
    "href": "chatgpt/suregfe_07_14_2023/index.html#suest-the-problem",
    "title": "How to suest a HDFE",
    "section": "suest the problem",
    "text": "suest the problem\nLets start with a simple wage regression model, where we aim to compare the coefficients of men and women. For this, we will use the data set oaxaca, and a simple Mincerian regression model:\nFirst, lets estimate both models:\n\nqui:frause oaxaca, clear\nqui:reg lnwage educ exper tenure age if female==0\nest sto male\nqui:reg lnwage educ exper tenure age if female==1\nest sto female\n\nand use suest to put them together, and test if coefficients are different from each other or not. For this I will use lincom and test commands:\n\nqui: suest male female\nlincom [male_mean]:educ-[female_mean]:educ\ntest [male_mean=female_mean]:educ\ntest [male_mean=female_mean], common\n\n\n ( 1)  [male_mean]educ - [female_mean]educ = 0\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |  -.0300371   .0119579    -2.51   0.012    -.0534741   -.0066001\n------------------------------------------------------------------------------\n\n ( 1)  [male_mean]educ - [female_mean]educ = 0\n\n           chi2(  1) =    6.31\n         Prob &gt; chi2 =    0.0120\n\n ( 1)  [male_mean]educ - [female_mean]educ = 0\n ( 2)  [male_mean]exper - [female_mean]exper = 0\n ( 3)  [male_mean]tenure - [female_mean]tenure = 0\n ( 4)  [male_mean]age - [female_mean]age = 0\n\n           chi2(  4) =   26.63\n         Prob &gt; chi2 =    0.0000\n\n\nI could also use more involved methods like creating my own ml or gmm option, but there is no need in this simplified method."
  },
  {
    "objectID": "chatgpt/suregfe_07_14_2023/index.html#stacking",
    "href": "chatgpt/suregfe_07_14_2023/index.html#stacking",
    "title": "How to suest a HDFE",
    "section": "Stacking",
    "text": "Stacking\nThe next option is do Stacking. This sounds difficult, but its nothing different than using the old trick of interactions. we simply need to estimate a model where all covariates are interacted with our sampling indicator (gender):\n\nqui:reg lnwage i.female##c.(educ exper tenure age), robust\nlincom 1.female#c.educ\ntest 1.female#c.educ\ntest 1.female#c.educ 1.female#c.exp 1.female#c.tenure 1.female#c.age\n\n\n ( 1)  1.female#c.educ = 0\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |   .0300371   .0119956     2.50   0.012     .0065062    .0535681\n------------------------------------------------------------------------------\n\n ( 1)  1.female#c.educ = 0\n\n       F(  1,  1424) =    6.27\n            Prob &gt; F =    0.0124\n\n ( 1)  1.female#c.educ = 0\n ( 2)  1.female#c.exper = 0\n ( 3)  1.female#c.tenure = 0\n ( 4)  1.female#c.age = 0\n\n       F(  4,  1424) =    6.62\n            Prob &gt; F =    0.0000\n\n\nAgain we will obtain the same results as before.\nBut now the hard question. What if we have a HDFE?"
  },
  {
    "objectID": "chatgpt/suregfe_07_14_2023/index.html#stacking-fe",
    "href": "chatgpt/suregfe_07_14_2023/index.html#stacking-fe",
    "title": "How to suest a HDFE",
    "section": "Stacking FE",
    "text": "Stacking FE\nTo simulate the sitution of a high-dimensional FE, I will use age. This will allow me to still obtain point estimates using simple regression (and say suest), while comparing it to the alternative:\n\nqui:reg lnwage educ exper tenure i.age if female==0\nest sto male\nqui:reg lnwage educ exper tenure i.age if female==1\nest sto female\nqui:suest male female, cluster(age)\nlincom [male_mean]:educ-[female_mean]:educ\ntest [male_mean=female_mean]:educ exper tenure\n\n\n ( 1)  [male_mean]educ - [female_mean]educ = 0\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |  -.0354668   .0095251    -3.72   0.000    -.0541357   -.0167978\n------------------------------------------------------------------------------\n\n ( 1)  [male_mean]educ - [female_mean]educ = 0\n ( 2)  [male_mean]exper - [female_mean]exper = 0\n ( 3)  [male_mean]tenure - [female_mean]tenure = 0\n\n           chi2(  3) =   30.72\n         Prob &gt; chi2 =    0.0000\n\n\nNow the second method, using reghdfe\n\negen age_fem = group(age  fem)\nqui:reghdfe lnwage i.female##c.(educ exper tenure), abs(female#age) cluster(age_fem)\nlincom 1.female#c.educ\ntest 1.female#c.educ 1.female#c.exp 1.female#c.tenure \n\n\n ( 1)  1.female#c.educ = 0\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |   .0354668   .0104804     3.38   0.001     .0146424    .0562911\n------------------------------------------------------------------------------\n\n ( 1)  1.female#c.educ = 0\n ( 2)  1.female#c.exper = 0\n ( 3)  1.female#c.tenure = 0\n\n       F(  3,    89) =    6.49\n            Prob &gt; F =    0.0005"
  },
  {
    "objectID": "chatgpt/suregfe_07_14_2023/index.html#using-suest-and-correlated-random-effects-model-cre",
    "href": "chatgpt/suregfe_07_14_2023/index.html#using-suest-and-correlated-random-effects-model-cre",
    "title": "How to suest a HDFE",
    "section": "Using suest and correlated random effects model cre",
    "text": "Using suest and correlated random effects model cre\nNow we use Correlated Random Effects model to estimate the FE models:\n\nqui:cre, keep prefix(ml) abs(age):reg lnwage educ exper tenure if female==0\nest sto male\nqui:cre, keep prefix(m2) abs(age):reg lnwage educ exper tenure if female==1\nest sto female\nqui:suest male female, cluster(age)\nlincom [male_mean]:educ-[female_mean]:educ\ntest [male_mean=female_mean]:educ exper tenure\n\n\n ( 1)  [male_mean]educ - [female_mean]educ = 0\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |  -.0354668   .0095251    -3.72   0.000    -.0541357   -.0167978\n------------------------------------------------------------------------------\n\n ( 1)  [male_mean]educ - [female_mean]educ = 0\n ( 2)  [male_mean]exper - [female_mean]exper = 0\n ( 3)  [male_mean]tenure - [female_mean]tenure = 0\n\n           chi2(  3) =   30.72\n         Prob &gt; chi2 =    0.0000\n\n\nWhich gives me exactly the same result!"
  },
  {
    "objectID": "chatgpt/suregfe_07_14_2023/index.html#hard-example",
    "href": "chatgpt/suregfe_07_14_2023/index.html#hard-example",
    "title": "How to suest a HDFE",
    "section": "Hard Example:",
    "text": "Hard Example:\nLets do this with a harder example, using nlswork dataset, comparing of a wage regression coefficients between north and south:\n\nwebuse nlswork, clear\negen cl = group(idcode south)\nqui: reghdfe ln_wage i.south##c.(age msp  not_smsa c_city union tenure hours) , abs(idcode#south) cluster(cl)\ntest 1.south#c.age 1.south#c.msp 1.south#c.union\n\n(National Longitudinal Survey of Young Women, 14-24 years old in 1968)\n(8 missing values generated)\n\n ( 1)  1.south#c.age = 0\n ( 2)  1.south#c.msp = 0\n ( 3)  1.south#c.union = 0\n\n       F(  3,  3586) =    1.24\n            Prob &gt; F =    0.2941\n\n\nBut also using CRE:\n\nwebuse nlswork, clear\nqui:cre, abs(idcode) keep prefix(m1): regress ln_wage age msp not_smsa c_city union tenure hours if south==0  \nest sto north\nqui:cre, abs(idcode) keep prefix(m2): regress ln_wage age msp not_smsa c_city union tenure hours if south==1  \nest sto south\nqui:suest north south\ntest [north_mean=south_mean]: age msp union\n\n(National Longitudinal Survey of Young Women, 14-24 years old in 1968)\n\n ( 1)  [north_mean]age - [south_mean]age = 0\n ( 2)  [north_mean]msp - [south_mean]msp = 0\n ( 3)  [north_mean]union - [south_mean]union = 0\n\n           chi2(  3) =    3.82\n         Prob &gt; chi2 =    0.2821"
  },
  {
    "objectID": "chatgpt/suregfe_07_14_2023/index.html#conclusions",
    "href": "chatgpt/suregfe_07_14_2023/index.html#conclusions",
    "title": "How to suest a HDFE",
    "section": "Conclusions",
    "text": "Conclusions\nThere you have it. Two ways to compare coefficients across two models using interactions or suest.\nBoth provide the same results, if you cluster variables with the absorbed variable.\nHope you find it useful"
  },
  {
    "objectID": "chatgpt/teeth_04_19_2023/index.html",
    "href": "chatgpt/teeth_04_19_2023/index.html",
    "title": "A Poem for Abby’s Teeth",
    "section": "",
    "text": "Note\n\n\n\nChatGPT was requested to write a poem about Abigail’s first tooth. The first one was a horror one, but I like this second one better\n\n\nAbigail’s first teeth, oh so small\nPearly white, they barely show at all\nBut they bring joy to all who see\nThis precious little one, so full of glee\n\n\nAbigail drools and chews with delight\nHer first teeth, such a wonderful sight\nShe smiles and giggles with every sound\nAs she explores the world all around\n \nHer first teeth, a milestone so sweet\nA sign of growth and development complete\nThey mark the start of her journey ahead\nAs Abigail grows, learns and is fed\n\n\nSo let’s celebrate Abigail’s first teeth\nAnd all the joy and wonder they bequeath\nFor they are a precious part of life’s start\nAnd a sign of the love in our hearts."
  },
  {
    "objectID": "chatgpt/weakiv_08_15_2023/index.html",
    "href": "chatgpt/weakiv_08_15_2023/index.html",
    "title": "Weak IV: And the problem of Inference",
    "section": "",
    "text": "If you are “old-school” like me, and you are using instrumental variables, you probably are still relying on the rule thumb of using and \\(F \\geq 10\\) to Judge if an instrument is strong enough to proceed and do statistical inference.\nThis “critical” value was constructed based on the research by Staiger and Stock (1997) and Stock and Yogo (2005), and is still included in introductory econometric textbooks, and thought in econometric courses. This are said to be valid in cases where there is no heteroskedasticity in the model.\nAs described in Wooldridge’s “Introductory Econometrics: A Modern Approach” 7e, if models are heteroskedastic, it makes sense that more strict requirements are needed to indicate one has a strong Instrument. Specifically, he cites work by Olea and Pflueger (2013) who suggest that one might need an \\(F\\geq 20\\) to ensure instruments are sufficiently strong.\nIn Lee et al. (2022), however, the bar to judge an instrument strong has been raised even higher. What they suggest is that one must consider the joint distribution of the first stage “F-stat” statistic and second stage “t-stat”, to make any inference judgements. Specifically, the authors indicate that, in order to use “standard” critical values to judge significance in second stage models (say the 1.96 for and \\(\\alpha=5\\%\\)), one should have an F-statistic of at least 104.7!. Alternatively, if we keep using an \\(F=10\\), the t-stat critical value should be at least 3.43."
  },
  {
    "objectID": "chatgpt/weakiv_08_15_2023/index.html#f-geq-10-rightarrow-strong-instrument-or-is-it",
    "href": "chatgpt/weakiv_08_15_2023/index.html#f-geq-10-rightarrow-strong-instrument-or-is-it",
    "title": "Weak IV: And the problem of Inference",
    "section": "",
    "text": "If you are “old-school” like me, and you are using instrumental variables, you probably are still relying on the rule thumb of using and \\(F \\geq 10\\) to Judge if an instrument is strong enough to proceed and do statistical inference.\nThis “critical” value was constructed based on the research by Staiger and Stock (1997) and Stock and Yogo (2005), and is still included in introductory econometric textbooks, and thought in econometric courses. This are said to be valid in cases where there is no heteroskedasticity in the model.\nAs described in Wooldridge’s “Introductory Econometrics: A Modern Approach” 7e, if models are heteroskedastic, it makes sense that more strict requirements are needed to indicate one has a strong Instrument. Specifically, he cites work by Olea and Pflueger (2013) who suggest that one might need an \\(F\\geq 20\\) to ensure instruments are sufficiently strong.\nIn Lee et al. (2022), however, the bar to judge an instrument strong has been raised even higher. What they suggest is that one must consider the joint distribution of the first stage “F-stat” statistic and second stage “t-stat”, to make any inference judgements. Specifically, the authors indicate that, in order to use “standard” critical values to judge significance in second stage models (say the 1.96 for and \\(\\alpha=5\\%\\)), one should have an F-statistic of at least 104.7!. Alternatively, if we keep using an \\(F=10\\), the t-stat critical value should be at least 3.43."
  },
  {
    "objectID": "chatgpt/weakiv_08_15_2023/index.html#how-bad-is-the-problem",
    "href": "chatgpt/weakiv_08_15_2023/index.html#how-bad-is-the-problem",
    "title": "Weak IV: And the problem of Inference",
    "section": "How bad is the problem?",
    "text": "How bad is the problem?\nWhile I was perplex at first regarding the “new” requirements for a high “F-stat”, I also understood that this problem was important. Because of this, I decided to do a small simulation study to see how bad the problem was in terms of inference, using different levels of Instruments strength. Let me show you how I did this, and what it tells me in regards to previous beliefs about the \\(F\\geq 10\\) rule.\nBottom line, in my design, an \\(E(F)=10\\) is not enough to judge an instrument to be strong enough to guarantee the distribution of the coefficient of interest follows a normal distribution. And, if the distribution of such coefficient is not normal, then standard “t-statistics” may be inappropiate when making statistical inference."
  },
  {
    "objectID": "chatgpt/weakiv_08_15_2023/index.html#setup",
    "href": "chatgpt/weakiv_08_15_2023/index.html#setup",
    "title": "Weak IV: And the problem of Inference",
    "section": "Setup",
    "text": "Setup\nFor this simulation, I will use a very simple structure that considers a single endogenous variable and a single instrument, without any other controls.\n\nFirst, The common Error: To simulate endogeneity, I will assume there is a common unobserved factor \\(e\\) that affects the endogenous variable \\(x\\) and outcome \\(y\\). This unobserved error is assumed to follow a standardized normal distribution.\n\n\\[e\\sim N(0,1)\\]\n\nThe instrument: The instrument will be a completely independent variable that also follows a standardized normal distribution.\n\n\\[z\\sim N(0,1)\\]\n\nThe endogenous variable: This variable will be created by adding the endogenous common error \\(e\\), the instrument \\(z\\) and a random error \\(u_x\\). To control the strength of the instrument, the endogenous variable \\(x\\) is defined as:\n\n\\[x=1 + z + (e+u_x)*\\sqrt{2\\frac{N}{F}}\\]\nwhere \\(N\\) is the number of observations, and \\(F\\) is the Expected Strength of the instrument.\n\nThe dependent variable: It will combine the common error and a random standardize normal distributed error, and assume \\(x\\) has no explanatory power. The common error \\(e\\) and the standardized normal distributed error \\(u_y\\) are scaled so the combined error has a variance of 1.\n\n\\[y = 1 + (e + u_y)*\\sqrt{2}\\]\nThe simulation is run 100’000 times, with a sample of 500 observations each, using various levels of “F-statistics”: 5, 10, 20, 40, 80 and 160. To speed up the simulation excercise, I use parallel, a community-contributed command written Vega Yon and Quistorff (2019). You can find the latest version of it on their github.\nThe code for the simulation program is as follows:\nprogram simx, eclass\n    clear\n    local N `1'\n    local F `2'\n    local sig = sqrt(`N'/ `F')\n    set obs `N'\n    gen e = rnormal()\n    gen z = rnormal() \n    gen x = 1 + z + (e+rnormal())*sqrt(.5)*`sig'\n    gen y = 1 + ( e + rnormal())*sqrt(.5)\n    reg x z, \n    matrix b=(_b[z]/_se[z])^2\n    ivregress 2sls  y (x=z), \n    matrix b=b,_b[x],_b[x]/_se[x]\n    ereturn post b\nend\nand the simulation itself can be run using:\nnet install parallel, from(https://raw.github.com/gvegayon/parallel/stable/) replace\nmata mata mlib index\n** Initializes number of \"Stata\" instances\nparallel initialize 17\n** Some tempfiles to store the simulations\ntempfile f0 f1 f2 f3 f4 f5\n** A loop for the simulations\nforvalues i = 0/5 {\n    local j = 5*2^`i'\n    parallel sim, reps(100000): simx 500 `j'\n    gen F=`j'\n    save `f`i''\n}\n \n** Append and rename\nclear \nappend using `f0'\nappend using `f1'\nappend using `f2'\nappend using `f3'\nappend using `f4'\nappend using `f5'\n\nren (_b_c1 _b_c2 _b_c3 F) (f_stat b_coef t_stat FF)\nIf you are interested in the data, it can be downloaded from here. It is a Stata file with 4 variables and 60k observations."
  },
  {
    "objectID": "chatgpt/weakiv_08_15_2023/index.html#results",
    "href": "chatgpt/weakiv_08_15_2023/index.html#results",
    "title": "Weak IV: And the problem of Inference",
    "section": "Results",
    "text": "Results\nFirst, to make sure data was created correctly, we can do a quick summary statistics of the F-statistic of the first stage regression.\n\n\nCode\nuse weakiv, clear\ntabstat f_stat , by(FF) stats(mean median)\n\n\n\n\n\n\nSummary for variables: f_stat\nGroup variable: FF \n\n      FF |      Mean       p50\n---------+--------------------\n       5 |  6.012032  4.989151\n      10 |  11.03701  10.01228\n      20 |   21.0688  19.97883\n      40 |  41.13833  39.95357\n      80 |  81.26734  79.84747\n     160 |  161.4196  159.6645\n---------+--------------------\n   Total |  53.65718  29.36334\n------------------------------\n\n\nThe mean is a bit higher than the intended F-Statistic, whereas the median is closer. Both statistics suggesting simulations were created as intended.\n\n\nCode\nset scheme white2\ncolor_style tableau\njoy_plot f_stat, by(FF) gap0 color(%50) alegend xtitle(\"F-stat\") xline(10 20 100)\n\n\n\n\n\nF-Statistics Distribution\n\n\n\n\nWe can also see the distribution of the F-statistics across all simulations.\nWhat about the endogenous variable coefficient?\nWhile we could examine the coefficient themselves, I find it more useful to examine the distribution of the t-statistics in the second Stage.\n\n\nCode\ntwo kdensity t_stat || function y = normalden(x) , range(-3 3) , by(FF, note(\"\")) legend(order(1 \"T-stat distribution\" 2 \"Normal\") cols(2)) xline(-1.96 1.96)\ngraph export fig1.png, replace width(1000) \n\ntwo kdensity t_stat if f_stat&gt;10 || function y = normalden(x) , range(-3 3) , by(FF, note(\"\")) legend(order(1 \"T-stat distribution\" 2 \"Normal\") cols(2)) xline(-1.96 1.96)\ngraph export fig2.png, replace width(1000) \n\n\n\n\n\n\n\n\nFigure 1: T-Stat distribution by E(F)\n\n\n\nWhat we see from Figure 1 is that the distribution of the t-statistic is not consistent with a normal distribution. For and \\(F\\leq 40\\), there is a clear discrepancy between the normal distribution (orange) and the empirical t-stat distribution (blue). As consequence, when coefficients are possitive, it is more likely to reject the null hypothesis, but also under-reject when the coefficients are negative.\nOnce we reach an F-statistic of at least 80 and 160, the difference between the normal distribution and the t-statistic empirical distribution dissapears. Lesson, if your F-stat is “low”, perhaps using standard distributions may be innapropriate.\nOne problem with the results in Figure 1 was that when \\(E(F)\\) is low, there are many instances where the F-distribution is less than 10. One may say that is the reason why the distribution is skew in Figure 1. A simple way to avoid this is by ploting the same information, but restricting F-statistics to be larger than 10.\n\n\n\n\n\n\nFigure 2: T-Stat distribution by E(F) if F&gt;10\n\n\n\nUnfortunately, the situation is much worse. By constraining the F-statistic to be larger than 10, we are doing the equivalent of p-hacking (F hacking?), which further distorts the distribution of the endogenous variable. While it looks a bit more normal (distributed), we can see now that it over rejects the null hypothesis, if the true F-statistic was, in fact, lower. You can call this evidence of biased estimations."
  },
  {
    "objectID": "chatgpt/weakiv_08_15_2023/index.html#conclusions",
    "href": "chatgpt/weakiv_08_15_2023/index.html#conclusions",
    "title": "Weak IV: And the problem of Inference",
    "section": "Conclusions",
    "text": "Conclusions\nThis note was prepared with the intention of showing in class the consequences of weak IV’s. While I knew that this was an important problem, it was not completely clear what would the consequences in terms of distribution of the coefficient of interest would be.\nAlthough this simulation is rather limited, it clearly shows problems of using weak instrumental variables (even when using the well known “rule of 10”). When the instrument is weak, even if its completely exogenous, the distribution of the endogenous variable coefficient will not be normal. Thus, further steps may be needed for constructing appropriate Confidence intervals.\nIs everything lost? Perhaps not. In Stata, there is the community-contributed command weakiv (ssc install weakiv). This command construct alternative CI that take into account the presence of weak instrumental variables reporting adjusted CI. They seem to be closer to what I expect, based on this simulations."
  },
  {
    "objectID": "chatgpt/weakiv_08_15_2023/index.html#extra",
    "href": "chatgpt/weakiv_08_15_2023/index.html#extra",
    "title": "Weak IV: And the problem of Inference",
    "section": "Extra",
    "text": "Extra\nOk, so while you do simulations like this, sometimes, you find very interesting results. Some worth exploring.\nWhat did I see (you may ask)? First, let me concentrate only on the cases with “stronger” IVs. In particular those with an \\(E(F)\\leq 80\\). None of the F-statistics is less than 10 across all simulations in this case.\nSecond, because the F-Statistics have different ranges, I will standardize them across groups. This will make it easier to compare scatter plots.\n\n\nCode\n** Keeping only FF&gt;=80\nqui {\n    keep if FF&gt;=80\n    gen f_stat2=.\n    foreach i in 80 160 {\n        sum f_stat if FF==`i'\n        replace f_stat2 = (f_stat-FF)/r(sd) if FF==`i'\n    }\n}\n\ntwo scatter t_stat  f_stat2 , by(FF , col(3) note(\"\")) ysize(5) xsize(10) ///\n    ytitle(T-stat) xtitle(\"Standardized F-Stat\")\ngraph export fig3.png, replace width(1000)\n\n\n\n\n\n\n\n\nFigure 3: T-Stat vs SF-Stat\n\n\n\nWhat I see in Figure 3 is that higher F-Stats in the first Stage seem to increase the likelihood of the t-value to be positive and significant. Whereas lower F-stats are associated with the opposite (but to a lesser extent possibly due to simulation design).\n\n\nCode\ntwo (kdensity t_stat if inrange(f_stat2,-.65,.7)) ///\n    (kdensity t_stat if f_stat2&lt;-.65) ///\n    (kdensity t_stat if f_stat2&gt;.7) , ///\n    legend(order(1 \"sF-Stat near 0\" 2 \"sF-Stat&lt;25thp\" 3 \"Sf-Stat&gt;75thp\")) ///\n    xtitle(\"t-Statistic Distribution\") ytitle(\"Density\")\ngraph export fig4.png, replace  width(1000) \ntwo (kdensity b_coef if inrange(f_stat2,-.65,.7)) ///\n    (kdensity b_coef if f_stat2&lt;-.65) ///\n    (kdensity b_coef if f_stat2&gt;.7) , ///\n    legend(order(1 \"sF-Stat near 0\" 2 \"sF-Stat&lt;25thp\" 3 \"Sf-Stat&gt;75thp\")) ///\n    xtitle(\"Coefficient Distribution\") ytitle(\"Density\")\ngraph export fig5.png, replace  width(1000)    \n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) t-Stat Distribution\n\n\n\n\n\n\n\n\n\n\n\n(b) Coefficient Distribution\n\n\n\n\n\n\n\nFigure 4: Distribution based on Standardized F-Statistic\n\n\n\nFigure 4 shows the distribution of both the t-statistic and the coefficient of the endogenous variables for three groups. Observations where the F-statistic is between the 25th and 75th percentile of the distribution (Thus close to the E(F)), and observations for the top (above 75th percentile) and bottom (below 25th percentile) of the distribution. This is similar to the previous conjecture. Even if the instrument is “strong”, if the sample F-statistic is too high or low compared to the E(F), inference may be biased.\nThis is just conjecture, however, it may be the case that IV’s are more difficult to trust that I previously expected. If your first stage has a high F-Statistic, it may be worth asking:\n\nis this F-statistic larger than expected?\n\nIf the answer is yes, then you are likely to be over (or under) stating your coefficients.\nIf the F-statistic is just as large as expected. Then we may be able to trust the estimated coefficients."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Fernando Rios-Avila",
    "section": "",
    "text": "64 Oak Streat • Rensselear, NY, 12144 • (404) 924-5176 • friosa@gmail.com"
  },
  {
    "objectID": "cv.html#contact-information",
    "href": "cv.html#contact-information",
    "title": "Fernando Rios-Avila",
    "section": "",
    "text": "64 Oak Streat • Rensselear, NY, 12144 • (404) 924-5176 • friosa@gmail.com"
  },
  {
    "objectID": "cv.html#summary",
    "href": "cv.html#summary",
    "title": "Fernando Rios-Avila",
    "section": "Summary",
    "text": "Summary\nPhD economist with over a decade of research experience at the Levy Economics Institute, specializing in labor economics, applied econometrics, and applied microeconomics. Expertise in poverty and inequality analysis, data-driven policy evaluation, econometric modeling, with experience teaching graduate-level courses. Key contributor to the Levy Institute Measure of Income and Wealth (LIMEW) and the Levy Institute Measure of Time and Income Poverty (LIMTIP)."
  },
  {
    "objectID": "cv.html#research-interests",
    "href": "cv.html#research-interests",
    "title": "Fernando Rios-Avila",
    "section": "Research Interests",
    "text": "Research Interests\n\nApplied Econometrics: Causal inference, decomposition methods, non-parametric methods\nApplied Microeconomics: Policy evaluation, consumer behavior\nLabor Economics: Wage dynamics, union impacts, labor market institutions\nPoverty and Inequality: Measurement techniques, intersectionality"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Fernando Rios-Avila",
    "section": "Education",
    "text": "Education\nPhD in Economics | Georgia State University | 2008-2013 Dissertation: “Essays on Unions, Wages, and Performance: Evidence from Latin America”\nAdvanced Studies (M.Sc) International Economics | Kiel University - Germany | 2007-2008\nLicenciatura en Economia (5 yr program, BS equivalent) | Universidad Católica Boliviana | 2000-2004"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Fernando Rios-Avila",
    "section": "Professional Experience",
    "text": "Professional Experience\nResearch Scholar | Levy Economics Institute of Bard College | August 2013 – Present\n\nWorked on research projects on labor market dynamics, inequality, and time and income poverty\nDeveloped and applied econometric models for the analysis of time use and consumption data.\nMantained and updated the Levy Institute Measure of Income and Wealth (LIMEW) and the Levy Institute Measure of Time and Income Poverty (LIMTIP)\nPrimary instructor for econometric courses at the Levy Institute’s Master of Science in Economic Theory and Policy program.\n\nConsultant | World Bank | 2022 - Present\n\nConducted analysis in labor and development economics topics in Latin America.\nDeveloped and applied econometric models for analysis of income and consumption data.\n\nResearch Assistant | Federal Reserve Bank of Atlanta | August 2010 – June 2013\nGraduate Research Assistant | Georgia State University | August 2008 – December 2013\nConsultant (UNICEF Project) | Economic and Social Policy Analysis Unit (UDAPE) | July 2005 – June 2007\nInternship | Superintendence of Hydrocarbon, Financial and Economic Analysis-Department | January 2004 – July 2004"
  },
  {
    "objectID": "cv.html#teaching-experience",
    "href": "cv.html#teaching-experience",
    "title": "Fernando Rios-Avila",
    "section": "Teaching Experience",
    "text": "Teaching Experience\nLevy Economics Institute\n\nEconometrics of Poverty, Inequality and Social Science Research | 2015- 2021\nResearch Methods I: Econometrics | 2018- Present\nApplied Econometric Methods for Empirical Research & Policy Evaluation | 2023 - Present\nResearch Methods II: Applied Methods in Microeconomics (Shared Course) | 2024- Present\nIntersecting Inequalities (Shared Course): Labor Market Institutions | 2018\nIntersecting Inequalities (Shared Course): Decompositions Methods in Economics | 2017"
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "Fernando Rios-Avila",
    "section": "Publications",
    "text": "Publications\n\nRefereed Publications\n\n\n1. Rios-Avila, F., Özekin, A., & Komuryakan, F. (n.d.). Glass ceiling, sticky floor, or both? Public and private sector differences in türkiye. Metroeconomica, n/a(n/a). https://doi.org/https://doi.org/10.1111/meca.12481\n\n\n2. Canavire-Bacarreza, G. J., Gomez-Aliaga, Guillermo., Britton, chevanne, Rios-Avila, F., Jimenez-Pozo, W., Granados Ibarra, S., & Li, R. (2024). Fiscal incidence on the island: Grenada’s fiscal system and its incidence. Social and Economic Studies.\n\n\n3. Hotchkiss, J. L., Moore, R. E., & Rios-Avila, F. (2024). Impact of the 2017 tax cuts and jobs act on household labor supply and welfare across the income distribution. National Tax Journal, 77(2), 313–348.\n\n\n4. Mundra, K., & Rios-Avila, F. (2024). Education-occupation mismatch and social networks for hispanics in the u.s.: Role of citizenship. Education Economics, 23(2), 185–209.\n\n\n5. Rios-Avila, F., Canavire-Bacarreza, G. J., & Sacco-Capurro, F. (2024). Recovering income distribution in the presence of interval-censored data. Journal of Economic Inequality. https://doi.org/10.1007/s10888-023-09617-2\n\n\n6. Jenkins, S. P., & Rios-Avila, F. (2023). Finite mixture model for linked survey and administrative data: Estimation and post estimation. The Stata Journal, 23(1), 53–85.\n\n\n7. Jenkins, S. P., & Rios-Avila, F. (2023). Reconciling reports: Modeling employment earnings and measurement errors using linked survey and administrative data. Journal of the Royal Statistical Society Series A: Statistics in Society, 186(1), 110–136. https://doi.org/10.1093/jrsssa/qnac003\n\n\n8. Rios-Avila, F., & Maroto, M. L. (2022). Moving beyond linear regression: Implementing and interpreting quantile regression models with fixed effects. Sociological Methods and Research. https://doi.org/10.1177/0049124121103616\n\n\n9. Jenkins, S. P., & Rios-Avila, F. (2021). Measurement error in earnings data: Replication of meijer, rohwedder, and wansbeek’s mixture model approach to combining survey and register data. Journal of Applied Econometrics, 1–10.\n\n\n10. Morrar, R., & Rios-Avila, F. (2021). Discrimination against refugees in the palestinian labor market. International Journal of Manpower, 42(2), 1002–1024.\n\n\n11. Mundra, K., & Rios-Avila, F. (2021). Using repeated cross-sectional data to examine the role of immigrant birth-country networks on unemployment duration: An application of guell and hu (2006) approach. Empirical Economics, 61, 389–415.\n\n\n12. Rios-Avila, F. (2021). Estimation of marginal effects for models with alternative variable transformations. The Stata Journal, 21(1), 81–96.\n\n\n13. Canavire-Bacarreza, G. J., Chong, A., Rios-Avila, F., & Yanez-Pagans, M. (2020). Will elders provide for their grandchildren? Unconditional cash transfers and educational expenditures in bolivia. Review of Development Economics, 24(2), 424–447.\n\n\n14. Hotchkiss, J. L., Moore, R. E., & Rios-Avila, F. (2020). Cost of policy choices: A microsimulation analysis of the impact on family welfare of unemployment and price changes. Journal of Macroeconomics, 63, 103167.\n\n\n15. Jenkins, S. P., & Rios-Avila, F. (2020). Modelling errors in survey and administrative data on employment earnings: Sensitivity to the fraction assumed to have error-free earnings. Economics Letters, 192, 109253.\n\n\n16. Rios-Avila, F. (2020). Recentered influence functions (RIF) in stata: RIF regression and RIF decomposition. Stata Journal, 20(1), 51–94.\n\n\n17. Rios-Avila, F. (2020). Smooth varying coefficient models in stata. The Stata Journal, 20(3), 647–679.\n\n\n18. Rios-Avila, F., & Canavire-Bacarreza, G. J. (2020). The effect of immigration on labor market transitions of native-born unemployed in the united states. Journal of Labor Research, 41, 295–331.\n\n\n19. Masterson, T., Rios-Avila, F., Zacharias, A., & Wolf, E. N. (2019). The great recession and racial inequality: Evidence from measures of economic well-being. Journal of Economic Issues, 53(4), 1048–1069.\n\n\n20. Rios-Avila, F. (2019). A semi-parametric approach to the oaxaca–blinder decomposition with continuous group variable and self-selection. Econometrics, 7(2), 28.\n\n\n21. Rios-Avila, F., & Canavire-Bacarreza, G. J. (2018). Standard error correction in two-stage optimization models: A quasi-maximum likelihood estimation approach. The Stata Journal, 18(1), 206–222.\n\n\n22. Canavire-Bacarreza, G. J., & Rios-Avila, F. (2017). On the determinants of wage inequality changes in urban bolivia. Journal of Human Development and Capabilities, 18(4), 464–496.\n\n\n23. Hotchkiss, J. L., Moore, R. E., Rios-Avila, F., & Trussel, M. (2017). A tale of two decades: Relative intra-family earning capacity and changes in family welfare over time. Review of Economics of the Household, 15(3), 707–737.\n\n\n24. Rioja, F. K., Rios-Avila, F., & Valev, N. T. (2017). Productivity during recessions with banking crises: Inter-industry evidence. Economics Letters, 152, 50–53.\n\n\n25. Rios-Avila, F. (2017). Unions and economic performance in developing countries: Case studies from latin america. Ecos de Economia: A Latin American Journal of Applied Economics, 21(44), 4–36.\n\n\n26. Saravia, A., Canavire-Bacarreza, G. J., & Rios-Avila, F. (2017). Intellectual property rights, foreign direct investment and economic freedom. Global Economy Journal, 23(2), 20170017.\n\n\n27. Canavire-Bacarreza, G. J., & Rios-Avila, F. (2016). The effect of intimate partner violence on labor market decisions: Evidence from a multi-ethnic country. International Journal of Social Economics, 44(1), 75–92.\n\n\n28. Aristizabal-Ramirez, M., Canavire-Bacarreza, G. J., & Rios-Avila, F. (2015). Revisiting the effects of innovation on growth: A threshold analysis. Applied Economics Letters, 22(18), 1474–1479.\n\n\n29. Hotchkiss, J. L., Moore, R. E., & Rios-Avila, F. (2015). Reevaluation of the employment impact of the 1996 summer olympic games. Southern Economic Journal, 81(3), 619–632.\n\n\n30. Hotchkiss, J. L., Quispe-Agnoli, M., & Rios-Avila, F. (2015). The wage impact of undocumented workers: Evidence from administrative data. Southern Economic Journal, 81(4), 874–906.\n\n\n31. Rios-Avila, F. (2015). Feasible estimation of linear models with n-fixed effects. Stata Journal, 15(3), 881–898.\n\n\n32. Hotchkiss, J. L., Pitts, M., & Rios-Avila, F. (2014). A search for evidence of skill mismatch in the aftermath of the great recession. Applied Economics Letters, 21(9), 587–592.\n\n\n33. Rioja, F. K., Rios-Avila, F., & Valev, N. T. (2014). Serial banking crises and capital investment. Emerging Markets Finance and Trade, 50(6), 193–208.\n\n\n34. Rioja, F. K., Rios-Avila, F., & Valev, N. T. (2014). The persistent effect of banking crises on investment and the role of financial markets. Journal of Financial Economic Policy, 6(1), 64–77.\n\n\n35. Rios-Avila, F., & Hirsch, B. (2014). Unions, wage gaps, and wage dispersion: New evidence from the americas. Industrial Relations: A Journal of Economy and Society, 53(1), 1–27.\n\n\n36. Hotchkiss, J. L., & Rios-Avila, F. (2013). Identifying factors behind the decline in the u.s. Labor force participation rate. Business and Economic Research, 3(1).\n\n\n37. Searing, E. A. M., Rios-Avila, F., & Lecy, J. D. (2013). Mental health and labor market outcomes in bosnia-herzegovina. Social Science & Medicine, 96(20), 165–173.\n\n\n38. Hotchkiss, J. L., Moore, R. E., & Rios-Avila, F. (2012). Assessing the welfare impact of tax reform: A case study of the 2001 US tax cut. The Review of Income and Wealth, 58(2), 233–256.\n\n\n39. Mercado-Salazar, A., Leiton, J., & Rios-Avila, F. (2004). Segmentación en el mercado matrimonial. Revista Latinoamericana de Desarrollo Económico, 3, 49–76."
  },
  {
    "objectID": "cv.html#awards-and-honors",
    "href": "cv.html#awards-and-honors",
    "title": "Fernando Rios-Avila",
    "section": "Awards and Honors",
    "text": "Awards and Honors\n\nJournal of Economic Issues, Editor’s Prize, 2019\nThe Stata Journal, Editors’ Prize, 2023"
  },
  {
    "objectID": "cv.html#thesis-advisor",
    "href": "cv.html#thesis-advisor",
    "title": "Fernando Rios-Avila",
    "section": "Thesis Advisor",
    "text": "Thesis Advisor\n\nMaría Aristizábal-Ramírez (Undergraduate Thesis)\nIan Bowen (Master Thesis)\nAlejandra Montoya (Master Thesis)\nDaniella Medina (Master Thesis)\nRhett Maiorama (Master Thesis)\nEsteban Rivera (Master Thesis)\nBrandon Istenes (Master Thesis)\nMasooma Hassan (Master Thesis)"
  },
  {
    "objectID": "cv.html#languages-and-software",
    "href": "cv.html#languages-and-software",
    "title": "Fernando Rios-Avila",
    "section": "Languages and Software",
    "text": "Languages and Software\nStatistical software with programming experience:\n\nStata: Advanced proficiency, developed and maintained various user-written programs\nR: Intermediate proficiency, used software for Econometrics classes\nPython: Intermediate proficiency for programming and basic econometric analysis\nBasic knowledge of Julia and SQL\n\nLanguages:\n\nSpanish (Native)\nEnglish (Fluent)"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Fernando Rios-Avila",
    "section": "Education",
    "text": "Education\n\n PhD in Economics, 2013\nGeorgia State University\n\n\n ASP in International Economics\nKiel University, Germany\n\n\n BS in Economics (Licenciatura)\nUniversidad Catolica Boliviana, La Paz, Bolivia"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Fernando Rios-Avila",
    "section": "Experience",
    "text": "Experience\n\n  Research Scholar, 2013 - Present\nLevy Economics Institute of Bard College\n\n\n Consultant, 2022 - Present\nThe World Bank"
  },
  {
    "objectID": "stataviz/index.html",
    "href": "stataviz/index.html",
    "title": "Stata Vizualizations",
    "section": "",
    "text": "Welcome to the data visualization section, where I explore the power of data visualization in Stata. Effective data visualization is essential for conveying insights and communicating results to diverse audiences.\nWhile Stata is a statistical software first, graphics software second, it does offer sufficient flexiblity, tools and options for creating informative data visualizations, although not without extra work.\nOn this webpage, I will compile a collection Stata programs that I have written for data visualization that will help you create visually appealing graphics for your research projects. I hope you will find these codes helpful in creating informative charts, histograms, scatterplots, and more.\nAs a data visualization enthusiast and Stata user, I am passionate about sharing my knowledge and skills with others. I believe that effective data visualization is crucial in conveying research findings and engaging audiences, but most importantly, programming them allows me to sharpen my skills!\nComments and suggestions are welcome!",
    "crumbs": [
      "Stata Viz",
      "Stata Vizualizations"
    ]
  },
  {
    "objectID": "stataviz/stataviz2.html",
    "href": "stataviz/stataviz2.html",
    "title": "Ploting Quantile Regressions: qregplot",
    "section": "",
    "text": "Quantile regressions are useful statistical tools that allow researchers to analyze the relationships between dependent and independent variables beyond their conditional means. To analyze such models, researchers commonly plot the estimation model coefficients across all estimated quantiles. This approach enables researchers to describe a larger set of coefficients, providing visuals of the trends and differences across the distribution.\nPreviously, the only command available to create distributional plots of this type was grqreg. However, this program had two limitations.\n\nIt does not support factor notation.\nIt could only be used after the official commands qreg, sqreg, and bsqreg.\n\nWith the introduction of new commands that focus on the estimation of quantile regressions, under different assumptions of strategies, there was a need for an update to the grqreg program. This update came in the form of a new command called qregplot.\nThis page aims to demonstrate how to use the qregplot command to plot coefficients from quantile regressions, which is compatible with a variety of quantile type commands.",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Ploting Quantile Regressions: `qregplot`"
    ]
  },
  {
    "objectID": "stataviz/stataviz2.html#plotting-quantile-coefficients",
    "href": "stataviz/stataviz2.html#plotting-quantile-coefficients",
    "title": "Ploting Quantile Regressions: qregplot",
    "section": "",
    "text": "Quantile regressions are useful statistical tools that allow researchers to analyze the relationships between dependent and independent variables beyond their conditional means. To analyze such models, researchers commonly plot the estimation model coefficients across all estimated quantiles. This approach enables researchers to describe a larger set of coefficients, providing visuals of the trends and differences across the distribution.\nPreviously, the only command available to create distributional plots of this type was grqreg. However, this program had two limitations.\n\nIt does not support factor notation.\nIt could only be used after the official commands qreg, sqreg, and bsqreg.\n\nWith the introduction of new commands that focus on the estimation of quantile regressions, under different assumptions of strategies, there was a need for an update to the grqreg program. This update came in the form of a new command called qregplot.\nThis page aims to demonstrate how to use the qregplot command to plot coefficients from quantile regressions, which is compatible with a variety of quantile type commands.",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Ploting Quantile Regressions: `qregplot`"
    ]
  },
  {
    "objectID": "stataviz/stataviz2.html#the-setup",
    "href": "stataviz/stataviz2.html#the-setup",
    "title": "Ploting Quantile Regressions: qregplot",
    "section": "The Setup",
    "text": "The Setup\nTo work with the following examples, you will need to install the following commands from SSC:\nssc install qregplot, replace\nssc install mmqreg, replace\nssc install qrprocess, replace\nssc install ivqreg2, replace\nssc install qreg2, replace\nssc install xtqreg, replace\nssc install sivqr, replace\nssc install color_style, replace\nAnd, for the data, we will use a very small dataset, available from Stata datasets examples:\n\nclear all\nwebuse womenwk, clear\nset scheme white2\ncolor_style tableau",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Ploting Quantile Regressions: `qregplot`"
    ]
  },
  {
    "objectID": "stataviz/stataviz2.html#how-does-it-work",
    "href": "stataviz/stataviz2.html#how-does-it-work",
    "title": "Ploting Quantile Regressions: qregplot",
    "section": "How does it work?",
    "text": "How does it work?\nAt its core, the command qregplot has a very simple structure, and its workflow can be described as follows:\n\nYou estimate a quantile regression model with the command of your choosing (if allowed in qregplot) for any quantile.\nqregplot reads all the metadata regarding the model specifictation, options, and command used from the information saved in e().\nUsing the same data structure, estimates the exactly same model using for all quantiles of interest.\nCollects all coefficients, and confidence intervals, to prepare the information for plotting.\nPlots the coefficients of the selected variables.",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Ploting Quantile Regressions: `qregplot`"
    ]
  },
  {
    "objectID": "stataviz/stataviz2.html#examples",
    "href": "stataviz/stataviz2.html#examples",
    "title": "Ploting Quantile Regressions: qregplot",
    "section": "Examples",
    "text": "Examples\nAssume you are interested in estimating a model, where wages are a function of age education, marital status, and county of residence.\nFurthermore, lets say that we are interested in conditional quantile regressions. You could estimate this as follows:\n\nqui:qreg wage age education married i.county\n\nSince we are using quantile regressions, you want to plot some of the coefficients across the distribution for all variables but the county dummies.\nBecause I know I may have to re-edit the graphs, I’ll store the coefficients into e_qreg. So I do not need to re-estimate all quantiles again.\n\nqregplot age education married ,  /// Variables to be plotted. (Must appear in the model specification)\n         estore(e_qreg) /// Request Storing the variables in memory\n         q(5(5)95) // and indicates what quantiles to plot\n\n\n\n\n\n\n\n\nThis is a very good start. In the newest version of qregplot, I add some default options to the plot, providing a look that is visually pleasing. Notice the margins are also tight (less waiste of space).\nNevertheless, I would like to use variable labels for titles in each figure. But I dont want to re-estimate them all, but what I can do is plot “from” the stored coefficients. When doint this, I do not need to specify q(list of q's) again.\n\nqregplot age education married , ///\n        from(e_qreg) /// &lt;- Indicates where to look for coefficients\n        label // No longer needed to add Q's and request Labels\n\n(results e_qreg are active now)\n\n\n\n\n\n\n\n\n\nIf you want, you can change the color, intensity, or transparency of the confidence intervals. But if you like what you see (I do), you can just keep it.\nNow, say that I want to see this, but in 3 cols. This would be a graph combine option. I will use the new syntax :\n\nqregplot age education married , ///\n        from(e_qreg) label ///\n        col(3) /// request 3 column with \n        ysize(5) xsize(12) // and different sizes for the graph\n\n(results e_qreg are active now)\n\n\n\n\n\n\n\n\n\nWith the new command, you can also modify the titles and costumize them as needed:\n\nqregplot age education married , ///\n        from(e_qreg) /// \n        col(2)   /// and different sizes for the graph\n        mtitles(\"Age in years o Edad en años desde 1900\" ///\n        \"Años de Educacion or Years of Education, incluye Highschool\" ///\n        \"Is Married - Esta Casado\") //&lt;- ads long titles\n\n(results e_qreg are active now)\n\n\n\n\n\n\n\n\n\nSo, seems that the titles are too long. So lets adjust them using labelopt(options). This option tells the program to break the long titles into two lines, so there is less overlapping in the figure.\n\nqregplot age education married , ///\n    from(e_qreg) /// \n    col(2)   /// and different sizes for the graph\n    mtitles(\"Age in years o Edad en años desde 1900\" ///\n    \"Años de Educacion or Years of Education, incluye Highschool\" ///\n    \"Is Married - Esta Casado\") ///&lt;- ads long titles\n    labelopt(lines(2)) // breaks it in two lines\n\n(results e_qreg are active now)",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Ploting Quantile Regressions: `qregplot`"
    ]
  },
  {
    "objectID": "stataviz/stataviz2.html#using-other-qreg-commands",
    "href": "stataviz/stataviz2.html#using-other-qreg-commands",
    "title": "Ploting Quantile Regressions: qregplot",
    "section": "Using other qreg commands",
    "text": "Using other qreg commands\nAbove I showed you the basic syntax for most options. An advantage of qregplot, however, is that you can use it for other commands as well.\nAs a side bonus, I showcase color_style, changing the colors of the different plots:\n\nqui:bsqreg wage age education married i.county, reps(25) \ncolor_style bay \nqregplot age education ,  q(5(5)95) seed(101) label title(\"BSqreg\") ///\nysize(5) xsize(12)\n\n\n\n\n\n\n\n\n\nqui:qreg2 wage age education married i.county, \ncolor_style  austria  \nqregplot age education , q(5(5)95) seed(101) label ///\n    title(\"QREG2: from SSC\") ysize(5) xsize(12)\n\n\n\n\n\n\n\n\n\nqui:qrprocess wage age education married i.county, \ncolor_style  robert \nqregplot age education ,  q(5(5)95) seed(101) label ///\n    title(\"qrprocess: from SSC\")  ysize(5) xsize(12)\n\n\n\n\n\n\n\n\n\nqui:mmqreg wage age education married, abs( county)\ncolor_style derain \nqregplot age education ,  q(5(5)95) seed(101) label ///\n    title(\"mmqreg: from SSC\") ysize(5) xsize(12)\n\n\n\n\n\n\n\n\n\nqui:rifhdreg wage age education married, abs( county) rif(q(50))\ncolor_style shakira\nqregplot age education ,  q(5(5)95) seed(101) label ///\n    title(\"rifhdreg: from SSC\") ysize(5) xsize(12)\n\n\n\n\n\n\n\n\n\nqui:sivqr wage age education married, q(50)\ncolor_style troy\nqregplot age education,  q(5(5)95) seed(101) label ///\n    title(\"sivqr: from SSC\")  ysize(5) xsize(12)",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Ploting Quantile Regressions: `qregplot`"
    ]
  },
  {
    "objectID": "stataviz/stataviz2.html#conclusions",
    "href": "stataviz/stataviz2.html#conclusions",
    "title": "Ploting Quantile Regressions: qregplot",
    "section": "Conclusions",
    "text": "Conclusions\nThere you have it. A small update that makes making qreg plots easy.\nJust remember, qregplot is not a quantile regression estimator, it should be considered as a post estimation command that simply replicates what you had before.\nQuestions? comments? suggestions?\nfeel free to drop me a message.",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Ploting Quantile Regressions: `qregplot`"
    ]
  },
  {
    "objectID": "stataviz/stataviz4.html",
    "href": "stataviz/stataviz4.html",
    "title": "Make Scatters more Colorful: mscatter",
    "section": "",
    "text": "The program mscatter came as an answer to two questions, often asked on statalist.\n\nHow to produce Scatter plots by groups? to each group is labeled?\nHow can I change the colors, if I have more 15 groups in my graph?\n\nDoing plots that produce this is simple. One just needs to create multiple scatterplots, and combine them with twoway. This will produce you this kind of plot. For example, if I had to produce and join 4 scatter plots (across different groups) I would do the following:\ntwoway (scatter y x if z ==1, options) ///\n       (scatter y x if z ==2, options) ///\n       (scatter y x if z ==3, options) ///\n       (scatter y x if z ==4, options), ///\n       overall options\nDoing this for multiple groups, however, can be a pain. Thus, I decided to write a do-file that address this problem.",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Make Scatters more Colorful: `mscatter`"
    ]
  },
  {
    "objectID": "stataviz/stataviz4.html#introduction",
    "href": "stataviz/stataviz4.html#introduction",
    "title": "Make Scatters more Colorful: mscatter",
    "section": "",
    "text": "The program mscatter came as an answer to two questions, often asked on statalist.\n\nHow to produce Scatter plots by groups? to each group is labeled?\nHow can I change the colors, if I have more 15 groups in my graph?\n\nDoing plots that produce this is simple. One just needs to create multiple scatterplots, and combine them with twoway. This will produce you this kind of plot. For example, if I had to produce and join 4 scatter plots (across different groups) I would do the following:\ntwoway (scatter y x if z ==1, options) ///\n       (scatter y x if z ==2, options) ///\n       (scatter y x if z ==3, options) ///\n       (scatter y x if z ==4, options), ///\n       overall options\nDoing this for multiple groups, however, can be a pain. Thus, I decided to write a do-file that address this problem.",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Make Scatters more Colorful: `mscatter`"
    ]
  },
  {
    "objectID": "stataviz/stataviz4.html#mscatter-scatter-with-multiple-groups",
    "href": "stataviz/stataviz4.html#mscatter-scatter-with-multiple-groups",
    "title": "Make Scatters more Colorful: mscatter",
    "section": "mscatter: scatter with Multiple groups",
    "text": "mscatter: scatter with Multiple groups\nFirst some minimal setup. The latest version of mscatter is available from SSC. You can install it using the following:\nssc install mscatter\nIn the examples below, I will also use the scheme white2 within [color_style]](https://friosavila.github.io/stataviz/stataviz1.html). See instructions there if you want to install all its components.\nNow, how do you produce a scatter with multiple groups. The task is, in fact, simple. If I were to use auto.dta dataset, it would be something like this:\n\nsysuse auto, clear\ntwo scatter price mpg if foreign == 1 || ///\n    scatter price mpg if foreign == 0\ngraph export ms1.png, width(1200) replace   \n\n\nIn fact, I have been pointted out to one command by Nick Cox, named linkplot, which may also help you produce similar plots.\n\nssc install linkplot\nlinkplot price mpg , link(foreign) asyvars recast(scatter)\ngraph export ms2.png, width(1200) replace   \n\n\nAnd produces pretty much the same. The limitation: only works with up to 15 groups. But who needs more right? Well, if you need more, you can use mscatter!\n\nset scheme white2                   // Lets use white scheme\nmscatter price mpg , over(rep78)    /// Upto here normal. I use over instead of By\n  alegend  legend(cols(5)) msize(3) /// add a legend with large dots\n  by(foreign)                       // and groups by foreign\ngraph export ms3.png, width(1200) replace   \n\n\nBut those are easy to do by hand. What if you have many groups. Lets see with some different data:\n\nwebuse nlswork\nmscatter ln_wage ttl_exp ,     /// normal\n          over(age)            /// but over many groups!\n          colorpalette(magma ) /// all color coded\n          alegend               // with a legend to match\ngraph export ms4.png, width(1200) replace           \n\n\nAnd as I show you before, mscatter can be combined with by()\n\nmscatter ln_wage ttl_exp , over(grade) ///\n      colorpalette(magma ) by(race, legend(off)) // legend(off) should go here\ngraph export ms5.png, width(1200) replace       \n\n\nOnly current limitation, you can use weights (for size of markers) but it will fail, if you have groups without observations based on over and by (difficult to explain and reproduce).",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Make Scatters more Colorful: `mscatter`"
    ]
  },
  {
    "objectID": "stataviz/stataviz4.html#other-features",
    "href": "stataviz/stataviz4.html#other-features",
    "title": "Make Scatters more Colorful: mscatter",
    "section": "Other features",
    "text": "Other features\nmscatter has other features that might put it apart from other commands. While the original version was written to take advantage of frames. The current version will also work with earlier Stata versions (at least V14), using preserve/restore commands.\nIn addition to this, mscatter may also allow you to add fitted plots to your scatter!\nLets see how this works:\n\nwebuse nlswork, clear\nset seed 10\nkeep if runiform()&lt;.2\ncolor_style tableau\nmscatter ln_wage ttl_exp , over(race)  ///\n        fit(qfitci) ///&lt;-adds quadratic fit with CI\n        mfcolor(%5) mlcolor(%5) alegend ytitle(\"Log Wages\")  \n\ngraph export ms6.png, width(1200) replace         \n\n\nAs you can see, this figure plots log of wages agains total experience across race groups. In addition to this, however, it also adds a fitted plot using quadratic fit using the option fit(qfitci). You can add here other options that are command specific. Of course, with the addition of this, you may want to drop the scatter plot all together\n\nmscatter ln_wage ttl_exp , over(race)  ///\n        fit(qfitci) mfcolor(%5) mlcolor(%5) ///\n        noscatter  /// Asks not to show the scatter points\n        alegend ytitle(\"Log Wages\") \n\ngraph export ms7.png, width(1200) replace",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Make Scatters more Colorful: `mscatter`"
    ]
  },
  {
    "objectID": "stataviz/stataviz4.html#conclusions",
    "href": "stataviz/stataviz4.html#conclusions",
    "title": "Make Scatters more Colorful: mscatter",
    "section": "Conclusions",
    "text": "Conclusions\nmscatter is a data-visualization utility that may enable you to create informative graphs capturing correlations between 2 variables.\nWhile its use may be limited, I believe that those who need it, may find this a good tool to have for their arsenal.\nLastly, try checking the helpfile, where I add other options, not described here.\nTil next time!",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Make Scatters more Colorful: `mscatter`"
    ]
  },
  {
    "objectID": "stataviz/stataviz8.html",
    "href": "stataviz/stataviz8.html",
    "title": "Making spirograph & harmograph!",
    "section": "",
    "text": "I believe the correct term for them is Guilloche patterns. From a programming perspective these are figures created by drawing circles, connected to circles, connected to more circles, so on and so forth. The subsequent circles build up on each other, and with the right parameters, and sufficient flexibility, one could potentially use the principle of Spirographs to draw anything using very complex functions.\n\nSo how does this work?\n\nFirst, we need to change the perspective from a cartesian coordinates \\((x,y)\\), to one with polar coordinates, where every point in space is determined by a distance to the origin (\\(r\\)), and the angle of a line that connects it to the origin, with respect to the horizon. However, for drawing them, we need to transform them back to the cartesian coordinates:\n\\[\n\\begin{aligned}\ny = r \\times sin (\\theta) \\\\\nx = r \\times cos (\\theta)\n\\end{aligned}\n\\]\nSo, if we fix \\(r\\) to be the same and change \\(\\theta\\) from 0 to \\(2\\pi\\), we could draw a simple circle:",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Making `spirograph` & `harmograph`!"
    ]
  },
  {
    "objectID": "stataviz/stataviz8.html#what-are-spirographs",
    "href": "stataviz/stataviz8.html#what-are-spirographs",
    "title": "Making spirograph & harmograph!",
    "section": "",
    "text": "I believe the correct term for them is Guilloche patterns. From a programming perspective these are figures created by drawing circles, connected to circles, connected to more circles, so on and so forth. The subsequent circles build up on each other, and with the right parameters, and sufficient flexibility, one could potentially use the principle of Spirographs to draw anything using very complex functions.\n\nSo how does this work?\n\nFirst, we need to change the perspective from a cartesian coordinates \\((x,y)\\), to one with polar coordinates, where every point in space is determined by a distance to the origin (\\(r\\)), and the angle of a line that connects it to the origin, with respect to the horizon. However, for drawing them, we need to transform them back to the cartesian coordinates:\n\\[\n\\begin{aligned}\ny = r \\times sin (\\theta) \\\\\nx = r \\times cos (\\theta)\n\\end{aligned}\n\\]\nSo, if we fix \\(r\\) to be the same and change \\(\\theta\\) from 0 to \\(2\\pi\\), we could draw a simple circle:",
    "crumbs": [
      "Stata Viz",
      "Stataviz",
      "Making `spirograph` & `harmograph`!"
    ]
  },
  {
    "objectID": "stata_do/stata_do1.html",
    "href": "stata_do/stata_do1.html",
    "title": "Using Quarto for Stata dynamic documents",
    "section": "",
    "text": "This article is a simple example of how to use Quarto to create dynamic documents using Stata. Which is possible using the Stata kernel nbstata thanks to Tim Huegerich, who put this incredible tool together.\nTo use this, you will need:\n\nStata 17 or higher\npython\nnbstata",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Using Quarto for Stata dynamic documents"
    ]
  },
  {
    "objectID": "stata_do/stata_do1.html#introduction",
    "href": "stata_do/stata_do1.html#introduction",
    "title": "Using Quarto for Stata dynamic documents",
    "section": "",
    "text": "This article is a simple example of how to use Quarto to create dynamic documents using Stata. Which is possible using the Stata kernel nbstata thanks to Tim Huegerich, who put this incredible tool together.\nTo use this, you will need:\n\nStata 17 or higher\npython\nnbstata",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Using Quarto for Stata dynamic documents"
    ]
  },
  {
    "objectID": "stata_do/stata_do1.html#installation",
    "href": "stata_do/stata_do1.html#installation",
    "title": "Using Quarto for Stata dynamic documents",
    "section": "Installation",
    "text": "Installation\nFor now lets assume that you have Stata and Python installed, and working together, so that you can use Stata from Python. If this is working, installing nbstata is as simple as typing the following from the command window:\npip install nbstata\npython -m nbstata.install \n\n# or to update \npip install nbstata --upgrade\nYou probably do not need --conf-file, but if you want to apply other options to Stata output, you may beed to create the config file, and modified as needed.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Using Quarto for Stata dynamic documents"
    ]
  },
  {
    "objectID": "stata_do/stata_do1.html#quarto-setup",
    "href": "stata_do/stata_do1.html#quarto-setup",
    "title": "Using Quarto for Stata dynamic documents",
    "section": "Quarto Setup",
    "text": "Quarto Setup\nWhile this page is being rendered as a quarto website project, you can also use the following YAML heading, once you installed Python and nbstata in your computer. You will also need Quarto (if using say VSC) or one of the latests iterations of Rstudio.\n---\ntitle: Using Quarto for Stata dynamic documents\nformat: html\njupyter: nbstata\n---\nThis request using the nbstata as the kernel, allowing Quarto to use Python to call on Stata.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Using Quarto for Stata dynamic documents"
    ]
  },
  {
    "objectID": "stata_do/stata_do1.html#lets-do-this",
    "href": "stata_do/stata_do1.html#lets-do-this",
    "title": "Using Quarto for Stata dynamic documents",
    "section": "Lets do this:",
    "text": "Lets do this:\nLet us consider an example where we study the mpg and weight variables in auto.dta. In this example, all code used to construct the desired output will be displayed as fenced block code, followed by the output it produces. This is done using the option *|echo: fenced.\nWe first use the sysuse command to load the dataset and then describe the data using the describe command.\n\n\nCode\n```{stata}\nsysuse auto, clear\ndescribe\n```\n\n\n\n\n\n(1978 automobile data)\n\nContains data from C:\\Program Files\\Stata17/ado\\base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2020 17:45\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nmake            str18   %-18s                 Make and model\nprice           int     %8.0gc                Price\nmpg             int     %8.0g                 Mileage (mpg)\nrep78           int     %8.0g                 Repair record 1978\nheadroom        float   %6.1f                 Headroom (in.)\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn circle (ft.)\ndisplacement    int     %8.0g                 Displacement (cu. in.)\ngear_ratio      float   %6.2f                 Gear ratio\nforeign         byte    %8.0g      origin     Car origin\n-------------------------------------------------------------------------------\nSorted by: foreign\n\n\nNow, we want to check if mpg is always greater than 0 and less than 100. We use the assert command to perform the check. In this case, we do not want to include any output in the target HTML file, so we use the quietly attribute to modify the behavior of the dd_do Stata dynamic tag.\n\n\nCode\n```{stata}\n assert mpg &gt; 0 & mpg &lt; 100\n```\n\n\nIf the data do not satisfy the conditions, quatro will fail with an error message, which will occur if we run the same assert command in a do-file.\nNext, we want to summarize the weight variable:\n\n\nCode\n```{stata}\nsummarize weight\n```\n\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      weight |         74    3019.459    777.1936       1760       4840\n\n\nWe want to use the minimum and maximum values of weight in a sentence. Instead of copying and pasting the numbers from the summarize output, we can use the display Stata to show r(min) and r(max) stored results. We will also use the options *| output: asis to obtain text that follows markdown formatting.\n\nCode\n```{stata}\n*| output: asis\ndisplay \"The variable weight has minimum value \"  %4.2f `r(min)' \" and \" ///\n         \"has maximum value \"   %4.2f `r(max)' \".\"\n```\n\nThe variable weight has minimum value 1760.00 and has maximum value 4840.00.\nIn other words, if one wants to use dynamic tags, its possible to do so by simply using display, with the corresponding locals, so Stata evaluates the expressions as normal. *| output: asis is used so the output can still be interpreted using markdown syntax.\nAs with dyndoc, display can also be used as a calculator. For example, if we want to include the \\(range = max - min\\) in a sentence, instead of calculating the number and then copying and pasting it, we can use\n\nCode\n```{stata}\n*| output: asis\ndisplay \"The variable weight has range \"  %4.2f `r(max)'-`r(min)' \".\"\n```\n\nThe variable weight has range 3080.00.\nNow, we want to graph mpg and weight using a scatterplot. There are at least two ways to do this.\nFirst, one can simply create the scatterplot using the same procedure as before:\n\n\nCode\n```{stata}\nscatter mpg weight, mcolor(blue%50)\n```\n\n\n\n\n\n\n\n\n\nwhich generates a scatterplot of mpg and weight with 50% opacity color markers.\nNow, we want to export the graph to a file and include an image link to the file.\n\n\nCode\n```{stata}\nqui:graph export fig1.png, width(1600) replace\n```\n\n\nThis produces a graph of 1600 pixels width.\n\n\n\nscatter\n\n\nIt is possible, however, to combine figure creation using quatro tags and directives. Here, however, you need to make sure all figures are named:\n\nCode\n```{stata}\n*| label: fig-cost\n*| fig-cap: Price vs MPG\n*| fig-subcap:\n*|   - Foreign Cars\n*|   - Domestic Cars\n*| layout-ncol: 2\n*| column: page\n\nscatter price mpg if foreign==1, name(m1, replace) ylabel(0(4000)16000)\nqui:graph export fig2a.png, width(1600) replace\nscatter price mpg if foreign==0, name(m2, replace) ylabel(0(4000)16000)\nqui:graph export fig2b.png, width(1600) replace\n```\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Foreign Cars\n\n\n\n\n\n\n\n\n\n\n\n(b) Domestic Cars\n\n\n\n\n\n\n\nFigure 1: Price vs MPG\n\n\n\nAnd of course, we can now the figure tags to link it to the text:\nFigure 1 provides a simple scatter between prices and MPG for foreign and domestic cars. While there seems to be a strong negative relationship between these variables among foreign cars (see Figure 1 (a)), the relationship among domestic cars is much weaker, when looking at cars with a fuel efficiency larger than 15mpg (see Figure 1 (b)).\nThe last approach, however, may not work with PDF format, or jupyter-notebook format, unless the figures are saved. But does seem to work with HTML and docx.\nNevertheless, one could also do the following:\n::: {#fig-mpgprice layout-ncol=2 .column-page }\n![foreign](fig2a.png){#fig-mpgprice-1}\n![domestic](fig2b.png){#fig-mpgprice-2}\nPrice vs MPG\n:::\nto produce\n\n\n\n\n\n\n\n\n\n\n\n(a) foreign\n\n\n\n\n\n\n\n\n\n\n\n(b) domestic\n\n\n\n\n\n\n\nFigure 2: Price vs MPG\n\n\n\nAs plot in Figure 2 provides a simple scatter between prices and MPG for foreign and domestic cars. While there seems to be a strong negative relationship between these variables among foreign cars (see Figure 2 (a)), the relationship among domestic cars is much weaker, when looking at cars with a fuel efficiency larger than 15mpg (see Figure 2 (b)).\nIf using VScode, to render all formats at once, you need to type quarto render filename.qmd in the terminal.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Using Quarto for Stata dynamic documents"
    ]
  },
  {
    "objectID": "stata_do/stata_do3.html",
    "href": "stata_do/stata_do3.html",
    "title": "Linear Regression via MLE",
    "section": "",
    "text": "Stata has a very useful command that can be used for the estimation of almost any linear and nonlinear models using maximum likelihood. This command is -ml-.\nProperly speaking, this command can be used to obtain M-type estimators, however, I’ll concentrate on maximum likelihood models.\nIn this small tutorial, I’ll provide a small example of how to use -ml- in combination with -margins- to estimate marginal effects for a linear model, as long as one identifies the outcome of interest.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Linear Regression via MLE"
    ]
  },
  {
    "objectID": "stata_do/stata_do3.html#introduction",
    "href": "stata_do/stata_do3.html#introduction",
    "title": "Linear Regression via MLE",
    "section": "",
    "text": "Stata has a very useful command that can be used for the estimation of almost any linear and nonlinear models using maximum likelihood. This command is -ml-.\nProperly speaking, this command can be used to obtain M-type estimators, however, I’ll concentrate on maximum likelihood models.\nIn this small tutorial, I’ll provide a small example of how to use -ml- in combination with -margins- to estimate marginal effects for a linear model, as long as one identifies the outcome of interest.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Linear Regression via MLE"
    ]
  },
  {
    "objectID": "stata_do/stata_do3.html#lr-as-mle",
    "href": "stata_do/stata_do3.html#lr-as-mle",
    "title": "Linear Regression via MLE",
    "section": "LR as MLE",
    "text": "LR as MLE\nAs I mentioned earlier, the ml command in Stata is a powerful tool for obtaining maximum likelihood estimators, although it can be used to find solutions for any m-type estimators. The one limitation I have encountered with this command is that it can be resource-intensive when estimating complex models on large datasets. For instance, If you have a dataset with one million observations but only use 10% of it for modeling, dropping the unused data before estimation can speed up the process. ml will not do that for you.\nThere are several ways to program ml, such as using the lf, df0, df1, or df2 options. The main difference among them is that you must define the objective function, its gradient, and its Hessian. However, for most purposes, I find that lf is the only one you will ever need.\nWhen using ml with the lf option, you only need to declare the loglikelihood function contributed by each observation in the sample. To illustrate this concept, let’s assume that we want to estimate a simple linear regression using MLE. For this, we need to assume that either the error follows a normal distribution or that the outcome follows a conditionally normal distribution.\n\\[\n\\begin{aligned}\ny_i &= X_i'\\beta + e_i \\\\\ne_i &\\sim N(0,\\sigma^2) \\ or \\ y_i \\sim N(X_i'\\beta,sigma^2)\n\\end{aligned}\n\\]\nThis implies that the Likelihood (\\(L_i\\)) and Log-Likelihood (\\(LL_i\\)) of a single observation is given by: \\[\n\\begin{aligned}\nL_i &= \\frac{1}{\\sigma \\sqrt{2\\pi}} exp\\left( -\\frac{1}{2} \\left(\\frac{y_i -X_i'\\beta }{\\sigma} \\right)^2 \\right) \\\\\nLL_i &= -log(\\sigma) - \\frac{1}{2} log (2\\pi) -\\frac{1}{2} \\left(\\frac{y_i -X_i'\\beta }{\\sigma} \\right)^2\n\\end{aligned}\n\\]\nSo we just need to create a program that defines this log-likelihood function.\n\n\n\n\n\n\nprogram myols_mle\n    args lnf xb lnsigma\n    local sigma exp(`lnsigma')\n    *qui:replace `lnf' = -`lnsigma' - 1/2 * log(2*_pi) - 1/2 *(($ML_y1-`xb')/`sigma')^2\n    qui: replace `lnf' = log(normalden($ML_y1,`xb',`sigma'))\nend\n\n\n\n\nNotice that this program has 3 arguments (which come after args) (see line 2)\n\nlnf: Will store the log-Likelihood for each observation\nxb: Will store the linear combination of variables and their coefficients\nlnsigma: Will store the \\(log(\\sigma)\\).\n\nWe do not estimate \\(\\sigma\\) directly, because its numerically more stable to estimate the \\(log(\\sigma)\\). Also, we require \\(\\sigma\\) to be strictly positive, which can only be done by using the log transformation.\nIn line 3, I specifically impose the transformation to obtain \\(\\sigma\\).\nIn line 4, I leave the comment of how I would write the full Loglikelihood using the formula I provided before, but for simplicilty I use the built-in normalden (line 5)\nYou should also notice that all arguments will be handled internally as locals, which is why they need to be written within single quotes: ' . The only exception is$ML_y1`, which represents the dependent variable.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Linear Regression via MLE"
    ]
  },
  {
    "objectID": "stata_do/stata_do3.html#programming-for-margins",
    "href": "stata_do/stata_do3.html#programming-for-margins",
    "title": "Linear Regression via MLE",
    "section": "Programming for Margins",
    "text": "Programming for Margins\nThe second component I’m interested in introducing today is to create a program that will allow you to modify how margins, operate.\nAs some may know, margins is a Stata built-in program that estimates marginal effects, or marginal means, for any official and unofficial model in Stata. The way it works is rather simple. Once you estimat you model of interest and call on margins it will:\n\nEstimate the predicted outcome for the model. This varies by model, but the default is to use the linear combination of variables and coefficients.\nDepending on other modeling factors, estimate derivatives, or means, of the outcome with respect to every variable in the model.\nCalculate standard errors using Delta method, and summarize results.\n\nSteps 2 and 3 above are common to all models. However, step 1 is the one that needs to be modified everytime one changes from one model to another.\nWhat I will do next is to write a program where I will define different types of outcomes that I may be interested in when analyzing Linear regressions. I will call this program myols_mle_p, following Stata naming standards:\n\nprogram myols_mle_p\n    syntax newvarname [if] [in] , [ mean sigma emean *]\n    if \"`mean'`sigma'`emean'\" ==\"\" {\n        ml_p `0'\n    }\n    marksample touse, novarlist\n    if \"`mean'\" !=\"\"  {\n        tempvar xb\n        _predict double `xb' , eq(#1)\n        gen `typlist' `varlist' = `xb' if `touse'\n        label var `varlist' \"E(y|X)\"\n    }\n    else if \"`sigma'\" !=\"\"  {\n        tempvar xb\n        _predict double `xb' , eq(#2)\n        gen `typlist' `varlist' = exp(`xb') if `touse'\n        label var `varlist' \"E(sigma|X)\"\n    }\n    else if \"`emean'\"!=\"\" {\n        tempvar xb lns\n        _predict double `xb' , eq(#1)\n        _predict double `lns' , eq(#2)\n        local sigma (exp(`lns'))\n        gen `typlist' `varlist' = exp(`xb')*exp(0.5*`sigma'^2) if `touse'\n        label var `varlist' \"E(exp(Y)|X)\"\n    }\nend\n\n\n\n\nThe way I’m defining this program, one could request 3 types of outcomes:\n\nmean: This is the standard outcome. Just looking into the linear combination of X and betas\nsigma: When this option is used, you will obtain the prediction for \\(\\sigma\\) instead of \\(log(\\sigma)\\). May be useful to compare and test heteroskedasticity directly\nemean: This is something different. This option could be used if your outcome of interest was “wages”, but you were modeling “log(wages)”. This will be estimated under the assumption of log-normality.\n\nIf neither option is used, it will revert to use the default.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Linear Regression via MLE"
    ]
  },
  {
    "objectID": "stata_do/stata_do3.html#the-estimation",
    "href": "stata_do/stata_do3.html#the-estimation",
    "title": "Linear Regression via MLE",
    "section": "The Estimation",
    "text": "The Estimation\nBefore we estimate the LR model using MLE, lets start by loading some data:\n\nfrause oaxaca, clear\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n\nNow, to estimate a model using -ml-, we need to use a somewhat complex syntax, that would be better explained in the following code:\nml model lf myols_mle /// \n   (mean:lnwage = educ exper i.female) ///\n   (ln_sigma:  = educ exper i.female) ///\n   , maximize  nolog\nml display   \nLine 1. Indicates you will try and estimate a model using ml, where the model will use lf option (which only requires the log-likelihood function at individual level to be provided. You also need to provide the name of the program that defines the LL function: myols_mle.\nLine 2 and 3. As described earlier, the program myols_mle requires 3 arguments. The first one is the LL function, so we do not need to be concernd about. The other two arguments refer to xb or conditional mean, and lnsigma or log of the variance, need to be declared here. They are order specific. Line 2 will always refer to xb, independent of the name I provide, and Line 3 will always refer to lnsigma.\nIn standard models, we assume homoskedasticity, so we do not need to add covariates to lnsigma, but we can do it and control for heteroskedasticity directly.\nIn line 4, I simply request model to be maximized, but could just as well requested clustered standard errors, or use other options allowed in ml.\nFinally line 5 request displaying the results. So lets see what we get:\n\n                                                        Number of obs =  1,434\n                                                        Wald chi2(3)  = 371.53\nLog likelihood = -871.18998                             Prob &gt; chi2   = 0.0000\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nmean         |\n        educ |   .0736371   .0045872    16.05   0.000     .0646464    .0826278\n       exper |   .0105825   .0010551    10.03   0.000     .0085147    .0126504\n    1.female |  -.1118167    .024106    -4.64   0.000    -.1590636   -.0645698\n       _cons |   2.430164   .0641158    37.90   0.000     2.304499    2.555828\n-------------+----------------------------------------------------------------\nln_sigma     |\n        educ |  -.0356134   .0067794    -5.25   0.000    -.0489008   -.0223259\n       exper |  -.0165439   .0016363   -10.11   0.000     -.019751   -.0133368\n    1.female |   .2066704   .0382432     5.40   0.000     .1317152    .2816256\n       _cons |  -.2813734   .0910696    -3.09   0.002    -.4598665   -.1028802\n------------------------------------------------------------------------------\nYou can compare these results with the standard regress outcome, or hetregress if you want to compare the results allowing for heteroskedastic errors.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Linear Regression via MLE"
    ]
  },
  {
    "objectID": "stata_do/stata_do3.html#margins",
    "href": "stata_do/stata_do3.html#margins",
    "title": "Linear Regression via MLE",
    "section": "margins",
    "text": "margins\nMargins can be used here to analyze the effect of covariates on the outcome of interest. The default from -ml- is to use linear combinations of coefficients and covariates. Now, if we want to use our predict command, we need to modify one piece of information in e(). If you type ereturn list, after every Stata command, you will see there is one local named e(predict). This local has the name of a program that is used to get predictions for a given model. We need to modify it, and change the default name to our program: myols_mle_p.\nWe will do this with the following program:\n\nprogram adde, eclass\n    ereturn `0'\nend\nadde local predict myols_mle_p\n\n\n\n\nOk with all of this, we are ready to estimate marginal effects. The next two lines should give you the same result, but I present them here as an example:\n\nmargins, dydx(*)\nmargins, dydx(*) predict(mean)  \n\n\nAverage marginal effects                                 Number of obs = 1,434\nModel VCE: OIM\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  educ exper 1.female\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0736371   .0045872    16.05   0.000     .0646464    .0826278\n       exper |   .0105825   .0010551    10.03   0.000     .0085147    .0126504\n    1.female |  -.1118167    .024106    -4.64   0.000    -.1590636   -.0645698\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\nAverage marginal effects                                 Number of obs = 1,434\nModel VCE: OIM\n\nExpression: E(y|X), predict(mean)\ndy/dx wrt:  educ exper 1.female\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0736371   .0045872    16.05   0.000     .0646464    .0826278\n       exper |   .0105825   .0010551    10.03   0.000     .0085147    .0126504\n    1.female |  -.1118167    .024106    -4.64   0.000    -.1590636   -.0645698\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\n\nWhat would be more interesting, however, would be to provide outcomes for the predicted standard deviation, or the exponentiated mean:\n\nmargins, dydx(*) predict(sigma)  \n\n\nAverage marginal effects                                 Number of obs = 1,434\nModel VCE: OIM\n\nExpression: E(sigma|X), predict(sigma)\ndy/dx wrt:  educ exper 1.female\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |  -.0161816   .0031133    -5.20   0.000    -.0222836   -.0100796\n       exper |   -.007517    .000774    -9.71   0.000    -.0090341       -.006\n    1.female |   .0938119   .0175522     5.34   0.000     .0594102    .1282136\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\n\n\nmargins, dydx(*) predict(emean)  \n\n\nAverage marginal effects                                 Number of obs = 1,434\nModel VCE: OIM\n\nExpression: E(exp(Y)|X), predict(emean)\ndy/dx wrt:  educ exper 1.female\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   2.175891    .169221    12.86   0.000     1.844224    2.507558\n       exper |    .236423   .0394785     5.99   0.000     .1590466    .3137995\n    1.female |   -2.27482   .8226334    -2.77   0.006    -3.887152   -.6624884\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\n\nSo how do we interpret the results?. Here one possibility:\nEach additional year of education increases wages by 7.4%, however, as education increases the dispersion of wages decreases. In terms of actual dollar change, in average, that additional year of education translates in a 2.17$ per hour increase.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Linear Regression via MLE"
    ]
  },
  {
    "objectID": "stata_do/stata_do3.html#conclusions",
    "href": "stata_do/stata_do3.html#conclusions",
    "title": "Linear Regression via MLE",
    "section": "Conclusions",
    "text": "Conclusions\nThe purpose of this small article was to walk you through how to use -ml- for the estimation of a simple linear regression.\nIn addition, it also introduces you to creating a program that will allow you to use margins, when you have a specific outcome in mind, that is not currently available in the command you are interested in using.\nHope you find it useful.\nComments and question welcome!",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Linear Regression via MLE"
    ]
  },
  {
    "objectID": "stata_do/stata_do5.html",
    "href": "stata_do/stata_do5.html",
    "title": "Simultaneous/Uniform Confidence Intervals",
    "section": "",
    "text": "Confidence intervals (CI), also known as interval estimates, are ranges of values that provided along point estimates as potential values for population parameters, based on the estimation method, level of confidence, and degree of precision of those estimates.\nOne usual confusion that arises from CI is its interpretation. Its important to emphasize that CI are just estimates, and are not guaranteed to contain the true estimate.\nFor example, when we say the estimate for \\(\\alpha\\) is 1 with a 95% CI of \\((-.5,.5)\\), it doesnt mean that the true parameter will be contained in that confidence interval with a 95% probability.\nInstead, it means that if we repeat the sampling excercise a large number of times, and construct a CI of the same level, it will contain the true parameter 95% of the time.\n\n\nCode\nclear\nset scheme white2\nset obs 200\ngen p = (_n*2-1)/(2*_N)\ngen sd = (1/sqrt(500))\ngen b1 = invnormal(p)*sd\ngen ll=b1-sd*invnormal(0.95)\ngen uu=b1+sd*invnormal(0.95)\n\ngen n=p\ntwo (rarea ll uu n if uu&lt;0,color(red%50)) (rarea ll uu n if ll&gt;0,color(red%50)) ///\n    (rarea ll uu n if inrange(0,ll,uu), color(green%20)) (line b1 n, color(gs1)) , ///\n    legend(order(1 \"Rejection of Null\" 3 \"Null not Rejected\"))  \ngraph export uci1.png, width(1000)  height(500) replace\n\n\n\n\n\n\n\n\nFigure 1: 90% CI\n\n\n\nAs an example, Figure 1 shows a 90% Confidence intervals for a coefficient that by construction has a true value of zero. When considering the CI, the middle 90% of the coefficients contain the true paramater of zero. But the upper and lower 5% do not. Thus, with a 90% confidence interval, we have 10% chances of rejecting the Null when it is true.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Simultaneous/Uniform Confidence Intervals"
    ]
  },
  {
    "objectID": "stata_do/stata_do5.html#introduction",
    "href": "stata_do/stata_do5.html#introduction",
    "title": "Simultaneous/Uniform Confidence Intervals",
    "section": "",
    "text": "Confidence intervals (CI), also known as interval estimates, are ranges of values that provided along point estimates as potential values for population parameters, based on the estimation method, level of confidence, and degree of precision of those estimates.\nOne usual confusion that arises from CI is its interpretation. Its important to emphasize that CI are just estimates, and are not guaranteed to contain the true estimate.\nFor example, when we say the estimate for \\(\\alpha\\) is 1 with a 95% CI of \\((-.5,.5)\\), it doesnt mean that the true parameter will be contained in that confidence interval with a 95% probability.\nInstead, it means that if we repeat the sampling excercise a large number of times, and construct a CI of the same level, it will contain the true parameter 95% of the time.\n\n\nCode\nclear\nset scheme white2\nset obs 200\ngen p = (_n*2-1)/(2*_N)\ngen sd = (1/sqrt(500))\ngen b1 = invnormal(p)*sd\ngen ll=b1-sd*invnormal(0.95)\ngen uu=b1+sd*invnormal(0.95)\n\ngen n=p\ntwo (rarea ll uu n if uu&lt;0,color(red%50)) (rarea ll uu n if ll&gt;0,color(red%50)) ///\n    (rarea ll uu n if inrange(0,ll,uu), color(green%20)) (line b1 n, color(gs1)) , ///\n    legend(order(1 \"Rejection of Null\" 3 \"Null not Rejected\"))  \ngraph export uci1.png, width(1000)  height(500) replace\n\n\n\n\n\n\n\n\nFigure 1: 90% CI\n\n\n\nAs an example, Figure 1 shows a 90% Confidence intervals for a coefficient that by construction has a true value of zero. When considering the CI, the middle 90% of the coefficients contain the true paramater of zero. But the upper and lower 5% do not. Thus, with a 90% confidence interval, we have 10% chances of rejecting the Null when it is true.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Simultaneous/Uniform Confidence Intervals"
    ]
  },
  {
    "objectID": "stata_do/stata_do5.html#the-problem-of-simultaneous-hypothesis",
    "href": "stata_do/stata_do5.html#the-problem-of-simultaneous-hypothesis",
    "title": "Simultaneous/Uniform Confidence Intervals",
    "section": "The Problem of Simultaneous hypothesis",
    "text": "The Problem of Simultaneous hypothesis\nWhile CI are useful to understand the level of precision of an estimate, they are not a good tool when one is interest in testing multiple hypothesis at the same time.\nConsider the following example:\n\nSay that you have access to two independent sources of data, and you test the same hypothesis in both. Say, on top of this, that you use 90% confidence intervals to make your hypothesis. How likely is that either of the test will come as possitve?\n\nThe answer in this case is straigh forward. The chances that the first estimator will be a false positive: 10%. When negative (90%) there is also a 10% chance of being a false positive, in the second sample: \\(10%+90%*10%=19%\\). So using a second sample, almost doubles the probability of finding a false significant result.\nLet’s take the excercise a bit farther. If we run the same experiment on 7 different samples, the chances of finding a possitive results is now over 50%. Its easy to see that one can very easily falsify results by just adding the number of samples used for the analysis.1\nWhat other alternatives do we have? Well, there are at least 2:\n\nUse joint hypothesis tests (wald-test). This will adjust for possible correlations (or lack there of) among the estimated parameters, and provide a single statistic that tests if at least one of the parameters is different from zero.\nUse Simultaneous Confidence Intervals, which adjusts standard critical values, so that only \\(\\alpha%\\) of the cases we jointly rejected coefficients of interest. I like to think of this as a way to translate joint tests into single hypothesis testing.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Simultaneous/Uniform Confidence Intervals"
    ]
  },
  {
    "objectID": "stata_do/stata_do5.html#how-will-this-work",
    "href": "stata_do/stata_do5.html#how-will-this-work",
    "title": "Simultaneous/Uniform Confidence Intervals",
    "section": "How will this work?",
    "text": "How will this work?\nLets reconsider the previous example. When I have two independent samples the naive joint share of false significance tests would be given by:\n\\[\\alpha_{12} = \\alpha_1 + (1-\\alpha_1)* \\alpha_2\\]\nwhere \\(1-\\alpha_j\\) is the level of confidence you would normaly used for single Hypothesis testing.\nNow, we know \\(\\alpha_j\\) is to high, and that it has to be the same for both groups. Our goal is find an appropriate \\(\\alpha_j\\), so that the joint probability of false positive is 5% (\\(\\alpha_{12}\\)):\n\\[\\alpha_{12}= \\alpha + (1-\\alpha)* \\alpha = 2\\alpha - \\alpha^2\\]\nWhich suggest the level of significance we need to use on each individual sampe should be:\n\\[\\alpha = 1-\\sqrt{1-\\alpha_{12}}\\]\nFor the previous case, if one is trying to use a 90% confidence interval, we need to set \\(\\alpha_{12}\\) at 10% (0.1). Which implies that for each individual test we need to use an \\(\\alpha\\) at 0.0513 or 5.13%. Which is almost half of what we started with.\nOf course, if you have more than 2 coefficients you need testing, or if there are some correlations among them, the formula above may need to be adjusted. (some approaches do something like that)",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Simultaneous/Uniform Confidence Intervals"
    ]
  },
  {
    "objectID": "stata_do/stata_do5.html#uci-via-influence-functions-if",
    "href": "stata_do/stata_do5.html#uci-via-influence-functions-if",
    "title": "Simultaneous/Uniform Confidence Intervals",
    "section": "UCI via Influence Functions (IF)",
    "text": "UCI via Influence Functions (IF)\nThe first time I came through the idea of uniform confidence Intervals was when I started working with csdid (@callaway_santanna_2021), where both standard errors and confidence intervals are estimated via Influence functions.\nThis is done as follows:\n\nDerive the influence functions of your estimator.\nUse a multiplicative wildbootstrap to disturb the IF, estimate standard errors and absolute value of t-statistics for all your parameters.\nFor each iteration of the wildbootstrap, choose the largest t-statistic. You have now a single column of t-statistics.\nFrom this new variable choose the (\\(1-\\alpha\\)) percentile. This will be your new critical value.\nRe-estimate your Confidence intervals.\n\nBecause the new critical value was constructed by looking at the highest t-stat, it will have a the same properties as the manual significance level I provided above.\nHere a small example of how it works.\n\nsysuse auto, clear\ngen one=1\nmata\n    x  =st_data(.,\"mpg foreign one\")\n    y  =st_data(.,\"price\")\n    n  = rows(y)\n    xx =cross(x,x); xy =cross(x,y)\n    ixx=invsym(xx)\n    b  =ixx*xy; e = y:-x*b\n    iff = 74*(x:*e)*ixx\n    iffm=J(1000,3,0)\n    for(i=1;i&lt;=1000;i++){\n        iffm[i,]=mean(iff:*rnormal(n,1,0,1))\n    }\n    se=diagonal(variance(iffm):^.5)'\n    ts=abs(iffm:/se);ts=rowmax(ts);_sort(ts,1)\n    tcrit=ts[ceil(0.95*rows(ts))]\n    st_matrix(\"tbl\", (b,se',b:-tcrit*se',b:+tcrit*se'))\nend\n\n\n\nCode\nreg price mpg foreign, robust\nmatrix rowname tbl =mpg foreign cons\nmatrix colname tbl =b se ll uu\nmatrix list tbl\nmata:\"Adjusted Critical Value\"\nmata:tcrit\n\n\n\nLinear regression                               Number of obs     =         74\n                                                F(2, 71)          =      12.72\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.2838\n                                                Root MSE          =     2530.9\n\n------------------------------------------------------------------------------\n             |               Robust\n       price | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -294.1955   60.33645    -4.88   0.000     -414.503   -173.8881\n     foreign |   1767.292   607.7385     2.91   0.005     555.4961    2979.088\n       _cons |   11905.42   1362.547     8.74   0.000     9188.573    14622.26\n------------------------------------------------------------------------------\n\ntbl[3,4]\n                  b          se          ll          uu\n    mpg  -294.19553   59.797238   -433.1367  -155.25436\nforeign   1767.2922   616.19052   335.54999   3199.0345\n   cons   11905.415   1335.2436   8802.9257   15007.905\n  Adjusted Critical Value\n  2.323538262\n\n\nFirst, I estimate the influence function iff for the model coefficients. Then, I disturb them using a standard normal noise. I estimate standard errors and t-statistics based on the disturb data iffm, and obtain the adjusted critical value tcrit. This critical value is then used to reconstruct the confidence intervals.\nI compare the results from the regression analysis, with the adjusted critical value. And, as expected, they are a bit wider than the original confidence interval.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Simultaneous/Uniform Confidence Intervals"
    ]
  },
  {
    "objectID": "stata_do/stata_do5.html#via-simulation",
    "href": "stata_do/stata_do5.html#via-simulation",
    "title": "Simultaneous/Uniform Confidence Intervals",
    "section": "Via Simulation",
    "text": "Via Simulation\nThe strategy I show above is relatively simple if you know how to estimate the IF. However, beyond simple statistics, obtaining them may be too involved for the average user.\nSo, one alternative I’m suggesting here is to do the same, but via simulation.\n\nFor any regression analysis obtain the matrix of coefficients b and variance covariance V.\nDraw N normal distributed samples with mean zero and Variance covariance V.\nObtain the z/t statistic from the simulated coefficients, and the model estimated variance matrix.\nJust like before, obtain the new critical value, based on the new set of t-statistics, and adjust the Confidence intervals!\n\nHere is where my new command uci comes into play. First, you may need to install it using fra. Also if you want to use fra please see here for instructions:\n\nfra install uci, replace\n\nchecking uci consistency and verifying not already installed...\ninstalling into c:\\ado\\plus\\...\ninstallation complete.\n\n\nThe command does not have a helpfile yet, but its use is straigh forward.\n\nIf you run it after any regression command, it will take the information from that regression and obtain the modified confidence intervals.\nIf one wants to do it using other pre-saved information, one can do so using the option bmatrix() and vmatrix().\nOne can choose how many repetitions to run, using reps(#), where the default is 999\nIts possible to set a seed using rseed() for cases where replication is needed.\nYou can also set the level of confidence intervals level(#), where the default is 95.\n\nSo, let me redo the above example:\n\nreg price mpg foreign, robust\nuci\n\n\nLinear regression                               Number of obs     =         74\n                                                F(2, 71)          =      12.72\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.2838\n                                                Root MSE          =     2530.9\n\n------------------------------------------------------------------------------\n             |               Robust\n       price | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -294.1955   60.33645    -4.88   0.000     -414.503   -173.8881\n     foreign |   1767.292   607.7385     2.91   0.005     555.4961    2979.088\n       _cons |   11905.42   1362.547     8.74   0.000     9188.573    14622.26\n------------------------------------------------------------------------------\nUniform Confidence Intervals based on Simulation\n\nrtable2[3,5]\n                  b          se           t          ll          lu\n    mpg  -294.19553   60.336453  -4.8759169  -434.12725  -154.26381\nforeign   1767.2922   607.73848   2.9079815   357.83104   3176.7534\n  _cons   11905.415   1362.5472   8.7376169    8745.409   15065.422\nRepetitions:999\nLevel:95\nnew T:2.31919\n\n\nThis provides a slighly different results, because of how standard errors are estimated, and the random nature bootstrap procedure, but it should be a good approximation to the Uniform Confidence intervals.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Simultaneous/Uniform Confidence Intervals"
    ]
  },
  {
    "objectID": "stata_do/stata_do5.html#conclusions",
    "href": "stata_do/stata_do5.html#conclusions",
    "title": "Simultaneous/Uniform Confidence Intervals",
    "section": "Conclusions",
    "text": "Conclusions\nThis aims to provide a small description of what Uniform confidence intervals are, and how to estimate them.\nIt also introduces a new small command, uci, which should help to estimate this type of confidence intervals, for almost any model.\nComments and suggestions are welcome.\nTil next time.",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Simultaneous/Uniform Confidence Intervals"
    ]
  },
  {
    "objectID": "stata_do/stata_do5.html#footnotes",
    "href": "stata_do/stata_do5.html#footnotes",
    "title": "Simultaneous/Uniform Confidence Intervals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis will not happen if we just increase the sample↩︎",
    "crumbs": [
      "Stata Do's Ado's",
      "Stata Do",
      "Simultaneous/Uniform Confidence Intervals"
    ]
  }
]